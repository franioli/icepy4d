{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from icepy4d.utils import initialization\n",
    "from icepy4d.classes import Image\n",
    "from icepy4d.matching import SuperGlueMatcher, LOFTRMatcher, Quality, TileSelection, GeometricVerification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though this step is not mandatory, it is suggested to setup a logger to see the output of the matching process. If no logger is setup, the output of the process is suppressed.\n",
    "The logger can be setup as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialization.setup_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to load the images as numpy arrays.  \n",
    "We will use the Image class implemented in ICEpy4D, which allows for creating an Image instance by passing the path to the image file as `Image('path_to_image')`.  \n",
    "Creating the Image instance will read the exif data of the image and store them in the Image object. The actual image value is read when the `Image.value` proprierty is accessed.\n",
    "Alternatevely, one can also use OpencCV imread function to read the image as a numpy array (pay attention to the channel order, that should be RGB, while Opencv uses BGR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(4008, 6012, 3)\n",
      "(4008, 6012, 3)\n"
     ]
    }
   ],
   "source": [
    "image0 = Image('../data/img/p1/IMG_2637.jpg').value\n",
    "image1 = Image('../data/img/p2/IMG_1112.jpg').value\n",
    "\n",
    "print(type(image0))\n",
    "print(image0.shape)\n",
    "print(image1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SuperGlue matching\n",
    "\n",
    "For running the matching with SuperGlue, a new SuperGlueMatcher object must be initialized with the parameters for SuperGlue matching (see the documentation of the class for more details). The parameters are given as a dictionary.\n",
    "\n",
    "The configuration dictionary may contain the following keys:\n",
    "- \"weights\": defines the type of the weights used for SuperGlue inference. It can be either \"indoor\" or \"outdoor\". The default value is \"outdoor\".\n",
    "- \"keypoint_threshold\": threshold for the SuperPoint keypoint detector. The default value is 0.001.\n",
    "- \"max_keypoints\": maximum number of keypoints to be detected by SuperPoint. If -1, no limit to keypoint detection is set. The default value is -1.\n",
    "- \"match_threshold\": threshold for the SuperGlue feature matcher. Default value is 0.3.\n",
    "- \"force_cpu\": if True, SuperGlue will run on CPU. Default value is False.\n",
    "- \"nms_radius\": radius for non-maximum suppression. Default value is 3.\n",
    "- \"sinkhorn_iterations\": number of iterations for the Sinkhorn algorithm. Default value is 20.\n",
    "\n",
    "If the configuration dictionary is not given, the default values are used.\n",
    "\n",
    "When running the matching, additional parameters can be given as arguments to the `match` method to define the matching behavior. The parameters are the following:\n",
    "- image0: the first image to be matched.\n",
    "- image1: the second image to be matched.\n",
    "- quality: define the resize factor for the input images. Possible values \"highest\", \"high\" or \"medium\", \"low\". With \"high\", images are matched with full resulution. With \"highest\" images are up-sampled by a factor 2. With \"medium\" and \"low\" images are downsampled respectively by a factor 2 and 4. The default value is \"high\".\n",
    "- tile_selection: tile selection approach. Possible values are `TileSelection.None`, `TileSelection.EXHAUSTIVE`, `TileSelection.GRID` or `TileSelection.PRESELECTION`. Refer to the following \"Tile Section\" section for more information. The default value is `TileSelection.PRESELCTION`.\n",
    "- grid: if tile_selection is not `TileSelection.None`, this parameter defines the grid size.\n",
    "- overlap: if tile_selection is not `TileSelection.None`, this parameter defines the overlap between tiles.\n",
    "- do_viz_matches: if True, the matches are visualized. Default value is False.\n",
    "- do_viz_tiles: if True, the tiles are visualized. Default value is False.\n",
    "- save_dir: if not None, the matches are saved in the given directory. Default value is None.\n",
    "- geometric_verification: defines the geometric verification approach.\n",
    "\n",
    "\n",
    "#### Tile Selection\n",
    "\n",
    "To guarantee the highest collimation accuracy, by default the matching is performed on full resolution images.\n",
    "However, due to limited memory capacity in mid-class GPUs, high- resolution images captured by DSLR cameras may not fit into GPU memory. To overcome this limitation, ICEPy4D divides the images into smaller regular tiles with maximum dimension of 2000 px, computed over a regular grid.\n",
    "The tile selection can be performed in four different ways:\n",
    "\n",
    "1. `TileSelection.None`  \n",
    "   Images are matched as a whole in just one step. No tiling is performed.\n",
    "2. `TileSelection.EXHAUSTIVE`  \n",
    "   All the tiles in the first image are matched with all the tiles in the second image. This approach is very computational demading as the pairs of tiles are all the possible combinations of tiles from the two images and the total number of pairs rises quickly with the number of tiles. Additionally, several spurios matches may be found in tiles that do not overlap in the two images. \n",
    "3. `TileSelection.GRID`  \n",
    "   Tiles pairs are selected only based on the position of each tile in the grid, i.e., tile 1 in imageA is matched with tile 1 in imageB, tile 2 in imageA is matched with tile 2 in imageB, and so on. This approach is less computational demanding than the exhaustive one, but it is suitable only for images that are well aligned along a stripe with regular viewing geometry.\n",
    "4. `TileSelection.PRESELECTION`  \n",
    "   This is the only actual 'preselection' of the tiles, as the process is carried out in two steps.\n",
    "   First, a matching is performed on downsampled images. Subsequently, the full-resolution images are subdivided into regulartiles, and only the tiles that have corresponding features in the low-resolution images are selected as candidates for a second matching step.\n",
    "\n",
    "When a tile pre-selection approach is chosen, the tile grid must be defined by the `tile_grid` argument. This is a list of integers that defines the number of tiles along the x and y direction (i.e., number of columns and number of rows). For example, `tile_grid=[3,2]` defines a grid with 3 columns and 2 rows.\n",
    "Additionally, a parameter specifiyng the overlap between different tiles can be defined by the `overlap` argument. This is an integer number that defines the number of pixels of overlap between adjacent tiles. For example, `overlap=200` defines an overlap of 100 pixels between adjacent tiles. The overlap helps to avoid missing matches at the tile boundaries.\n",
    "\n",
    "The following figure shows the tile preselection process. An example of the tiles that are selected for the second matching step are highlighted in green.\n",
    "\n",
    "![title](notebook_figs/tile_preselection.png)\n",
    "\n",
    "#### Geometric Verification\n",
    "Geometric verification of the matches is performed by using Pydegensac (Mishkin et al., 2015), that allows for robustly estimate the fundamental matrix. \n",
    "The maximum re-projection error to accept a match is set to 1.5 px by default, but it can be changed by the user. \n",
    "The successfully matched features, together with their descriptors and scores, are saved as a Features object for each camera and stored into the current Epoch object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;37m2023-09-12 10:06:01 | [INFO    ] Running inference on device cuda\u001b[0m\n",
      "Loaded SuperPoint model\n",
      "Loaded SuperGlue model (\"outdoor\" weights)\n",
      "\u001b[0;37m2023-09-12 10:06:01 | [INFO    ] Matching by tiles...\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:06:01 | [INFO    ] Matching tiles by preselection tile selection\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:06:02 | [INFO    ] Matching completed.\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:06:02 | [INFO    ]  - Matching tile pair (3, 2)\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:06:05 | [INFO    ]  - Matching tile pair (4, 7)\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:06:07 | [INFO    ]  - Matching tile pair (5, 7)\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:06:10 | [INFO    ]  - Matching tile pair (5, 8)\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:06:13 | [INFO    ]  - Matching tile pair (6, 6)\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:06:15 | [INFO    ]  - Matching tile pair (6, 9)\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:06:18 | [INFO    ]  - Matching tile pair (7, 6)\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:06:20 | [INFO    ]  - Matching tile pair (7, 7)\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:06:23 | [INFO    ]  - Matching tile pair (7, 9)\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:06:25 | [INFO    ]  - Matching tile pair (7, 10)\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:06:28 | [INFO    ]  - Matching tile pair (8, 7)\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:06:30 | [INFO    ]  - Matching tile pair (8, 8)\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:06:33 | [INFO    ]  - Matching tile pair (8, 10)\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:06:36 | [INFO    ]  - Matching tile pair (8, 11)\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:06:38 | [INFO    ]  - Matching tile pair (9, 9)\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:06:41 | [INFO    ]  - Matching tile pair (10, 9)\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:06:43 | [INFO    ]  - Matching tile pair (10, 10)\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:06:46 | [INFO    ]  - Matching tile pair (11, 10)\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:06:48 | [INFO    ] Restoring full image coordinates of matches...\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:06:48 | [INFO    ] Matching by tile completed.\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:06:48 | [INFO    ] Matching done!\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:06:48 | [INFO    ] Performing geometric verification...\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:06:49 | [INFO    ] Pydegensac found 2585 inliers (47.98%)\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:06:49 | [INFO    ] Geometric verification done.\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:06:50 | [INFO    ] [Timer] | [Matching] preselection=0.994, matching=46.102, geometric_verification=0.081, \u001b[0m\n",
      "Function match took 48.2911 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_cfg = {\n",
    "    \"weights\": \"outdoor\",\n",
    "    \"keypoint_threshold\": 0.0001,\n",
    "    \"max_keypoints\": 8192,\n",
    "    \"match_threshold\": 0.2,\n",
    "    \"force_cpu\": False,\n",
    "}\n",
    "\n",
    "matcher = SuperGlueMatcher(matching_cfg)\n",
    "matcher.match(\n",
    "    image0,\n",
    "    image1,\n",
    "    quality=Quality.HIGH,\n",
    "    tile_selection=TileSelection.PRESELECTION,\n",
    "    grid=[4, 3],\n",
    "    overlap=200,\n",
    "    do_viz_matches=True,\n",
    "    do_viz_tiles=False,\n",
    "    save_dir = \"./matches/superglue_matches\",\n",
    "    geometric_verification=GeometricVerification.PYDEGENSAC,\n",
    "    threshold=1.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matches with their descriptors and scores are saved in the matcher object.\n",
    "All the results are saved as numpy arrays with float32 dtype.\n",
    "They can be accessed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 2585\n",
      "Matches on image0 (first 5):\n",
      "[[   9. 1373.]\n",
      " [  10. 1348.]\n",
      " [  10. 1439.]\n",
      " [  11. 1411.]\n",
      " [  18. 1426.]]\n",
      "Matches on image1 (first 5):\n",
      "[[5342.  126.]\n",
      " [5350.   91.]\n",
      " [5249.  244.]\n",
      " [5267.  204.]\n",
      " [5268.  224.]]\n",
      "Descriptors shape: (256, 2585)\n",
      "Scores shape: (2585,)\n",
      "Confidence shape: (2585,)\n",
      "Confidence (first 5): [0.38059255 0.1878048  0.17584147 0.06356245 0.16298756]\n"
     ]
    }
   ],
   "source": [
    "# Get matched keypoints\n",
    "mktps0 = matcher.mkpts0\n",
    "mktps1 = matcher.mkpts1\n",
    "\n",
    "print(f\"Number of matches: {len(mktps0)}\")\n",
    "print(f\"Matches on image0 (first 5):\\n{mktps0[0:5]}\")\n",
    "print(f\"Matches on image1 (first 5):\\n{mktps1[0:5]}\")\n",
    "\n",
    "# Get descriptors\n",
    "descs0 = matcher.descriptors0\n",
    "descs1 = matcher.descriptors1\n",
    "print(f\"Descriptors shape: {descs0.shape}\") \n",
    "\n",
    "# Get scores of each matched keypoint\n",
    "scores0 = matcher.scores0\n",
    "scores1 = matcher.scores1\n",
    "print(f\"Scores shape: {scores0.shape}\")\n",
    "\n",
    "# Matching confidence\n",
    "confidence = matcher.mconf\n",
    "print(f\"Confidence shape: {confidence.shape}\")\n",
    "print(f\"Confidence (first 5): {confidence[0:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOFTR matching\n",
    "\n",
    "The LOFTR matcher shares the same interface as the SuperGlue matcher, therefore the same parameters can be used for the `match` method. \n",
    "The only difference is in the matcher initialization, which takes no parameters, as default values are defined from Kornia (see the documentation of the class for more details).\n",
    "\n",
    "The matched points can be retrieved as before, but the descriptors are not saved in the matcher object, as they are not computed by LOFTR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;37m2023-09-12 10:37:28 | [INFO    ] Running inference on device cuda\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:37:28 | [INFO    ] Matching by tiles...\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:37:28 | [INFO    ] Matching tiles by preselection tile selection\u001b[0m\n",
      "\u001b[0;37m2023-09-12 10:37:28 | [INFO    ]  - Matching tile pair (0, 1)\u001b[0m\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.61 GiB (GPU 0; 11.75 GiB total capacity; 7.67 GiB already allocated; 1.19 GiB free; 9.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m matcher \u001b[39m=\u001b[39m LOFTRMatcher()\n\u001b[0;32m----> 2\u001b[0m matcher\u001b[39m.\u001b[39;49mmatch(\n\u001b[1;32m      3\u001b[0m     image0,\n\u001b[1;32m      4\u001b[0m     image1,\n\u001b[1;32m      5\u001b[0m     quality\u001b[39m=\u001b[39;49mQuality\u001b[39m.\u001b[39;49mHIGH,\n\u001b[1;32m      6\u001b[0m     tile_selection\u001b[39m=\u001b[39;49mTileSelection\u001b[39m.\u001b[39;49mPRESELECTION,\n\u001b[1;32m      7\u001b[0m     grid\u001b[39m=\u001b[39;49m[\u001b[39m5\u001b[39;49m, \u001b[39m4\u001b[39;49m],\n\u001b[1;32m      8\u001b[0m     overlap\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m     save_dir\u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m./matches/LOFTR_matches\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m     geometric_verification\u001b[39m=\u001b[39;49mGeometricVerification\u001b[39m.\u001b[39;49mPYDEGENSAC,\n\u001b[1;32m     11\u001b[0m     threshold\u001b[39m=\u001b[39;49m\u001b[39m1.5\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m mktps0 \u001b[39m=\u001b[39m matcher\u001b[39m.\u001b[39mmkpts0\n\u001b[1;32m     15\u001b[0m mktps1 \u001b[39m=\u001b[39m matcher\u001b[39m.\u001b[39mmkpts1\n",
      "File \u001b[0;32m~/phd/icepy4d/src/icepy4d/utils/timer.py:12\u001b[0m, in \u001b[0;36mtimeit.<locals>.timeit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtimeit_wrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     11\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[0;32m---> 12\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     13\u001b[0m     end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m     14\u001b[0m     total_time \u001b[39m=\u001b[39m end_time \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/phd/icepy4d/src/icepy4d/matching/matchers.py:234\u001b[0m, in \u001b[0;36mImageMatcherBase.match\u001b[0;34m(self, image0, image1, quality, tile_selection, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mMatching by tiles...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 234\u001b[0m     features0, features1, matches0, mconf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_match_tiles(\n\u001b[1;32m    235\u001b[0m         image0_, image1_, tile_selection, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    236\u001b[0m     )\n\u001b[1;32m    238\u001b[0m \u001b[39m# Retrieve original image coordinates if matching was performed on up/down-sampled images\u001b[39;00m\n\u001b[1;32m    239\u001b[0m features0, features1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_resize_features(quality, features0, features1)\n",
      "File \u001b[0;32m~/phd/icepy4d/src/icepy4d/matching/matchers.py:1019\u001b[0m, in \u001b[0;36mLOFTRMatcher._match_tiles\u001b[0;34m(self, image0, image1, tile_selection, **kwargs)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode():\n\u001b[1;32m   1018\u001b[0m     input_dict \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mimage0\u001b[39m\u001b[39m\"\u001b[39m: timg0_, \u001b[39m\"\u001b[39m\u001b[39mimage1\u001b[39m\u001b[39m\"\u001b[39m: timg1_}\n\u001b[0;32m-> 1019\u001b[0m     correspondences \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmatcher(input_dict)\n\u001b[1;32m   1021\u001b[0m \u001b[39m# Get matches and build features\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m mkpts0 \u001b[39m=\u001b[39m correspondences[\u001b[39m\"\u001b[39m\u001b[39mkeypoints0\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/miniconda3/envs/icepy4d/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/icepy4d/lib/python3.10/site-packages/kornia/feature/loftr/loftr.py:162\u001b[0m, in \u001b[0;36mLoFTR.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    159\u001b[0m feat_c0, feat_c1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloftr_coarse(feat_c0, feat_c1, mask_c0, mask_c1)\n\u001b[1;32m    161\u001b[0m \u001b[39m# 3. match coarse-level\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoarse_matching(feat_c0, feat_c1, _data, mask_c0\u001b[39m=\u001b[39;49mmask_c0, mask_c1\u001b[39m=\u001b[39;49mmask_c1)\n\u001b[1;32m    164\u001b[0m \u001b[39m# 4. fine-level refinement\u001b[39;00m\n\u001b[1;32m    165\u001b[0m feat_f0_unfold, feat_f1_unfold \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfine_preprocess(feat_f0, feat_f1, feat_c0, feat_c1, _data)\n",
      "File \u001b[0;32m~/miniconda3/envs/icepy4d/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/icepy4d/lib/python3.10/site-packages/kornia/feature/loftr/utils/coarse_matching.py:123\u001b[0m, in \u001b[0;36mCoarseMatching.forward\u001b[0;34m(self, feat_c0, feat_c1, data, mask_c0, mask_c1)\u001b[0m\n\u001b[1;32m    120\u001b[0m feat_c0, feat_c1 \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m feat: feat \u001b[39m/\u001b[39m feat\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m, [feat_c0, feat_c1])\n\u001b[1;32m    122\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmatch_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdual_softmax\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 123\u001b[0m     sim_matrix \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49meinsum(\u001b[39m\"\u001b[39;49m\u001b[39mnlc,nsc->nls\u001b[39;49m\u001b[39m\"\u001b[39;49m, feat_c0, feat_c1) \u001b[39m/\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtemperature\n\u001b[1;32m    124\u001b[0m     \u001b[39mif\u001b[39;00m mask_c0 \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m mask_c1 \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m         sim_matrix\u001b[39m.\u001b[39mmasked_fill_(\u001b[39m~\u001b[39m(mask_c0[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m mask_c1[:, \u001b[39mNone\u001b[39;00m])\u001b[39m.\u001b[39mbool(), \u001b[39m-\u001b[39mINF)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.61 GiB (GPU 0; 11.75 GiB total capacity; 7.67 GiB already allocated; 1.19 GiB free; 9.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "matcher = LOFTRMatcher()\n",
    "matcher.match(\n",
    "    image0,\n",
    "    image1,\n",
    "    quality=Quality.HIGH,\n",
    "    tile_selection=TileSelection.PRESELECTION,\n",
    "    grid=[5, 4],\n",
    "    overlap=50,\n",
    "    save_dir= \"./matches/LOFTR_matches\",\n",
    "    geometric_verification=GeometricVerification.PYDEGENSAC,\n",
    "    threshold=1.5,\n",
    ")\n",
    "\n",
    "mktps0 = matcher.mkpts0\n",
    "mktps1 = matcher.mkpts1\n",
    "\n",
    "print(f\"Number of matches: {len(mktps0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up result folders\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "if os.path.exists(\"./matches\"):\n",
    "    shutil.rmtree(\"./matches\")\n",
    "if os.path.exists(\"./logs\"):\n",
    "    shutil.rmtree(\"./logs\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icepy4d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
