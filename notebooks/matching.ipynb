{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Matching\n",
    "\n",
    "This module introduce different algorithms for performing feature matching on pairs of images by using Deep Learning feature matching algorithms, such as [SuperGlue](https://github.com/magicleap/SuperGluePretrainedNetwork), [LOFTR](https://github.com/zju3dv/LoFTR) or [LightGlue](https://github.com/cvg/LightGlue).\n",
    "\n",
    "Feature matching consists of extracting corresponding points between two images of the same scene/object. This is a fundamental step in many computer vision applications, such as object detection, tracking, and motion estimation, as well as in the photogrammetric process of image-based 3D reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the required modules.  \n",
    "Additionally, even though this step is not mandatory, it is suggested to setup a logger to see the output of the matching process. If no logger is setup, the output of the process is suppressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from icepy4d.core import Image\n",
    "from icepy4d.utils import setup_logger\n",
    "from icepy4d.matching import (SuperGlueMatcher, LOFTRMatcher, LightGlueMatcher, Quality, TileSelection, GeometricVerification)\n",
    "\n",
    "setup_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the images as numpy arrays.  \n",
    "We will use the Image class implemented in ICEpy4D, which allows for creating an Image instance by passing the path to the image file as `Image('path_to_image')`.  \n",
    "Creating the Image instance will read the exif data of the image and store them in the Image object. The actual image value is read when the `Image.value` proprierty is accessed.\n",
    "Alternatevely, one can also use OpencCV imread function to read the image as a numpy array (pay attention to the channel order, that should be RGB, while Opencv uses BGR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image data-type: <class 'numpy.ndarray'>\n",
      "Image0 shape: (4008, 6012, 3)\n",
      "Image1 shape: (4008, 6012, 3)\n"
     ]
    }
   ],
   "source": [
    "image0 = Image('../data/img/p1/IMG_2650.jpg').value\n",
    "image1 = Image('../data/img/p2/IMG_1125.jpg').value\n",
    "\n",
    "print(f\"Image data-type: {type(image0)}\")\n",
    "print(f\"Image0 shape: {image0.shape}\")\n",
    "print(f\"Image1 shape: {image1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to run the matching\n",
    "\n",
    "All the matching algorithms implemented in ICEpy4D are implemented as a class, which can be initialized by passing a dictionary of parameters as input.\n",
    "The actual matching is then run by calling the `match` method of the class instance.\n",
    "\n",
    "Some parameters are common to all the matching algorithms, such as the the `Tiling` parameters, which are used to split the image in tiles to reduce the memory usage, and the `Geometric Verification` parameters, which are used to filter out the outliers from the matching results.\n",
    "\n",
    "The common parameters are presented here, while the specific parameters for each algorithm are presented in the corresponding section.\n",
    "\n",
    "\n",
    "\n",
    "When running the matching, additional parameters can be given as arguments to the `match` method to define the matching behavior. The parameters are the following:\n",
    "- image0: the first image to be matched.\n",
    "- image1: the second image to be matched.\n",
    "- quality: define the resize factor for the input images. Possible values \"highest\", \"high\" or \"medium\", \"low\". With \"high\", images are matched with full resulution. With \"highest\" images are up-sampled by a factor 2. With \"medium\" and \"low\" images are downsampled respectively by a factor 2 and 4. The default value is \"high\".\n",
    "- tile_selection: tile selection approach. Possible values are `TileSelection.None`, `TileSelection.EXHAUSTIVE`, `TileSelection.GRID` or `TileSelection.PRESELECTION`. Refer to the following \"Tile Section\" section for more information. The default value is `TileSelection.PRESELCTION`.\n",
    "- grid: if tile_selection is not `TileSelection.None`, this parameter defines the grid size.\n",
    "- overlap: if tile_selection is not `TileSelection.None`, this parameter defines the overlap between tiles.\n",
    "- do_viz_matches: if True, the matches are visualized. Default value is False.\n",
    "- do_viz_tiles: if True, the tiles are visualized. Default value is False.\n",
    "- save_dir: if not None, the matches are saved in the given directory. Default value is None.\n",
    "- geometric_verification: defines the geometric verification approach.\n",
    "\n",
    "\n",
    "#### Tile Selection\n",
    "\n",
    "To guarantee the highest collimation accuracy, by default the matching is performed on full resolution images.\n",
    "However, due to limited memory capacity in mid-class GPUs, high- resolution images captured by DSLR cameras may not fit into GPU memory. To overcome this limitation, ICEPy4D divides the images into smaller regular tiles with maximum dimension of 2000 px, computed over a regular grid.\n",
    "The tile selection can be performed in four different ways:\n",
    "\n",
    "1. `TileSelection.None`  \n",
    "   Images are matched as a whole in just one step. No tiling is performed.\n",
    "2. `TileSelection.EXHAUSTIVE`  \n",
    "   All the tiles in the first image are matched with all the tiles in the second image. This approach is very computational demading as the pairs of tiles are all the possible combinations of tiles from the two images and the total number of pairs rises quickly with the number of tiles. Additionally, several spurios matches may be found in tiles that do not overlap in the two images. \n",
    "3. `TileSelection.GRID`  \n",
    "   Tiles pairs are selected only based on the position of each tile in the grid, i.e., tile 1 in imageA is matched with tile 1 in imageB, tile 2 in imageA is matched with tile 2 in imageB, and so on. This approach is less computational demanding than the exhaustive one, but it is suitable only for images that are well aligned along a stripe with regular viewing geometry.\n",
    "4. `TileSelection.PRESELECTION`  \n",
    "   This is the only actual 'preselection' of the tiles, as the process is carried out in two steps.\n",
    "   First, a matching is performed on downsampled images. Subsequently, the full-resolution images are subdivided into regulartiles, and only the tiles that have corresponding features in the low-resolution images are selected as candidates for a second matching step.\n",
    "\n",
    "When a tile pre-selection approach is chosen, the tile grid must be defined by the `tile_grid` argument. This is a list of integers that defines the number of tiles along the x and y direction (i.e., number of columns and number of rows). For example, `tile_grid=[3,2]` defines a grid with 3 columns and 2 rows.\n",
    "Additionally, a parameter specifiyng the overlap between different tiles can be defined by the `overlap` argument. This is an integer number that defines the number of pixels of overlap between adjacent tiles. For example, `overlap=200` defines an overlap of 100 pixels between adjacent tiles. The overlap helps to avoid missing matches at the tile boundaries.\n",
    "\n",
    "The following figure shows the tile preselection process. An example of the tiles that are selected for the second matching step are highlighted in green.\n",
    "\n",
    "![Tile preselection](./img/tile_preselection.png)\n",
    "\n",
    "#### Geometric Verification\n",
    "Geometric verification of the matches is performed by using Pydegensac (Mishkin et al., 2015), that allows for robustly estimate the fundamental matrix. \n",
    "The maximum re-projection error to accept a match is set to 1.5 px by default, but it can be changed by the user. \n",
    "The successfully matched features, together with their descriptors and scores, are saved as a Features object for each camera and stored into the current Epoch object.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SuperGlue matching\n",
    "\n",
    "SuperGlue is a Deep Learning-based feature matching algorithm that uses a SuperPoint keypoint detector and a SuperGlue feature matcher.\n",
    "You can find some more information on SuperGlue in the [original paper](https://arxiv.org/abs/1911.11763) and in the [original repository](https://github.com/magicleap/SuperGluePretrainedNetwork).\n",
    "\n",
    "For running the matching with SuperGlue, a new SuperGlueMatcher object must be initialized.\n",
    "A set of additional parameters can be set when initializing the SuperGlueMatcher object. The parameters are given as a dictionary (see the documentation of the class for more details).\n",
    "\n",
    "The configuration dictionary may contain the following keys:\n",
    "- \"weights\": defines the type of the weights used for SuperGlue inference. It can be either \"indoor\" or \"outdoor\". The default value is \"outdoor\".\n",
    "- \"keypoint_threshold\": threshold for the SuperPoint keypoint detector. The default value is 0.001.\n",
    "- \"max_keypoints\": maximum number of keypoints to be detected by SuperPoint. If -1, no limit to keypoint detection is set. The default value is -1.\n",
    "- \"match_threshold\": threshold for the SuperGlue feature matcher. Default value is 0.3.\n",
    "- \"force_cpu\": if True, SuperGlue will run on CPU. Default value is False.\n",
    "- \"nms_radius\": radius for non-maximum suppression. Default value is 3.\n",
    "- \"sinkhorn_iterations\": number of iterations for the Sinkhorn algorithm. Default value is 20.\n",
    "\n",
    "If the configuration dictionary is not given, the default values are used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;37m2023-10-03 09:09:38 | [INFO    ] Running inference on device cuda\u001b[0m\n",
      "Loaded SuperPoint model\n",
      "Loaded SuperGlue model (\"outdoor\" weights)\n",
      "\u001b[0;37m2023-10-03 09:09:39 | [INFO    ] Matching by tiles...\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:09:39 | [INFO    ] Matching tiles by preselection tile selection\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:09:39 | [INFO    ]  - Matching tile pair (0, 1)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:09:42 | [INFO    ]  - Matching tile pair (2, 1)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:09:45 | [INFO    ]  - Matching tile pair (2, 2)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:09:48 | [INFO    ]  - Matching tile pair (2, 4)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:09:51 | [INFO    ]  - Matching tile pair (3, 2)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:09:53 | [INFO    ]  - Matching tile pair (3, 3)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:09:56 | [INFO    ]  - Matching tile pair (3, 4)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:09:59 | [INFO    ]  - Matching tile pair (3, 5)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:02 | [INFO    ]  - Matching tile pair (4, 4)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:04 | [INFO    ]  - Matching tile pair (5, 4)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:07 | [INFO    ]  - Matching tile pair (5, 5)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:10 | [INFO    ] Restoring full image coordinates of matches...\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:10 | [INFO    ] Matching by tile completed.\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:10 | [INFO    ] Matching done!\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:10 | [INFO    ] Performing geometric verification...\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:10 | [INFO    ] Pydegensac found 1150 inliers (50.46%)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:10 | [INFO    ] Geometric verification done.\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:10 | [INFO    ] [Timer] | [Matching] preselection=0.864, matching=30.168, geometric_verification=0.022, \u001b[0m\n",
      "Function match took 31.0577 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_cfg = {\n",
    "    \"weights\": \"outdoor\",\n",
    "    \"keypoint_threshold\": 0.0001,\n",
    "    \"max_keypoints\": 8192,\n",
    "    \"match_threshold\": 0.2,\n",
    "    \"force_cpu\": False,\n",
    "}\n",
    "\n",
    "matcher = SuperGlueMatcher(matching_cfg)\n",
    "matcher.match(\n",
    "    image0,\n",
    "    image1,\n",
    "    quality=Quality.HIGH,\n",
    "    tile_selection=TileSelection.PRESELECTION,\n",
    "    grid=[3,2],\n",
    "    overlap=200,\n",
    "    min_matches_per_tile = 5,\n",
    "    do_viz_tiles=False,\n",
    "    save_dir = \"./matches/superglue_matches\",\n",
    "    geometric_verification=GeometricVerification.PYDEGENSAC,\n",
    "    threshold=1.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matches with their descriptors and scores are saved in the matcher object.\n",
    "All the results are saved as numpy arrays with float32 dtype.\n",
    "They can be accessed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 1150\n",
      "Matches on image0 (first 5):\n",
      "[[   8. 1356.]\n",
      " [   8. 1383.]\n",
      " [   8. 1384.]\n",
      " [  10. 1372.]\n",
      " [  11. 1313.]]\n",
      "Matches on image1 (first 5):\n",
      "[[5342.   98.]\n",
      " [5335.  137.]\n",
      " [5335.  137.]\n",
      " [5341.  122.]\n",
      " [5449.    8.]]\n",
      "Descriptors shape: (256, 1150)\n",
      "Scores shape: (1150,)\n",
      "Confidence shape: (1150,)\n",
      "Confidence (first 5): [0.05422363 0.1714691  0.14908865 0.14837408 0.1697674 ]\n"
     ]
    }
   ],
   "source": [
    "# Get matched keypoints\n",
    "mktps0 = matcher.mkpts0\n",
    "mktps1 = matcher.mkpts1\n",
    "\n",
    "print(f\"Number of matches: {len(mktps0)}\")\n",
    "print(f\"Matches on image0 (first 5):\\n{mktps0[0:5]}\")\n",
    "print(f\"Matches on image1 (first 5):\\n{mktps1[0:5]}\")\n",
    "\n",
    "# Get descriptors\n",
    "descs0 = matcher.descriptors0\n",
    "descs1 = matcher.descriptors1\n",
    "print(f\"Descriptors shape: {descs0.shape}\") \n",
    "\n",
    "# Get scores of each matched keypoint\n",
    "scores0 = matcher.scores0\n",
    "scores1 = matcher.scores1\n",
    "print(f\"Scores shape: {scores0.shape}\")\n",
    "\n",
    "# Matching confidence\n",
    "confidence = matcher.mconf\n",
    "print(f\"Confidence shape: {confidence.shape}\")\n",
    "print(f\"Confidence (first 5): {confidence[0:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also plot the matches by using the `plot_matches` function of the ICEpy4D visualization module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icepy4d.visualization import plot_matches\n",
    "\n",
    "out = plot_matches(image0=image0, image1=image1, pts0=mktps0, pts1=mktps1, path=\"./matches/superglue_matches.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "### LightGlue matching\n",
    "\n",
    "LightGlue is a Deep Learning-based feature matching algorithm that uses a SuperPoint or DISK keypoint detectors. \n",
    "It is a recent evolution of the SuperGlue matcher, developed by the Computer Vision Group of ETH Zurich. \n",
    "You can find more information on LightGlue in the [original paper](https://arxiv.org/pdf/2306.13643.pdf) and in the [original repository](https://github.com/cvg/LightGlue).\n",
    "\n",
    "The process of running the matching with LightGlue is very similar to the one of SuperGlue. You just need to initialize a LightGlueMatcher object and run the matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;37m2023-10-03 09:10:13 | [INFO    ] Running inference on device cuda\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:13 | [INFO    ] Matching by tiles...\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:13 | [INFO    ] Matching tiles by preselection tile selection\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:14 | [INFO    ]  - Matching tile pair (0, 2)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:15 | [INFO    ]  - Matching tile pair (1, 1)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:16 | [INFO    ]  - Matching tile pair (1, 4)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:18 | [INFO    ]  - Matching tile pair (2, 4)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:19 | [INFO    ]  - Matching tile pair (2, 5)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:21 | [INFO    ]  - Matching tile pair (3, 3)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:22 | [INFO    ]  - Matching tile pair (4, 3)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:24 | [INFO    ]  - Matching tile pair (4, 4)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:25 | [INFO    ]  - Matching tile pair (5, 4)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:26 | [INFO    ]  - Matching tile pair (5, 5)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:28 | [INFO    ] Restoring full image coordinates of matches...\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:28 | [INFO    ] Matching by tile completed.\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:28 | [INFO    ] Matching done!\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:28 | [INFO    ] Performing geometric verification...\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:28 | [INFO    ] Pydegensac found 1763 inliers (51.66%)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:28 | [INFO    ] Geometric verification done.\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:29 | [INFO    ] [Timer] | [Matching] preselection=0.621, matching=14.147, geometric_verification=0.019, \u001b[0m\n",
      "Function match took 16.0027 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matcher = LightGlueMatcher()\n",
    "matcher.match(\n",
    "    image0,\n",
    "    image1,\n",
    "    quality=Quality.HIGH,\n",
    "    tile_selection=TileSelection.PRESELECTION,\n",
    "    grid=[2, 3],\n",
    "    overlap=200,\n",
    "    origin=[0, 0],\n",
    "    do_viz_matches=True,\n",
    "    do_viz_tiles=True,\n",
    "    min_matches_per_tile = 3,\n",
    "    max_keypoints = 10240,    \n",
    "    save_dir=\"./matches/LIGHTGLUE\",\n",
    "    geometric_verification=GeometricVerification.PYDEGENSAC,\n",
    "    threshold=2,\n",
    "    confidence=0.9999,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOFTR matching\n",
    "\n",
    "The LOFTR matcher shares the same interface as the SuperGlue matcher, therefore the same parameters can be used for the `match` method. \n",
    "The only difference is in the matcher initialization, which takes no parameters, as default values are defined from Kornia (see the documentation of the class for more details).\n",
    "\n",
    "The matched points can be retrieved as before, but the descriptors are not saved in the matcher object, as they are not computed by LOFTR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;37m2023-10-03 09:10:29 | [INFO    ] Running inference on device cuda\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:29 | [INFO    ] Matching by tiles...\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:29 | [INFO    ] Matching tiles by preselection tile selection\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:30 | [INFO    ]  - Matching tile pair (1, 1)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:30 | [INFO    ]  - Matching tile pair (4, 3)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:31 | [INFO    ]  - Matching tile pair (5, 1)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:32 | [INFO    ]  - Matching tile pair (5, 2)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:33 | [INFO    ]  - Matching tile pair (8, 8)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:34 | [INFO    ]  - Matching tile pair (8, 9)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:35 | [INFO    ]  - Matching tile pair (9, 9)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:36 | [INFO    ]  - Matching tile pair (9, 12)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:37 | [INFO    ]  - Matching tile pair (9, 13)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:37 | [INFO    ]  - Matching tile pair (10, 10)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:38 | [INFO    ]  - Matching tile pair (10, 13)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:39 | [INFO    ]  - Matching tile pair (10, 14)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:40 | [INFO    ]  - Matching tile pair (11, 10)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:41 | [INFO    ]  - Matching tile pair (11, 14)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:42 | [INFO    ]  - Matching tile pair (11, 15)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:43 | [INFO    ]  - Matching tile pair (12, 16)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:44 | [INFO    ]  - Matching tile pair (13, 12)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:44 | [INFO    ]  - Matching tile pair (13, 13)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:45 | [INFO    ]  - Matching tile pair (13, 17)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:46 | [INFO    ]  - Matching tile pair (14, 13)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:47 | [INFO    ]  - Matching tile pair (14, 17)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:48 | [INFO    ]  - Matching tile pair (15, 14)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:49 | [INFO    ]  - Matching tile pair (15, 18)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:50 | [INFO    ]  - Matching tile pair (17, 17)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:51 | [INFO    ]  - Matching tile pair (18, 17)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:52 | [INFO    ] Restoring full image coordinates of matches...\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:52 | [INFO    ] Matching by tile completed.\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:52 | [INFO    ] Matching done!\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:52 | [INFO    ] Performing geometric verification...\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:53 | [INFO    ] Pydegensac found 3393 inliers (17.91%)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:53 | [INFO    ] Geometric verification done.\u001b[0m\n",
      "\u001b[0;37m2023-10-03 09:10:53 | [INFO    ] [Timer] | [Matching] preselection=0.309, matching=22.242, geometric_verification=0.815, \u001b[0m\n",
      "Function match took 23.3744 seconds\n",
      "Number of matches: 3393\n"
     ]
    }
   ],
   "source": [
    "matcher = LOFTRMatcher()\n",
    "matcher.match(\n",
    "    image0,\n",
    "    image1,\n",
    "    quality=Quality.HIGH,\n",
    "    tile_selection=TileSelection.PRESELECTION,\n",
    "    grid=[5, 4],\n",
    "    overlap=50,\n",
    "    save_dir= \"./matches/LOFTR_matches\",\n",
    "    geometric_verification=GeometricVerification.PYDEGENSAC,\n",
    "    threshold=1.5,\n",
    ")\n",
    "\n",
    "mktps0 = matcher.mkpts0\n",
    "mktps1 = matcher.mkpts1\n",
    "\n",
    "print(f\"Number of matches: {len(mktps0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up result folders\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "if os.path.exists(\"./matches\"):\n",
    "    shutil.rmtree(\"./matches\")\n",
    "if os.path.exists(\"./logs\"):\n",
    "    shutil.rmtree(\"./logs\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icepy4d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
