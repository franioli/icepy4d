{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first set up the python environment and define the configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import required standard modules\n",
    "import shutil\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Import required icepy4d4D modules\n",
    "from icepy4d import classes as icepy4d_classes\n",
    "from icepy4d.classes.epoch import Epoch, Epoches\n",
    "from icepy4d import matching\n",
    "from icepy4d import sfm\n",
    "from icepy4d import io\n",
    "from icepy4d import utils as icepy4d_utils\n",
    "from icepy4d.metashape import metashape as MS\n",
    "from icepy4d.utils import initialization as inizialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the configuration file\n",
    "CFG_FILE = \"./config.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inizialize all the required variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================\n",
      "ICEpy4D\n",
      "Image-based Continuos monitoring of glaciers' Evolution with low-cost stereo-cameras and Deep Learning photogrammetry\n",
      "2023 - Francesco Ioli - francesco.ioli@polimi.it\n",
      "================================================================\n",
      "\n",
      "\u001b[0;37m2023-09-11 11:55:06 | [INFO    ] Configuration file: config\u001b[0m\n",
      "\u001b[0;37m2023-09-11 11:55:06 | [INFO    ] Epoch_to_process set to a pair of values. Expanding it for a range of epoches from epoch 0 to 158.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;37m2023-09-11 11:55:06 | [INFO    ] Image datastores created successfully.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Parse the configuration file\n",
    "cfg_file = Path(CFG_FILE)\n",
    "cfg = inizialization.parse_cfg(cfg_file)\n",
    "\n",
    "# Initialize the timer and logger\n",
    "timer_global = icepy4d_utils.AverageTimer()\n",
    "logger = icepy4d_utils.get_logger()\n",
    "\n",
    "# Get the list of cameras from the configuration file\n",
    "cams = cfg.cams\n",
    "\n",
    "# Get the list of images from the configuration file\n",
    "images, epoch_dict = inizialization.initialize_image_ds(cfg)\n",
    "\n",
    "# Initialize an empty Epoches object to store the results of each epoch\n",
    "epoches = Epoches(starting_epoch=cfg.proc.epoch_to_process[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stereo Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stereo processing is carried out for each epoch in order to find matched features, estimating camera pose, and triangulating the 3D points. \n",
    "The output of this step is a set of 3D points and their corresponding descriptors.\n",
    "\n",
    "The processing for all the epoches is then iterated in a big loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load or create a new Epoch object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2022-05-01_14:01:15\n"
     ]
    }
   ],
   "source": [
    "# Initialize a timer to measure the processing time\n",
    "timer = icepy4d_utils.AverageTimer()\n",
    "\n",
    "# Get epoch id to process\n",
    "ep = cfg.proc.epoch_to_process[0]\n",
    "\n",
    "# Define paths to the epoch directory\n",
    "epoch_name = epoch_dict[ep]\n",
    "epochdir = cfg.paths.results_dir / epoch_name\n",
    "\n",
    "# Get the list of images for the current epoch\n",
    "im_epoch = {\n",
    "    cam: icepy4d_classes.Image(images[cam][0]) for cam in cams\n",
    "}\n",
    "\n",
    "# Load cameras\n",
    "cams_ep = {}\n",
    "for cam in cams:\n",
    "    calib = icepy4d_classes.Calibration(cfg.paths.calibration_dir / f\"{cam}.txt\")\n",
    "    cams_ep[cam] = calib.to_camera()\n",
    "\n",
    "# Load targets\n",
    "target_paths = [\n",
    "    cfg.georef.target_dir\n",
    "    / (im_epoch[cam].stem + cfg.georef.target_file_ext)\n",
    "    for cam in cams\n",
    "]\n",
    "targ_ep = icepy4d_classes.Targets(\n",
    "    im_file_path=target_paths,\n",
    "    obj_file_path=cfg.georef.target_dir\n",
    "    / cfg.georef.target_world_file,\n",
    ")\n",
    "\n",
    "# Create empty features and points\n",
    "feat_ep = {cam: icepy4d_classes.Features() for cam in cams}\n",
    "pts_ep = icepy4d_classes.Points()\n",
    "\n",
    "# Create the epoch object\n",
    "epoch = Epoch(\n",
    "    im_epoch[cams[0]].datetime,\n",
    "    images=im_epoch,\n",
    "    cameras=cams_ep,\n",
    "    features=feat_ep,\n",
    "    points=pts_ep,\n",
    "    targets=targ_ep,\n",
    "    epoch_dir=epochdir,\n",
    ")\n",
    "print(f\"Epoch: {epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature matching with SuperGlue\n",
    "\n",
    "Wide-baseline feature matching is performed using the SuperGlue algorithm.\n",
    "Refer to the `matching.ipynb` notebook for more details about the matching process and explanation of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;37m2023-09-03 13:19:12 | [INFO    ] Running inference on device cuda\u001b[0m\n",
      "Loaded SuperPoint model\n",
      "Loaded SuperGlue model (\"outdoor\" weights)\n",
      "\u001b[0;37m2023-09-03 13:19:13 | [INFO    ] Matching by tiles...\u001b[0m\n",
      "\u001b[0;37m2023-09-03 13:19:13 | [INFO    ] Matching tiles by preselection tile selection\u001b[0m\n",
      "\u001b[0;37m2023-09-03 13:19:14 | [INFO    ] Matching completed.\u001b[0m\n",
      "\u001b[0;37m2023-09-03 13:19:14 | [INFO    ]  - Matching tile pair (3, 2)\u001b[0m\n",
      "\u001b[0;37m2023-09-03 13:19:16 | [INFO    ]  - Matching tile pair (4, 7)\u001b[0m\n",
      "\u001b[0;37m2023-09-03 13:19:18 | [INFO    ]  - Matching tile pair (5, 7)\u001b[0m\n",
      "\u001b[0;37m2023-09-03 13:19:20 | [INFO    ]  - Matching tile pair (5, 8)\u001b[0m\n",
      "\u001b[0;37m2023-09-03 13:19:23 | [INFO    ]  - Matching tile pair (6, 6)\u001b[0m\n",
      "\u001b[0;37m2023-09-03 13:19:25 | [INFO    ]  - Matching tile pair (6, 9)\u001b[0m\n",
      "\u001b[0;37m2023-09-03 13:19:27 | [INFO    ]  - Matching tile pair (7, 6)\u001b[0m\n",
      "\u001b[0;37m2023-09-03 13:19:29 | [INFO    ]  - Matching tile pair (7, 7)\u001b[0m\n",
      "\u001b[0;37m2023-09-03 13:19:31 | [INFO    ]  - Matching tile pair (7, 9)\u001b[0m\n",
      "\u001b[0;37m2023-09-03 13:19:33 | [INFO    ]  - Matching tile pair (7, 10)\u001b[0m\n",
      "\u001b[0;37m2023-09-03 13:19:36 | [INFO    ]  - Matching tile pair (8, 7)\u001b[0m\n",
      "\u001b[0;37m2023-09-03 13:19:38 | [INFO    ]  - Matching tile pair (8, 8)\u001b[0m\n",
      "\u001b[0;37m2023-09-03 13:19:40 | [INFO    ]  - Matching tile pair (8, 10)\u001b[0m\n",
      "\u001b[0;37m2023-09-03 13:19:42 | [INFO    ]  - Matching tile pair (8, 11)\u001b[0m\n",
      "\u001b[0;37m2023-09-03 13:19:44 | [INFO    ]  - Matching tile pair (9, 9)\u001b[0m\n",
      "\u001b[0;37m2023-09-03 13:19:46 | [INFO    ]  - Matching tile pair (10, 9)\u001b[0m\n",
      "\u001b[0;37m2023-09-03 13:19:49 | [INFO    ]  - Matching tile pair (10, 10)\u001b[0m\n",
      "\u001b[0;37m2023-09-03 13:19:51 | [INFO    ]  - Matching tile pair (11, 10)\u001b[0m\n",
      "\u001b[0;37m2023-09-03 13:19:53 | [INFO    ] Restoring full image coordinates of matches...\u001b[0m\n",
      "\u001b[0;37m2023-09-03 13:19:53 | [INFO    ] Matching by tile completed.\u001b[0m\n",
      "\u001b[0;37m2023-09-03 13:19:53 | [INFO    ] Matching done!\u001b[0m\n",
      "\u001b[0;37m2023-09-03 13:19:53 | [INFO    ] Performing geometric verification...\u001b[0m\n",
      "\u001b[0;37m2023-09-03 13:19:53 | [INFO    ] Pydegensac found 2012 inliers (36.58%)\u001b[0m\n",
      "\u001b[0;37m2023-09-03 13:19:53 | [INFO    ] Geometric verification done.\u001b[0m\n",
      "\u001b[0;37m2023-09-03 13:19:55 | [INFO    ] [Timer] | [Matching] preselection=1.014, matching=39.122, geometric_verification=0.449, \u001b[0m\n",
      "Function match took 41.6525 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define matching parameters\n",
    "matching_quality = matching.Quality.HIGH\n",
    "tile_selection = matching.TileSelection.PRESELECTION\n",
    "tiling_grid = [4, 3]\n",
    "tiling_overlap = 200\n",
    "geometric_verification = matching.GeometricVerification.PYDEGENSAC\n",
    "geometric_verification_threshold = 1\n",
    "geometric_verification_confidence = 0.9999\n",
    "match_dir = epoch.epoch_dir / \"matching\"\n",
    "\n",
    "# Create a new matcher object\n",
    "matcher = matching.SuperGlueMatcher(cfg.matching)\n",
    "matcher.match(\n",
    "    epoch.images[cams[0]].value,\n",
    "    epoch.images[cams[1]].value,\n",
    "    quality=matching_quality,\n",
    "    tile_selection=tile_selection,\n",
    "    grid=tiling_grid,\n",
    "    overlap=tiling_overlap,\n",
    "    do_viz_matches=True,\n",
    "    do_viz_tiles=False,\n",
    "    save_dir=match_dir,\n",
    "    geometric_verification=geometric_verification,\n",
    "    threshold=geometric_verification_threshold,\n",
    "    confidence=geometric_verification_confidence,\n",
    ")\n",
    "timer.update(\"matching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the matched features from the Matcher object and save them in the current Epoch object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary with empty Features objects for each camera, which will be filled with the matched keypoints, descriptors and scores\n",
    "f = {cam: icepy4d_classes.Features() for cam in cams}\n",
    "\n",
    "# Stack matched keypoints, descriptors and scores into Features objects\n",
    "f[cams[0]].append_features_from_numpy(\n",
    "    x=matcher.mkpts0[:, 0],\n",
    "    y=matcher.mkpts0[:, 1],\n",
    "    descr=matcher.descriptors0,\n",
    "    scores=matcher.scores0,\n",
    ")\n",
    "f[cams[1]].append_features_from_numpy(\n",
    "    x=matcher.mkpts1[:, 0],\n",
    "    y=matcher.mkpts1[:, 1],\n",
    "    descr=matcher.descriptors1,\n",
    "    scores=matcher.scores1,\n",
    ")\n",
    "\n",
    "# Store the dictionary with the features in the Epoch object\n",
    "epoch.features = f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scene reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, perform Relative orientation of the two cameras by using the matched features and the a-priori camera interior orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Initialize RelativeOrientation class with a list containing the two\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# cameras and a list contaning the matched features location on each camera.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m relative_ori \u001b[39m=\u001b[39m sfm\u001b[39m.\u001b[39mRelativeOrientation(\n\u001b[1;32m      4\u001b[0m     [epoch\u001b[39m.\u001b[39mcameras[cams[\u001b[39m0\u001b[39m]], epoch\u001b[39m.\u001b[39mcameras[cams[\u001b[39m1\u001b[39m]]],\n\u001b[1;32m      5\u001b[0m     [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     ],\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m relative_ori\u001b[39m.\u001b[39;49mestimate_pose(\n\u001b[1;32m     11\u001b[0m     threshold\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mmatching\u001b[39m.\u001b[39;49mpydegensac_threshold,\n\u001b[1;32m     12\u001b[0m     confidence\u001b[39m=\u001b[39;49m\u001b[39m0.999999\u001b[39;49m,\n\u001b[1;32m     13\u001b[0m     scale_factor\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49mnorm(\n\u001b[1;32m     14\u001b[0m         cfg\u001b[39m.\u001b[39;49mgeoref\u001b[39m.\u001b[39;49mcamera_centers_world[\u001b[39m0\u001b[39;49m] \u001b[39m-\u001b[39;49m cfg\u001b[39m.\u001b[39;49mgeoref\u001b[39m.\u001b[39;49mcamera_centers_world[\u001b[39m1\u001b[39;49m]\n\u001b[1;32m     15\u001b[0m     ),\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[39m# Store result in camera 1 object\u001b[39;00m\n\u001b[1;32m     18\u001b[0m epoch\u001b[39m.\u001b[39mcameras[cams[\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m relative_ori\u001b[39m.\u001b[39mcameras[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/phd/icepy4d/src/icepy4d/sfm/two_view_geometry.py:81\u001b[0m, in \u001b[0;36mRelativeOrientation.estimate_pose\u001b[0;34m(self, threshold, confidence, scale_factor)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcameras[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mextrinsics \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39mprint\u001b[39m(\n\u001b[1;32m     76\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mExtrinsics matrix is not available for camera 0. Please, compute it before running RelativeOrientation estimation.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m )\n\u001b[1;32m     79\u001b[0m \u001b[39m# Estimate Realtive Pose with Essential Matrix\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39m# R, t make up a tuple that performs a change of basis from the first camera's coordinate system to the second camera's coordinate system.\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m R, t, valid \u001b[39m=\u001b[39m estimate_pose(\n\u001b[1;32m     82\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures[\u001b[39m0\u001b[39m],\n\u001b[1;32m     83\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures[\u001b[39m1\u001b[39m],\n\u001b[1;32m     84\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcameras[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mK,\n\u001b[1;32m     85\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcameras[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mK,\n\u001b[1;32m     86\u001b[0m     thresh\u001b[39m=\u001b[39mthreshold,\n\u001b[1;32m     87\u001b[0m     conf\u001b[39m=\u001b[39mconfidence,\n\u001b[1;32m     88\u001b[0m )\n\u001b[1;32m     89\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRelative Orientation - valid points: \u001b[39m\u001b[39m{\u001b[39;00mvalid\u001b[39m.\u001b[39msum()\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(valid)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     91\u001b[0m \u001b[39m# If the scaling factor is given, scale the stereo model\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# Initialize RelativeOrientation class with a list containing the two\n",
    "# cameras and a list contaning the matched features location on each camera.\n",
    "relative_ori = sfm.RelativeOrientation(\n",
    "    [epoch.cameras[cams[0]], epoch.cameras[cams[1]]],\n",
    "    [\n",
    "        epoch.features[cams[0]].kpts_to_numpy(),\n",
    "        epoch.features[cams[1]].kpts_to_numpy(),\n",
    "    ],\n",
    ")\n",
    "relative_ori.estimate_pose(\n",
    "    threshold=cfg.matching.pydegensac_threshold,\n",
    "    confidence=0.999999,\n",
    "    scale_factor=np.linalg.norm(\n",
    "        cfg.georef.camera_centers_world[0] - cfg.georef.camera_centers_world[1]\n",
    "    ),\n",
    ")\n",
    "# Store result in camera 1 object\n",
    "epoch.cameras[cams[1]] = relative_ori.cameras[1]\n",
    "\n",
    "cfg.georef.camera_centers_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_baseline = np.linalg.norm(\n",
    "        cfg.georef.camera_centers_world[0] - cfg.georef.camera_centers_world[1]\n",
    "    )\n",
    "image = epoch.images[cams[0]].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m feature1 \u001b[39m=\u001b[39m epoch\u001b[39m.\u001b[39mfeatures[cams[\u001b[39m1\u001b[39m]]\u001b[39m.\u001b[39mkpts_to_numpy()\n\u001b[1;32m      5\u001b[0m relative_ori \u001b[39m=\u001b[39m sfm\u001b[39m.\u001b[39mRelativeOrientation(\n\u001b[1;32m      6\u001b[0m     [epoch\u001b[39m.\u001b[39mcameras[cams[\u001b[39m0\u001b[39m]], epoch\u001b[39m.\u001b[39mcameras[cams[\u001b[39m1\u001b[39m]]],\n\u001b[1;32m      7\u001b[0m     [feature0, feature1],\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m relative_ori\u001b[39m.\u001b[39;49mestimate_pose(\n\u001b[1;32m     10\u001b[0m     scale_factor\u001b[39m=\u001b[39;49mcamera_baseline,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m epoch\u001b[39m.\u001b[39mcameras[cams[\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m relative_ori\u001b[39m.\u001b[39mcameras[\u001b[39m1\u001b[39m]\n\u001b[1;32m     14\u001b[0m \u001b[39m# Traingulation\u001b[39;00m\n",
      "File \u001b[0;32m~/phd/icepy4d/src/icepy4d/sfm/two_view_geometry.py:81\u001b[0m, in \u001b[0;36mRelativeOrientation.estimate_pose\u001b[0;34m(self, threshold, confidence, scale_factor)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcameras[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mextrinsics \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39mprint\u001b[39m(\n\u001b[1;32m     76\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mExtrinsics matrix is not available for camera 0. Please, compute it before running RelativeOrientation estimation.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m )\n\u001b[1;32m     79\u001b[0m \u001b[39m# Estimate Realtive Pose with Essential Matrix\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39m# R, t make up a tuple that performs a change of basis from the first camera's coordinate system to the second camera's coordinate system.\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m R, t, valid \u001b[39m=\u001b[39m estimate_pose(\n\u001b[1;32m     82\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures[\u001b[39m0\u001b[39m],\n\u001b[1;32m     83\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures[\u001b[39m1\u001b[39m],\n\u001b[1;32m     84\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcameras[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mK,\n\u001b[1;32m     85\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcameras[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mK,\n\u001b[1;32m     86\u001b[0m     thresh\u001b[39m=\u001b[39mthreshold,\n\u001b[1;32m     87\u001b[0m     conf\u001b[39m=\u001b[39mconfidence,\n\u001b[1;32m     88\u001b[0m )\n\u001b[1;32m     89\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRelative Orientation - valid points: \u001b[39m\u001b[39m{\u001b[39;00mvalid\u001b[39m.\u001b[39msum()\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(valid)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     91\u001b[0m \u001b[39m# If the scaling factor is given, scale the stereo model\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# Relative orientation\n",
    "feature0 = epoch.features[cams[0]].kpts_to_numpy()\n",
    "feature1 = epoch.features[cams[1]].kpts_to_numpy()\n",
    "\n",
    "relative_ori = sfm.RelativeOrientation(\n",
    "    [epoch.cameras[cams[0]], epoch.cameras[cams[1]]],\n",
    "    [feature0, feature1],\n",
    ")\n",
    "relative_ori.estimate_pose(scale_factor=camera_baseline)\n",
    "epoch.cameras[cams[1]] = relative_ori.cameras[1]\n",
    "\n",
    "# Triangulation\n",
    "triang = sfm.Triangulate(\n",
    "    [epoch.cameras[cams[0]], epoch.cameras[cams[1]]],\n",
    "    [feature0, feature1],\n",
    ")\n",
    "points3d = triang.triangulate_two_views(\n",
    "    compute_colors=True, image=image, cam_id=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Triangulate points into the object space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;37m2023-08-29 17:34:37 | [INFO    ] Point triangulation succeded: 1.0.\u001b[0m\n",
      "\u001b[0;37m2023-08-29 17:34:37 | [INFO    ] Point colors interpolated\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "triang = sfm.Triangulate(\n",
    "    [epoch.cameras[cams[0]], epoch.cameras[cams[1]]],\n",
    "    [\n",
    "        epoch.features[cams[0]].kpts_to_numpy(),\n",
    "        epoch.features[cams[1]].kpts_to_numpy(),\n",
    "    ],\n",
    ")\n",
    "points3d = triang.triangulate_two_views(\n",
    "    compute_colors=True, image=images[cams[1]].read_image(ep).value, cam_id=1\n",
    ")\n",
    "\n",
    "# Update timer\n",
    "timer.update(\"triangulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform an absolute orientation of the current solution (i.e., cameras' exterior orientation and 3D points) by using the ground control points.\n",
    "\n",
    "The coordinates of the two cameras are used as additional ground control points for estimating a Helmert transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m2023-08-29 17:34:18 | [WARNING ] Warning: target T2 is not present on camera 0.\u001b[0m\n",
      "\u001b[1;33m2023-08-29 17:34:18 | [WARNING ] Warning: target F10_2 is not present on camera 0.\u001b[0m\n",
      "\u001b[1;33m2023-08-29 17:34:18 | [WARNING ] Warning: target T2 is not present on camera 1.\u001b[0m\n",
      "\u001b[1;33m2023-08-29 17:34:18 | [WARNING ] Warning: target F10_2 is not present on camera 1.\u001b[0m\n",
      "\u001b[1;33m2023-08-29 17:34:18 | [WARNING ] Not all targets found. Using onlys ['F2', 'F12', 'F13']\u001b[0m\n",
      "\u001b[0;37m2023-08-29 17:34:18 | [INFO    ] Point triangulation succeded: 1.0.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Get targets available in all cameras. The Labels of valid targets are returned as second element by the get_image_coor_by_label() method\n",
    "valid_targets = epoch.targets.get_image_coor_by_label(\n",
    "    cfg.georef.targets_to_use, cam_id=0\n",
    ")[1]\n",
    "\n",
    "# Check if the same targets are available in all cameras\n",
    "for id in range(1, len(cams)):\n",
    "    assert (\n",
    "        valid_targets\n",
    "        == epoch.targets.get_image_coor_by_label(\n",
    "            cfg.georef.targets_to_use, cam_id=id\n",
    "        )[1]\n",
    "    ), f\"\"\"epoch {ep} - {epoch_dict[ep]}: \n",
    "    Different targets found in image {id} - {images[cams[id]][ep]}\"\"\"\n",
    "\n",
    "# Check if there are enough targets\n",
    "assert len(valid_targets) > 1, f\"Not enough targets found in epoch {ep}\"\n",
    "\n",
    "# If not all the targets defined in the config file are found, log a warning and use only the valid targets\n",
    "if valid_targets != cfg.georef.targets_to_use:\n",
    "    logger.warning(f\"Not all targets found. Using onlys {valid_targets}\")\n",
    "\n",
    "# Get image and object coordinates of valid targets\n",
    "image_coords = [\n",
    "    epoch.targets.get_image_coor_by_label(valid_targets, cam_id=id)[0]\n",
    "    for id, cam in enumerate(cams)\n",
    "]\n",
    "obj_coords = epoch.targets.get_object_coor_by_label(valid_targets)[0]\n",
    "\n",
    "# Perform absolute orientation\n",
    "abs_ori = sfm.Absolute_orientation(\n",
    "    (epoch.cameras[cams[0]], epoch.cameras[cams[1]]),\n",
    "    points3d_final=obj_coords,\n",
    "    image_points=image_coords,\n",
    "    camera_centers_world=cfg.georef.camera_centers_world,\n",
    ")\n",
    "T = abs_ori.estimate_transformation_linear(estimate_scale=True)\n",
    "points3d = abs_ori.apply_transformation(points3d=points3d)\n",
    "for i, cam in enumerate(cams):\n",
    "    epoch.cameras[cam] = abs_ori.cameras[i]\n",
    "\n",
    "# Convert the 3D points to an icepy4d Points object\n",
    "pts = icepy4d_classes.Points()\n",
    "pts.append_points_from_numpy(\n",
    "    points3d,\n",
    "    track_ids=epoch.features[cams[0]].get_track_ids(),\n",
    "    colors=triang.colors,\n",
    ")\n",
    "\n",
    "# Store the points in the Epoch object\n",
    "epoch.points = pts\n",
    "\n",
    "# Update timer\n",
    "timer.update(\"absolute orientation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the current Epoch object as a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;37m2023-08-29 18:32:30 | [INFO    ] 2022-05-01_14:01:15 saved successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Save epoch as a pickle object\n",
    "if epoch.save_pickle(f\"{epoch.epoch_dir}/{epoch}.pickle\"):\n",
    "    logger.info(f\"{epoch} saved successfully\")\n",
    "else:\n",
    "    logger.error(f\"Unable to save {epoch}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icepy4d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
