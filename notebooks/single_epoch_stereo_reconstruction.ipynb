{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stereo reconstruction\n",
    "\n",
    "This notebook introduce you to the procedure of building a 3D reconstruction from two stereo cameras. The procedure is as follows:\n",
    "1. Create a `Epoch` object that stores all the relevant information for the 3D reconstruction. This includes:\n",
    "   * Load the images from the two cameras taken at the same time\n",
    "   * Create the camera objects given the pre-calibrated intrinsic parameters\n",
    "   * Load information about the targets\n",
    "2. Detect and match features on the two images\n",
    "3. Perform a relative orientation between the two cameras based on the matched tie points (optionally, you can also set the scale of the reconstruction, e.g., by giving the camera baseline)\n",
    "4. Triangulate the tie points into the object space\n",
    "5. Perform an absolute orientation to refine the camera positions and the 3D points\n",
    "6. (optional) Perform a bundle adjustment to refine the camera positions and the 3D points by using Agisoft Metashape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first set up the python environment by importing the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import required standard modules\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Import required icepy4d4D modules\n",
    "from icepy4d import core as icecore\n",
    "from icepy4d.core import Epoch\n",
    "from icepy4d import matching\n",
    "from icepy4d import sfm\n",
    "from icepy4d import io\n",
    "from icepy4d import utils\n",
    "from icepy4d.metashape import metashape as MS\n",
    "from icepy4d.utils import initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you have to define the path to the configuration file (`.yaml` file).\n",
    "This file contains all the paths and parameters needed to run the code.\n",
    "See the `config.yaml` file in the nootebook folder for an example and refer to the documentation for how to prepare all the data for ICEpy4D.  \n",
    "Additionally, you can setup a logger for the code to print out some information and a timer to measure the runtime of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================\n",
      "ICEpy4D\n",
      "Image-based Continuos monitoring of glaciers' Evolution with low-cost stereo-cameras and Deep Learning photogrammetry\n",
      "2023 - Francesco Ioli - francesco.ioli@polimi.it\n",
      "================================================================\n",
      "\n",
      "\u001b[0;37m2023-10-03 10:41:41 | [INFO    ] Configuration file: config.yaml\u001b[0m\n",
      "\u001b[0;37m2023-10-03 10:41:41 | [INFO    ] Epoch_to_process set to a pair of values. Expanding it for a range of epoches from epoch 0 to 158.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Parse the configuration file\n",
    "CFG_FILE = \"config.yaml\"\n",
    "\n",
    "# Parse the configuration file\n",
    "cfg_file = Path(CFG_FILE)\n",
    "cfg = initialization.parse_cfg(cfg_file, ignore_errors=True)\n",
    "\n",
    "# Initialize the logger\n",
    "logger = utils.get_logger()\n",
    "\n",
    "# Initialize a timer to measure the processing time\n",
    "timer = utils.AverageTimer()\n",
    "\n",
    "# Get the list of cameras from the configuration file\n",
    "cams = cfg.cams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch\n",
    "\n",
    "An Epoch object is the main object that stores all the information about the 3D reconstruction. It is initialized by defining the timestamp of the epoch and, optionally, by storing the other information about the epoch (e.g., images, cameras, tie points etc...).\n",
    "\n",
    "Let's first define a dictionary that contains the information about all the images that we are going to process. This dictionary has the camera name as key and an `Image` object as value. The `Image` object is an ICEpy4D object that manage the image in a lazy way (i.e., it doesn't load the image into memory, but read only image metadata). An image object is initialized by passing the path to the image to the constructor, and it automatically reads the image timestamp and other exif metadata.\n",
    "\n",
    "```python\n",
    "image = Image('path_to_image')\n",
    "```\n",
    "To get the image timestamp, you can access the `timestamp` attribute of the image object.\n",
    "\n",
    "```python\n",
    "image.timestamp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dictionary of images containing the name of the cameras as keys and Image objects as values\n",
    "im_epoch = {\n",
    "    cams[0]: icecore.Image(\"../data/img/p1/IMG_2637.jpg\"),\n",
    "    cams[1]: icecore.Image(\"../data/img/p2/IMG_1112.jpg\")\n",
    "}\n",
    "\n",
    "# Get epoch timestamp as the timestamp of the first image and define epoch directory\n",
    "epoch_timestamp = im_epoch[cams[0]].timestamp\n",
    "epochdir = cfg.paths.results_dir / epoch_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to load/define all the other information for the 3D reconstruction. \n",
    "In particular, this includes the camera objects (by loading their pre-calibrated intrinsics orientation) and the GCPs (by loading their coordinates in the object space and their projections in the image space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cameras\n",
    "cams_ep = {}\n",
    "for cam in cams:\n",
    "    calib = icecore.Calibration(cfg.paths.calibration_dir / f\"{cam}.txt\")\n",
    "    cams_ep[cam] = calib.to_camera()\n",
    "\n",
    "# Load targets\n",
    "target_paths = [\n",
    "    cfg.georef.target_dir\n",
    "    / (im_epoch[cam].stem + cfg.georef.target_file_ext)\n",
    "    for cam in cams\n",
    "]\n",
    "targ_ep = icecore.Targets(\n",
    "    im_file_path=target_paths,\n",
    "    obj_file_path=cfg.georef.target_dir\n",
    "    / cfg.georef.target_world_file,\n",
    ")\n",
    "\n",
    "# Create empty features\n",
    "feat_ep = {cam: icecore.Features() for cam in cams}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create an Epoch object by passing all the information that we have previously defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2022-05-01_14-01-15\n"
     ]
    }
   ],
   "source": [
    "# Create the epoch object\n",
    "epoch = Epoch(\n",
    "    timestamp=epoch_timestamp,\n",
    "    images=im_epoch,\n",
    "    cameras=cams_ep,\n",
    "    features=feat_ep,\n",
    "    targets=targ_ep,\n",
    "    epoch_dir=epochdir,\n",
    ")\n",
    "print(f\"Epoch: {epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information stored inside the epoch object can be easily accessed as attributes of the object. For example, to get the camera objects, you can access the `cameras` attribute of the epoch object.\n",
    "\n",
    "```python\n",
    "epoch.timestamp # Gives the timestamp of the epoch\n",
    "epoch.cameras # Gives a dictionary with the camera name as key and the camera object as value\n",
    "epoch.images # GIves a dictionary with the camera name as key and the image object as value\n",
    "epoch.features # Gives a dictionary with the camera name as key and the features object as value\n",
    "epoch.targets # Gives a Target object containing both the coordinates in the object space and the projections in the image space\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01 14:01:15\n",
      "{'p1': Camera (f=6621.743457206283, img_size=(6012.0, 4008.0), 'p2': Camera (f=9267.892627662095, img_size=(6012.0, 4008.0)}\n",
      "{'p1': Image ../data/img/p1/IMG_2637.jpg, 'p2': Image ../data/img/p2/IMG_1112.jpg}\n",
      "{'p1': Features with 0 features, 'p2': Features with 0 features}\n",
      "<icepy4d.core.targets.Targets object at 0x7f915b22b4f0>\n"
     ]
    }
   ],
   "source": [
    "print(epoch.timestamp)\n",
    "print(epoch.cameras)\n",
    "print(epoch.images)\n",
    "print(epoch.features)\n",
    "print(epoch.targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stereo Processing\n",
    "The stereo processing is carried out for each epoch in order to find matched features, estimating camera pose, and triangulating the 3D points. \n",
    "The output of this step is a set of 3D points and their corresponding descriptors.\n",
    "\n",
    "The same procedure is then iterated in a big loop for all the epoches in a multitemporal processing (refer to the `multitemporal_workflow.ipynb` notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature matching with LightGlue\n",
    "\n",
    "Wide-baseline feature matching is performed using the LightGlue algorithm.\n",
    "Refer to the `matching.ipynb` notebook for more details about the matching process and explanation of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;37m2023-10-03 10:30:44 | [INFO    ] Running inference on device cuda\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;37m2023-10-03 10:30:45 | [INFO    ] Matching by tiles...\u001b[0m\n",
      "\u001b[0;37m2023-10-03 10:30:45 | [INFO    ] Matching tiles by preselection tile selection\u001b[0m\n",
      "\u001b[0;37m2023-10-03 10:30:46 | [INFO    ]  - Matching tile pair (1, 1)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 10:30:48 | [INFO    ]  - Matching tile pair (1, 4)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 10:30:49 | [INFO    ]  - Matching tile pair (2, 4)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 10:30:50 | [INFO    ]  - Matching tile pair (2, 5)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 10:30:52 | [INFO    ]  - Matching tile pair (3, 3)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 10:30:53 | [INFO    ]  - Matching tile pair (4, 3)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 10:30:54 | [INFO    ]  - Matching tile pair (4, 4)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 10:30:56 | [INFO    ]  - Matching tile pair (5, 4)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 10:30:57 | [INFO    ]  - Matching tile pair (5, 5)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 10:30:58 | [INFO    ] Restoring full image coordinates of matches...\u001b[0m\n",
      "\u001b[0;37m2023-10-03 10:30:58 | [INFO    ] Matching by tile completed.\u001b[0m\n",
      "\u001b[0;37m2023-10-03 10:30:58 | [INFO    ] Matching done!\u001b[0m\n",
      "\u001b[0;37m2023-10-03 10:30:58 | [INFO    ] Performing geometric verification...\u001b[0m\n",
      "\u001b[0;37m2023-10-03 10:30:59 | [INFO    ] Pydegensac found 1061 inliers (36.49%)\u001b[0m\n",
      "\u001b[0;37m2023-10-03 10:30:59 | [INFO    ] Geometric verification done.\u001b[0m\n",
      "\u001b[0;37m2023-10-03 10:31:00 | [INFO    ] [Timer] | [Matching] preselection=1.626, matching=12.153, geometric_verification=0.146, \u001b[0m\n",
      "Function match took 15.1186 seconds\n"
     ]
    }
   ],
   "source": [
    "matcher = matching.LightGlueMatcher()\n",
    "matcher.match(\n",
    "    epoch.images[cams[0]].value,\n",
    "    epoch.images[cams[1]].value,\n",
    "    quality=matching.Quality.HIGH,\n",
    "    tile_selection= matching.TileSelection.PRESELECTION,\n",
    "    grid=[2, 3],\n",
    "    overlap=200,\n",
    "    origin=[0, 0],\n",
    "    do_viz_matches=True,\n",
    "    do_viz_tiles=True,\n",
    "    min_matches_per_tile = 3,\n",
    "    max_keypoints = 8196,    \n",
    "    save_dir=epoch.epoch_dir / \"matching\",\n",
    "    geometric_verification=matching.GeometricVerification.PYDEGENSAC,\n",
    "    threshold=2,\n",
    "    confidence=0.9999,\n",
    ")\n",
    "timer.update(\"matching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now extract the matched features from the Matcher object and save them in the current Epoch object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary with empty Features objects for each camera, which will be filled with the matched keypoints, descriptors and scores\n",
    "f = {cam: icecore.Features() for cam in cams}\n",
    "\n",
    "# Stack matched keypoints, descriptors and scores into Features objects\n",
    "f[cams[0]].append_features_from_numpy(\n",
    "    x=matcher.mkpts0[:, 0],\n",
    "    y=matcher.mkpts0[:, 1],\n",
    "    descr=matcher.descriptors0,\n",
    "    scores=matcher.scores0,\n",
    ")\n",
    "f[cams[1]].append_features_from_numpy(\n",
    "    x=matcher.mkpts1[:, 0],\n",
    "    y=matcher.mkpts1[:, 1],\n",
    "    descr=matcher.descriptors1,\n",
    "    scores=matcher.scores1,\n",
    ")\n",
    "\n",
    "# Store the dictionary with the features in the Epoch object\n",
    "epoch.features = f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Scene reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relative orientation\n",
    "\n",
    "First, perform Relative orientation of the two cameras by using the matched features and the a-priori camera interior orientation.\n",
    "\n",
    "To perform the relative orientation, you have to define a `RelativeOrientation` object by passing first a list containing the two camera objects and then a list containing the matched features on each image. The matched features are Nx2 numpy arrary containing the x-y pixel coordinates of the matched features.\n",
    "\n",
    "```python\n",
    "relative_orientation = RelativeOrientation([camera1, camera2], [features1, features2])\n",
    "```\n",
    "\n",
    "To get the pixel coordinates of the matched features as numpy arrays you can use the `kpts_to_numpy()` method of a Features object (that is now stored into the current Epoch object). \n",
    "\n",
    "```python\n",
    "epoch.features[cams[0]].kpts_to_numpy()\n",
    "```\n",
    "\n",
    "The relative orientation is then performed by calling the `estimate_pose` method of the object.\n",
    "You can pass some additional parameters such as the camera baseline (in meters) to scale the reconstruction.\n",
    "\n",
    "```python\n",
    "relative_orientation.estimate_pose(scale_factor=camera_baseline)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;37m2023-10-03 10:33:11 | [INFO    ] Relative Orientation - valid points: 963/1061\u001b[0m\n",
      "\u001b[0;37m2023-10-03 10:33:11 | [INFO    ] Relative orientation Succeded.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Compute the camera baseline from a-priori camera positions\n",
    "baseline = np.linalg.norm(\n",
    "    cfg.georef.camera_centers_world[0] - cfg.georef.camera_centers_world[1]\n",
    ")\n",
    "\n",
    "# Initialize RelativeOrientation class with a list containing the two\n",
    "# cameras and a list contaning the matched features location on each camera.\n",
    "relative_ori = sfm.RelativeOrientation(\n",
    "    [epoch.cameras[cams[0]], epoch.cameras[cams[1]]],\n",
    "    [\n",
    "        epoch.features[cams[0]].kpts_to_numpy(),\n",
    "        epoch.features[cams[1]].kpts_to_numpy(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Estimate the relative orientation\n",
    "relative_ori.estimate_pose(\n",
    "    threshold=cfg.matching.pydegensac_threshold,\n",
    "    confidence=0.999999,\n",
    "    scale_factor=baseline,\n",
    ")\n",
    "\n",
    "# Store result in camera 1 object\n",
    "epoch.cameras[cams[1]] = relative_ori.cameras[1]\n",
    "\n",
    "timer.update(\"relative orientation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Triangulation\n",
    "\n",
    "You can now triangulate the tie points (i.e., the matched features) into the object space.\n",
    "Similarly as before, you have to define a `Triangulation` object by passing first a list containing the two camera objects and then a list containing the matched features on each image. The matched features are Nx2 numpy arrary containing the x-y pixel coordinates of the matched features.\n",
    "\n",
    "```python\n",
    "triangulation = Triangulation([camera1, camera2], [features1, features2])\n",
    "```\n",
    "\n",
    "The triangulation is then performed by calling the `triangulate` method of the object. \n",
    "\n",
    "```python\n",
    "triangulation.triangulate()\n",
    "```\n",
    "\n",
    "You can decide if you want to compute the point colors by interpolating them from one of the two images. If so, you have to pass the index of the image that you want to use for the color interpolation. For example, to interpolate colors from image 1, you can do:\n",
    "\n",
    "```python\n",
    "triangulation.triangulate(compute_colors=True, image=epoch.images[cams[1]].value, cam_id=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;37m2023-10-03 10:40:02 | [INFO    ] Point triangulation succeded: 1.0.\u001b[0m\n",
      "\u001b[0;37m2023-10-03 10:40:02 | [INFO    ] Point colors interpolated\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "triang = sfm.Triangulate(\n",
    "    [epoch.cameras[cams[0]], epoch.cameras[cams[1]]],\n",
    "    [\n",
    "        epoch.features[cams[0]].kpts_to_numpy(),\n",
    "        epoch.features[cams[1]].kpts_to_numpy(),\n",
    "    ],\n",
    ")\n",
    "points3d = triang.triangulate_two_views(\n",
    "    compute_colors=True, image=epoch.images[cams[1]].value, cam_id=1\n",
    ")\n",
    "\n",
    "# Update timer\n",
    "timer.update(\"triangulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Absolute orientation\n",
    "\n",
    "Now, you can perform an absolute orientation of the current solution (i.e., cameras' exterior orientation and 3D points) by using the ground control points.\n",
    "\n",
    "The coordinates of the two cameras are used as additional ground control points for estimating a Helmert transformation.\n",
    "\n",
    "You need first to extract the image and object coordinates of the targets from the Target object, stored into the current Epoch. Alternatively, you can also define manually the coordinates of the targets as numpy arrays.\n",
    "To extract the image coordinates of the targets, you can use the `get_image_coor_by_label()` method of the Target object, by passing the list of the target labels that you want to extract and the camera id they are referred to.\n",
    "\n",
    "```python\n",
    "    epoch.targets.get_image_coor_by_label(cfg.georef.targets_to_use, cam_id=id)\n",
    "```\n",
    "\n",
    "To get the targets object coordinates, you can use the `get_object_coor_by_label()` method of the Target object, by passing the list of the target labels that you want to extract.\n",
    "\n",
    "```python\n",
    "    epoch.targets.get_object_coor_by_label(cfg.georef.targets_to_use)\n",
    "```\n",
    "\n",
    "Eventually, you need to have 3 numpy array, all with the number of rows (the number of targets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets coordinates on image 0:\n",
      "[[4002.709  3543.0627]\n",
      " [1611.3804 1420.486 ]\n",
      " [4671.8179 3465.3052]]\n",
      "Targets coordinates on image 1:\n",
      "[[1003.5037 3859.1558]\n",
      " [5694.3535  620.8673]\n",
      " [1565.0667 3927.6331]]\n",
      "Targets coordinates in object space:\n",
      "[[  49.6488  192.0875   71.7466]\n",
      " [-532.7409  391.02    238.8015]\n",
      " [  51.1682  210.4649   70.9032]]\n"
     ]
    }
   ],
   "source": [
    "# Extract the image coordinates of the targets from the Targets object\n",
    "image_coords = [\n",
    "    epoch.targets.get_image_coor_by_label(cfg.georef.targets_to_use, cam_id=id)[0] for id, cam in enumerate(cams)\n",
    "]\n",
    "print(f\"Targets coordinates on image 0:\\n{image_coords[0]}\")\n",
    "print(f\"Targets coordinates on image 1:\\n{image_coords[1]}\")\n",
    "\n",
    "obj_coords = epoch.targets.get_object_coor_by_label(cfg.georef.targets_to_use)[0]\n",
    "print(f\"Targets coordinates in object space:\\n{obj_coords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now perform the absolute orientation in a similar way as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;37m2023-10-03 10:49:29 | [INFO    ] Point triangulation succeded: 1.0.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Initialize AbsoluteOrientation object with a list containing the two\n",
    "abs_ori = sfm.Absolute_orientation(\n",
    "    (epoch.cameras[cams[0]], epoch.cameras[cams[1]]),\n",
    "    points3d_final=obj_coords,\n",
    "    image_points=image_coords,\n",
    "    camera_centers_world=cfg.georef.camera_centers_world,\n",
    ")\n",
    "\n",
    "# Estimate the absolute orientation transformation\n",
    "T = abs_ori.estimate_transformation_linear(estimate_scale=True)\n",
    "\n",
    "# Transform the 3D points\n",
    "points3d = abs_ori.apply_transformation(points3d=points3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now save the estimated camera positions and the 3D points into the current Epoch object. The 3D coordinates of the points in the object space can be saved as a ICEpy4D Points object, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the absolute orientation transformation in the camera objects\n",
    "for i, cam in enumerate(cams):\n",
    "    epoch.cameras[cam] = abs_ori.cameras[i]\n",
    "\n",
    "# Convert the 3D points to an icepy4d Points object\n",
    "pts = icecore.Points()\n",
    "pts.append_points_from_numpy(\n",
    "    points3d,\n",
    "    track_ids=epoch.features[cams[0]].get_track_ids(),\n",
    "    colors=triang.colors,\n",
    ")\n",
    "\n",
    "# Store the points in the Epoch object\n",
    "epoch.points = pts\n",
    "\n",
    "# Update timer\n",
    "timer.update(\"absolute orientation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save the current Epoch object as a pickle file in the previously defined epoch directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;37m2023-10-03 10:50:49 | [INFO    ] 2022-05-01_14-01-15 saved successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Save epoch as a pickle object\n",
    "if epoch.save_pickle(f\"{epoch.epoch_dir}/{epoch}.pickle\"):\n",
    "    logger.info(f\"{epoch} saved successfully\")\n",
    "else:\n",
    "    logger.error(f\"Unable to save {epoch}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icepy4d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
