{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stereo reconstruction in single epoch with 2 different cameras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first set up the python environment by importing the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import required standard modules\n",
    "import shutil\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Import required icepy4d4D modules\n",
    "from icepy4d import core as icecore\n",
    "from icepy4d.core import Epoch, Epoches, EpochDataMap\n",
    "from icepy4d import matching\n",
    "from icepy4d import sfm\n",
    "from icepy4d import io\n",
    "from icepy4d import utils as icepy4d_utils\n",
    "from icepy4d.metashape import metashape as MS\n",
    "from icepy4d.utils import initialization as inizialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you have to define the path to the configuration file (`.yaml` file).\n",
    "This file contains all the paths and parameters needed to run the code.\n",
    "See the `config.yaml` file in the nootebook folder for an example and refer to the documentation for how to prepare all the data for ICEpy4D.  \n",
    "Additionally, you can setup a logger for the code to print out some information and a timer to measure the runtime of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================\n",
      "ICEpy4D\n",
      "Image-based Continuos monitoring of glaciers' Evolution with low-cost stereo-cameras and Deep Learning photogrammetry\n",
      "2023 - Francesco Ioli - francesco.ioli@polimi.it\n",
      "================================================================\n",
      "\n",
      "\u001b[0;37m2023-09-19 10:33:43 | [INFO    ] Configuration file: config.yaml\u001b[0m\n",
      "\u001b[0;37m2023-09-19 10:33:43 | [INFO    ] Epoch_to_process set to a pair of values. Expanding it for a range of epoches from epoch 0 to 158.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Parse the configuration file\n",
    "CFG_FILE = \"config.yaml\"\n",
    "\n",
    "# Parse the configuration file\n",
    "cfg_file = Path(CFG_FILE)\n",
    "cfg = inizialization.parse_cfg(cfg_file, ignore_errors=True)\n",
    "\n",
    "# Initialize the logger\n",
    "logger = icepy4d_utils.get_logger()\n",
    "\n",
    "# Initialize a timer to measure the processing time\n",
    "timer = icepy4d_utils.AverageTimer()\n",
    "\n",
    "# Get the list of cameras from the configuration file\n",
    "cams = cfg.cams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICEpy4D main structures\n",
    "\n",
    "For the processing, you have to initialize all the required variables. This procedure is the same also for multi-epoch processing.\n",
    "\n",
    "\n",
    "### EpochDataMap\n",
    "The `EpochDataMap` class is a critical structure for multi-epoch processing in ICEpy4D. It helps organize and manage data for each epoch, including timestamps, associated images, and time differences.\n",
    "`EpochDataMap` is a dictionary that contains the information the timestamp (i.e., date and time) of each epoch and the images belonging to that epoch.\n",
    "The purpose of this class is to give a east-to-use tool for associating the timestamp of each epoch to the images belonging to that epoch.\n",
    "\n",
    "The `EpochDataMap` allows for automatically define the association of the images to the epochs, based on the timestamp of the images. \n",
    "The images are associated to the epoch with the closest timestamp. The `EpochDataMap` first select the master camera (by default, the first camera). Then, for each image of the master camera, it looks for the closest image taken by all the other cameras (i.e., named 'slave cameras') based on their timestamp. As the cameras may not be perfectly syncronized, a time tolerance can be defined to allow for a maximum time difference between the images of different cameras (by default, 180 seconds). If the time difference between a slave image and the master image is larger than the time tolerance, the slave image is not associated to the current epoch.\n",
    "It may be possible that one epoch contains only the image of the master camera (e.g., due to a disruption of the other cameras during that day), therefore a minimum number of images can be defined for an epoch to be included in the `EpochDataMap` (by default, 2 images). If less than the minimum number of images are associated to an epoch, the epoch is discarded.\n",
    "\n",
    "1. The EpochDataMap structure\n",
    "\n",
    "    The key of the dictionary is the epoch number (an integer number, starting from 0). The values of the dictionary are the `EpochData` class are again dictionaries that contains the timestamp of the epoch, the images associated at that epoch and the time difference between the timestamp of each camera and that of the master camera (by default, the first camera).\n",
    "    For instance, a value of the `EpochDataMap` dictionary is the following:\n",
    "\n",
    "    ```python\n",
    "    epoch_data_map[0] = {\n",
    "        'timestamp': datetime.datetime(2023, 6, 23, 9, 59, 58), # timestamp of the first epoch\n",
    "        'images': { \n",
    "            'p1': Image data/img/p1/p1_20230623_095958_IMG_0845.JPG,\n",
    "            'p2': Image data/img/p2/p2_20230623_100000_IMG_0582.JPG\n",
    "            }, # images of the first epoch\n",
    "        'dt': {'p1': datetime.timedelta(0), 'p2': datetime.timedelta(seconds=2)} # time difference between each camera and the master camera\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    For accessing the data inside the dictionary for each epoch, you can use the `dot notation` for getting the timestamp, the image dictionary and the time differences.\n",
    "\n",
    "    ``` python\n",
    "    epoch_map[0].timestamp \n",
    "    epoch_map[0].images \n",
    "    epoch_map[0].dt \n",
    "    ```\n",
    "\n",
    "    The epoch timestamp (`epoch_map[n].timestamp`) is taken from the timestamp of the master camera, and it is stored as a datetime object.\n",
    "\n",
    "    ``` \n",
    "    epoch_map[0].timestamp = datetime.datetime(2023, 6, 23, 9, 59, 58)\n",
    "    ```\n",
    "\n",
    "    The images of each epoch is again a dictionary with the camera names (i.e., the name of the folder containing the image sequence) as keys and Image objects as values.\n",
    "\n",
    "    ```python\n",
    "    epoch_map[0].images['p1'] = Image(\"data/img/p1/p1_20230623_095958_IMG_0845.JPG\")\n",
    "    epoch_map[0].images['p2'] = Image(\"data/img/p2/p2_20230623_100000_IMG_0582.JPG\")\n",
    "    ```\n",
    "\n",
    "\n",
    "2. How to inizialize a EpochDataMap\n",
    "\n",
    "    Initialize the `EpochDataMap` object by providing the image directory and optional parameters like the time tolerance (maximum time difference allowed between images from different cameras) and minimum number of images required for an epoch to be included.\n",
    "\n",
    "    ```python\n",
    "    epoch_map = EpochDataMap('path_to_image_directory', time_tolerance_sec=180, min_images=2)\n",
    "    ```\n",
    "\n",
    "    If not specified, the time tolerance is set to 180 seconds and the minimum number of images is set to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;37m2023-09-19 10:33:58 | [INFO    ] Building EpochDataMap: found 158 epochs\u001b[0m\n",
      "\u001b[0;37m2023-09-19 10:33:58 | [INFO    ] Mean max dt: 590.37 seconds (max: 709.00 seconds))\u001b[0m\n",
      "\u001b[0;37m2023-09-19 10:33:58 | [INFO    ] Removed 0 epochs with less than 2 images\u001b[0m\n",
      "Epoch 0  Timestamp: 2022-05-01 14:01:15\n",
      "\t Images: {'p1': Image /home/francesco/phd/icepy4d/notebooks/../data/img/p1/IMG_2637.jpg, 'p2': Image /home/francesco/phd/icepy4d/notebooks/../data/img/p2/IMG_1112.jpg}\n",
      "\t Delta t from master camera (Cam 0): {'p1': datetime.timedelta(0), 'p2': datetime.timedelta(seconds=464)}\n"
     ]
    }
   ],
   "source": [
    "# Build the EpochDataMap object find pairs of coheval images for each epoch\n",
    "epoch_map = EpochDataMap(cfg.paths.image_dir, time_tolerance_sec=1200)\n",
    "\n",
    "print(f\"Epoch 0  Timestamp: {epoch_map[0].timestamp}\")\n",
    "print(f\"\\t Images: {epoch_map[0].images}\")\n",
    "print(f\"\\t Delta t from master camera (Cam 0): {epoch_map[0].dt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch\n",
    "\n",
    "Another crucial structure for multi-epoch processing is the `Epoch` class. It all the data belonging to each epoch and i\n",
    "\n",
    "#### Create a new Epoch object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2022-05-01_14-01-15\n"
     ]
    }
   ],
   "source": [
    "# Set id to process\n",
    "ep = cfg.proc.epoch_to_process[0]\n",
    "\n",
    "# Define paths to the epoch directory\n",
    "epoch_name = epoch_map.get_timestamp_str(ep)\n",
    "epochdir = cfg.paths.results_dir / epoch_name\n",
    "\n",
    "# Get the list of images for the current epoch\n",
    "im_epoch = epoch_map.get_images(ep)\n",
    "\n",
    "# Load cameras\n",
    "cams_ep = {}\n",
    "for cam in cams:\n",
    "    calib = icecore.Calibration(cfg.paths.calibration_dir / f\"{cam}.txt\")\n",
    "    cams_ep[cam] = calib.to_camera()\n",
    "\n",
    "# Load targets\n",
    "target_paths = [\n",
    "    cfg.georef.target_dir\n",
    "    / (im_epoch[cam].stem + cfg.georef.target_file_ext)\n",
    "    for cam in cams\n",
    "]\n",
    "targ_ep = icecore.Targets(\n",
    "    im_file_path=target_paths,\n",
    "    obj_file_path=cfg.georef.target_dir\n",
    "    / cfg.georef.target_world_file,\n",
    ")\n",
    "\n",
    "# Create empty features\n",
    "feat_ep = {cam: icecore.Features() for cam in cams}\n",
    "\n",
    "# Create the epoch object\n",
    "epoch = Epoch(\n",
    "    timestamp=epoch_map.get_timestamp_str(ep),\n",
    "    images=im_epoch,\n",
    "    cameras=cams_ep,\n",
    "    features=feat_ep,\n",
    "    targets=targ_ep,\n",
    "    epoch_dir=epochdir,\n",
    ")\n",
    "print(f\"Epoch: {epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stereo Processing\n",
    "The stereo processing is carried out for each epoch in order to find matched features, estimating camera pose, and triangulating the 3D points. \n",
    "The output of this step is a set of 3D points and their corresponding descriptors.\n",
    "\n",
    "The processing for all the epoches is then iterated in a big loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature matching with SuperGlue\n",
    "\n",
    "Wide-baseline feature matching is performed using the SuperGlue algorithm.\n",
    "Refer to the `matching.ipynb` notebook for more details about the matching process and explanation of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define matching parameters\n",
    "matching_quality = matching.Quality.HIGH\n",
    "tile_selection = matching.TileSelection.PRESELECTION\n",
    "tiling_grid = [4, 3]\n",
    "tiling_overlap = 200\n",
    "geometric_verification = matching.GeometricVerification.PYDEGENSAC\n",
    "geometric_verification_threshold = 1\n",
    "geometric_verification_confidence = 0.9999\n",
    "match_dir = epoch.epoch_dir / \"matching\"\n",
    "\n",
    "# Create a new matcher object\n",
    "matcher = matching.SuperGlueMatcher(cfg.matching)\n",
    "matcher.match(\n",
    "    epoch.images[cams[0]].value,\n",
    "    epoch.images[cams[1]].value,\n",
    "    quality=matching_quality,\n",
    "    tile_selection=tile_selection,\n",
    "    grid=tiling_grid,\n",
    "    overlap=tiling_overlap,\n",
    "    do_viz_matches=True,\n",
    "    do_viz_tiles=False,\n",
    "    save_dir=match_dir,\n",
    "    geometric_verification=geometric_verification,\n",
    "    threshold=geometric_verification_threshold,\n",
    "    confidence=geometric_verification_confidence,\n",
    ")\n",
    "timer.update(\"matching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the matched features from the Matcher object and save them in the current Epoch object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary with empty Features objects for each camera, which will be filled with the matched keypoints, descriptors and scores\n",
    "f = {cam: icecore.Features() for cam in cams}\n",
    "\n",
    "# Stack matched keypoints, descriptors and scores into Features objects\n",
    "f[cams[0]].append_features_from_numpy(\n",
    "    x=matcher.mkpts0[:, 0],\n",
    "    y=matcher.mkpts0[:, 1],\n",
    "    descr=matcher.descriptors0,\n",
    "    scores=matcher.scores0,\n",
    ")\n",
    "f[cams[1]].append_features_from_numpy(\n",
    "    x=matcher.mkpts1[:, 0],\n",
    "    y=matcher.mkpts1[:, 1],\n",
    "    descr=matcher.descriptors1,\n",
    "    scores=matcher.scores1,\n",
    ")\n",
    "\n",
    "# Store the dictionary with the features in the Epoch object\n",
    "epoch.features = f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scene reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, perform Relative orientation of the two cameras by using the matched features and the a-priori camera interior orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RelativeOrientation class with a list containing the two\n",
    "# cameras and a list contaning the matched features location on each camera.\n",
    "relative_ori = sfm.RelativeOrientation(\n",
    "    [epoch.cameras[cams[0]], epoch.cameras[cams[1]]],\n",
    "    [\n",
    "        epoch.features[cams[0]].kpts_to_numpy(),\n",
    "        epoch.features[cams[1]].kpts_to_numpy(),\n",
    "    ],\n",
    ")\n",
    "relative_ori.estimate_pose(\n",
    "    threshold=cfg.matching.pydegensac_threshold,\n",
    "    confidence=0.999999,\n",
    "    scale_factor=np.linalg.norm(\n",
    "        cfg.georef.camera_centers_world[0] - cfg.georef.camera_centers_world[1]\n",
    "    ),\n",
    ")\n",
    "# Store result in camera 1 object\n",
    "epoch.cameras[cams[1]] = relative_ori.cameras[1]\n",
    "\n",
    "cfg.georef.camera_centers_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_baseline = np.linalg.norm(\n",
    "        cfg.georef.camera_centers_world[0] - cfg.georef.camera_centers_world[1]\n",
    "    )\n",
    "image = epoch.images[cams[0]].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative orientation\n",
    "feature0 = epoch.features[cams[0]].kpts_to_numpy()\n",
    "feature1 = epoch.features[cams[1]].kpts_to_numpy()\n",
    "\n",
    "relative_ori = sfm.RelativeOrientation(\n",
    "    [epoch.cameras[cams[0]], epoch.cameras[cams[1]]],\n",
    "    [feature0, feature1],\n",
    ")\n",
    "relative_ori.estimate_pose(scale_factor=camera_baseline)\n",
    "epoch.cameras[cams[1]] = relative_ori.cameras[1]\n",
    "\n",
    "# Triangulation\n",
    "triang = sfm.Triangulate(\n",
    "    [epoch.cameras[cams[0]], epoch.cameras[cams[1]]],\n",
    "    [feature0, feature1],\n",
    ")\n",
    "points3d = triang.triangulate_two_views(\n",
    "    compute_colors=True, image=image, cam_id=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Triangulate points into the object space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triang = sfm.Triangulate(\n",
    "    [epoch.cameras[cams[0]], epoch.cameras[cams[1]]],\n",
    "    [\n",
    "        epoch.features[cams[0]].kpts_to_numpy(),\n",
    "        epoch.features[cams[1]].kpts_to_numpy(),\n",
    "    ],\n",
    ")\n",
    "points3d = triang.triangulate_two_views(\n",
    "    compute_colors=True, image=images[cams[1]].read_image(ep).value, cam_id=1\n",
    ")\n",
    "\n",
    "# Update timer\n",
    "timer.update(\"triangulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform an absolute orientation of the current solution (i.e., cameras' exterior orientation and 3D points) by using the ground control points.\n",
    "\n",
    "The coordinates of the two cameras are used as additional ground control points for estimating a Helmert transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get targets available in all cameras. The Labels of valid targets are returned as second element by the get_image_coor_by_label() method\n",
    "valid_targets = epoch.targets.get_image_coor_by_label(\n",
    "    cfg.georef.targets_to_use, cam_id=0\n",
    ")[1]\n",
    "\n",
    "# Check if the same targets are available in all cameras\n",
    "for id in range(1, len(cams)):\n",
    "    assert (\n",
    "        valid_targets\n",
    "        == epoch.targets.get_image_coor_by_label(\n",
    "            cfg.georef.targets_to_use, cam_id=id\n",
    "        )[1]\n",
    "    ), f\"\"\"epoch {ep} - {epoch_map.get_timestamp(ep)}: \n",
    "    Different targets found in image {id} - {images[cams[id]][ep]}\"\"\"\n",
    "\n",
    "# Check if there are enough targets\n",
    "assert len(valid_targets) > 1, f\"Not enough targets found in epoch {ep}\"\n",
    "\n",
    "# If not all the targets defined in the config file are found, log a warning and use only the valid targets\n",
    "if valid_targets != cfg.georef.targets_to_use:\n",
    "    logger.warning(f\"Not all targets found. Using onlys {valid_targets}\")\n",
    "\n",
    "# Get image and object coordinates of valid targets\n",
    "image_coords = [\n",
    "    epoch.targets.get_image_coor_by_label(valid_targets, cam_id=id)[0]\n",
    "    for id, cam in enumerate(cams)\n",
    "]\n",
    "obj_coords = epoch.targets.get_object_coor_by_label(valid_targets)[0]\n",
    "\n",
    "# Perform absolute orientation\n",
    "abs_ori = sfm.Absolute_orientation(\n",
    "    (epoch.cameras[cams[0]], epoch.cameras[cams[1]]),\n",
    "    points3d_final=obj_coords,\n",
    "    image_points=image_coords,\n",
    "    camera_centers_world=cfg.georef.camera_centers_world,\n",
    ")\n",
    "T = abs_ori.estimate_transformation_linear(estimate_scale=True)\n",
    "points3d = abs_ori.apply_transformation(points3d=points3d)\n",
    "for i, cam in enumerate(cams):\n",
    "    epoch.cameras[cam] = abs_ori.cameras[i]\n",
    "\n",
    "# Convert the 3D points to an icepy4d Points object\n",
    "pts = icecore.Points()\n",
    "pts.append_points_from_numpy(\n",
    "    points3d,\n",
    "    track_ids=epoch.features[cams[0]].get_track_ids(),\n",
    "    colors=triang.colors,\n",
    ")\n",
    "\n",
    "# Store the points in the Epoch object\n",
    "epoch.points = pts\n",
    "\n",
    "# Update timer\n",
    "timer.update(\"absolute orientation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the current Epoch object as a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save epoch as a pickle object\n",
    "if epoch.save_pickle(f\"{epoch.epoch_dir}/{epoch}.pickle\"):\n",
    "    logger.info(f\"{epoch} saved successfully\")\n",
    "else:\n",
    "    logger.error(f\"Unable to save {epoch}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icepy4d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
