{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4acfe25c-89f3-4708-a19f-7b5f0c5336f9",
   "metadata": {},
   "source": [
    "# **Belvedere stereo matching**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7175b971-cb2c-4a78-8db3-c4ccf5a9d187",
   "metadata": {},
   "source": [
    " v0.1 2022.05.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c2bdc86-eeda-46f8-8cc2-800da7eaf51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import cv2 \n",
    "import pydegensac\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib widget\n",
    "import matplotlib.cm as cm\n",
    "import open3d as o3d\n",
    "\n",
    "from src.match_pairs import match_pair\n",
    "from src.track_matches import track_matches\n",
    "# from src.sg.utils import make_matching_plot\n",
    "\n",
    "from src.io import read_img\n",
    "from src.geometry import (estimate_pose, P_from_KRT, X0_from_P, project_points)\n",
    "from src.utils import (normalize_and_und_points, draw_epip_lines, make_matching_plot, undistort_image, interpolate_point_colors)\n",
    "from src.thirdParties.triangulation import (linear_LS_triangulation, iterative_LS_triangulation)\n",
    "\n",
    "#  Parameters (to be put in parser)\n",
    "\n",
    "rootDirPath = '.'\n",
    "\n",
    "#- Folders and paths\n",
    "imFld = 'data/img'\n",
    "imExt = '.tif'\n",
    "calibFld = 'data/calib'\n",
    "\n",
    "#- CAMERAS\n",
    "numCams = 2\n",
    "camNames = ['p2', 'p3']\n",
    "\n",
    "#- Image cropping boundaries\n",
    "maskBB = [[600,1900,5300, 3600], [800,1800,5500,3500]]             # Bounding box for processing the images from the two cameras\n",
    "\n",
    "#  Load data\n",
    "cameras = []  # List for storing cameras information (as dicts)\n",
    "images = []   # List for storing image paths\n",
    "features = []  # Dict for storing all the valid matched features at all epochs\n",
    "F = [] # List for storing fundamental matrixes\n",
    "points3d = [] # List for storing 3D points\n",
    "\n",
    "#- images\n",
    "for jj, cam in enumerate(camNames):\n",
    "    d  = os.listdir(os.path.join(rootDirPath, imFld, cam))\n",
    "    for i, f in enumerate(d):\n",
    "        d[i] = os.path.join(rootDirPath, imFld, cam, f)\n",
    "    d.sort()\n",
    "    if jj > 0 and len(d) is not len(images[jj-1]):\n",
    "        print('Error: different number of images per camera')\n",
    "    else:\n",
    "        images.insert(jj, d)\n",
    "# TODO: change order of epoches and cameras to make everything consistent!\n",
    "        \n",
    "#- Cameras structures\n",
    "# TO DO: implement camera class!\n",
    "for jj, cam in enumerate(camNames):\n",
    "    path = (os.path.join(rootDirPath, calibFld, cam+'.txt'))\n",
    "    with open(path, 'r') as f:\n",
    "        data = np.loadtxt(f)\n",
    "    K = data[0:9].astype(float).reshape(3, 3, order='C')\n",
    "    dist = data[9:13].astype(float)\n",
    "    cameras.insert(jj, {'K': K, 'dist': dist})\n",
    "\n",
    "# Remove some variables\n",
    "del d, data, K, dist, path, f, i, jj\n",
    "\n",
    "print('Data loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a90f37-afdc-4030-8888-a64809ccc6cf",
   "metadata": {},
   "source": [
    "# **Process epoches** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba9049e7-3ee0-41b4-8f9d-a79f43004030",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_matches = 1\n",
    "if find_matches:\n",
    "    epoches2process = [0,1] # #1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "\n",
    "\n",
    "    for epoch in epoches2process:\n",
    "        print(f'Processing epoch {epoch}...')\n",
    "\n",
    "        #=== Find Matches at current epoch ===#\n",
    "        print('Run Superglue to find matches at epoch {}'.format(epoch))    \n",
    "        epochdir = os.path.join('res','epoch_'+str(epoch))      \n",
    "        pair = [images[0][epoch], images[1][epoch]]\n",
    "        maskBB = np.array(maskBB).astype('int')\n",
    "        opt_matching ={'output_dir': epochdir, \n",
    "\n",
    "                       'resize': [-1],\n",
    "                       'resize_float': True,\n",
    "                       'equalize_hist': False,\n",
    "\n",
    "                       'nms_radius': 3,  # default 3 \n",
    "                       'keypoint_threshold': 0.0001, \n",
    "                       'max_keypoints': 8192, #4096, # Use 8192 for returning more matches\n",
    "\n",
    "                       'superglue': 'outdoor',\n",
    "                       'sinkhorn_iterations': 100,\n",
    "                       'match_threshold': 0.2, \n",
    "\n",
    "                       'viz':  True,\n",
    "                       'viz_extension': 'png', \n",
    "                       'fast_viz': True,\n",
    "                       'opencv_display' : False, \n",
    "                       'show_keypoints': False, \n",
    "\n",
    "                       'cache': False,\n",
    "                       'force_cpu': False,\n",
    "\n",
    "                       'useTile': True, \n",
    "                       'writeTile2Disk': False,\n",
    "                       'do_viz_tile': False,\n",
    "                       'rowDivisor': 2,\n",
    "                       'colDivisor': 3,\n",
    "                       'overlap': 300,            \n",
    "                       }\n",
    "        matchedPts, matchedDescriptors, matchedPtsScores, _ = match_pair(pair, maskBB, opt_matching)\n",
    "\n",
    "        # Store matches in features structure\n",
    "        if epoch == 0:\n",
    "            features = [{   'mkpts0': matchedPts['mkpts0'], \n",
    "                            'mkpts1': matchedPts['mkpts1'],\n",
    "                            # 'mconf': matchedPts['match_confidence'],\n",
    "                            'descr0': matchedDescriptors[0], \n",
    "                            'descr1': matchedDescriptors[1],\n",
    "                            'scores0': matchedPtsScores[0], \n",
    "                            'scores1': matchedPtsScores[1] }] \n",
    "        # TODO: Store match confidence!\n",
    "\n",
    "        #=== Track previous matches at current epoch ===#\n",
    "        if epoch > 0:\n",
    "            print('Track points from epoch {} to epoch {}'.format(epoch-1, epoch))\n",
    "\n",
    "            trackoutdir = os.path.join('res','epoch_'+str(epoch), 'from_t'+str(epoch-1))\n",
    "            pairs = [ [ images[0][epoch-1], images[0][epoch] ], \n",
    "                      [ images[1][epoch-1], images[1][epoch] ] ] \n",
    "            maskBB = np.array(maskBB).astype('int')\n",
    "            opt_tracking = {'output_dir': trackoutdir,\n",
    "\n",
    "                            'resize': [-1],\n",
    "                            'resize_float': True,\n",
    "                            'equalize_hist': False,\n",
    "\n",
    "                            'nms_radius': 4 , \n",
    "                            'keypoint_threshold': 0.0001, \n",
    "                            'max_keypoints': 8192, \n",
    "\n",
    "                            'superglue': 'outdoor',\n",
    "                            'sinkhorn_iterations': 100,\n",
    "                            'match_threshold': 0.2, \n",
    "\n",
    "                            'viz':  True,\n",
    "                            'viz_extension': 'png',  \n",
    "                            'fast_viz': True,\n",
    "                            'opencv_display' : False, \n",
    "                            'show_keypoints': False, \n",
    "\n",
    "                            'cache': False,\n",
    "                            'force_cpu': False,\n",
    "\n",
    "                            'useTile': True, \n",
    "                            'writeTile2Disk': False,\n",
    "                            'do_viz_tile': False,\n",
    "                            'rowDivisor': 2,\n",
    "                            'colDivisor': 4,\n",
    "                               }   \n",
    "\n",
    "            prevs = [{'keypoints0': np.float32(features[epoch-1]['mkpts0']), \n",
    "                      'descriptors0': np.float32(features[epoch-1]['descr0']),\n",
    "                      'scores0': np.float32(features[epoch-1]['scores0']) }, \n",
    "                     {'keypoints0': np.float32(features[epoch-1]['mkpts1']), \n",
    "                      'descriptors0': np.float32(features[epoch-1]['descr1']), \n",
    "                      'scores0': np.float32(features[epoch-1]['scores1'])  }  ]\n",
    "            tracked_cam0, tracked_cam1 = track_matches(pairs, maskBB, prevs, opt_tracking)\n",
    "            # TO DO: tenere traccia anche dei descriptors and scores dei punti traccati!\n",
    "            # TO DO: tenere traccia dell'epoca in cui Ã¨ stato trovato il match\n",
    "            # TO CHECK: Problema nei punti tracciati... vengono rigettati da pydegensac\n",
    "\n",
    "            # Store all matches in features structure\n",
    "            features.append({'mkpts0': np.concatenate((matchedPts['mkpts0'], tracked_cam0['keypoints1']), axis=0 ), \n",
    "                             'mkpts1': np.concatenate((matchedPts['mkpts1'], tracked_cam1['keypoints1']), axis=0 ),\n",
    "                             # 'mconf': matchedPts['match_confidence'],\n",
    "                              'descr0': np.concatenate((matchedDescriptors[0], tracked_cam0['descriptors1']), axis=1 ),\n",
    "                              'descr1': np.concatenate((matchedDescriptors[1], tracked_cam1['descriptors1']), axis=1 ),\n",
    "                              'scores0': np.concatenate((matchedPtsScores[0], tracked_cam0['scores1']), axis=0 ), \n",
    "                              'scores1': np.concatenate((matchedPtsScores[1], tracked_cam1['scores1']), axis=0 ), \n",
    "                             })\n",
    "\n",
    "            # Run Pydegensac to estimate F matrix and reject outliers                         \n",
    "            F[epoch], inlMask = pydegensac.findFundamentalMatrix(features[epoch]['mkpts0'], features[epoch]['mkpts1'], px_th=3, conf=0.9,\n",
    "                                                          max_iters=100000, laf_consistensy_coef=-1.0, error_type='sampson',\n",
    "                                                          symmetric_error_check=True, enable_degeneracy_check=True)\n",
    "            print('Matches at epoch {}: pydegensac found {} inliers ({:.2f}%)'.format(epoch, int(deepcopy(inlMask).astype(np.float32).sum()),\n",
    "                            int(deepcopy(inlMask).astype(np.float32).sum())*100 / len(features[epoch]['mkpts0'])))\n",
    "\n",
    "        # Write matched points to disk   \n",
    "        stem0, stem1 = Path(images[0][epoch]).stem, Path(images[1][epoch]).stem\n",
    "        np.savetxt(os.path.join(epochdir, stem0+'_matchedPts.txt'), \n",
    "                   features[epoch]['mkpts0'] , fmt='%i', delimiter=',', newline='\\n',\n",
    "                   header='x,y') \n",
    "        np.savetxt(os.path.join(epochdir, stem1+'_matchedPts.txt'), \n",
    "                   features[epoch]['mkpts1'] , fmt='%i', delimiter=' ', newline='\\n',                   \n",
    "                   header='x,y') \n",
    "        with open(os.path.join(epochdir, stem0+'_'+stem1+'_features.pickle'), 'wb') as f:\n",
    "            pickle.dump(features, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print('Matching completed')\n",
    "\n",
    "else: \n",
    "    epoch = 0\n",
    "    if not features:\n",
    "        matches_path = 'res/epoch_0/IMG_0520_IMG_2131_features.pickle'\n",
    "        with open(matches_path, 'rb') as f:\n",
    "            features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef699362-7a6a-48e0-bfda-fefb95594c7e",
   "metadata": {},
   "source": [
    "# **SfM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8b98ef-bc05-4cec-9443-504d7f57dbdd",
   "metadata": {},
   "source": [
    "### Realtive Pose with Essential Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f899bb5e-7485-406a-ac01-2b8715a6f180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing relative pose. Valid points: 3224/5018\n"
     ]
    }
   ],
   "source": [
    "pts0, pts1 = features[0]['mkpts0'], features[0]['mkpts1']\n",
    "rel_pose = estimate_pose(pts0, pts1, cameras[0]['K'],  cameras[1]['K'], thresh=1, conf=0.99999)\n",
    "R = rel_pose[0]\n",
    "t = rel_pose[1]\n",
    "valid = rel_pose[2]\n",
    "print('Computing relative pose. Valid points: {}/{}'.format(valid.sum(),len(valid)))\n",
    "\n",
    "# Build cameras structures\n",
    "cameras[0]['R'], cameras[0]['t'] = np.eye(3), np.zeros((3,1))\n",
    "cameras[1]['R'], cameras[1]['t'] = R, t.reshape(3,1)\n",
    "for jj in range(0,numCams):\n",
    "    cameras[jj]['P'] = P_from_KRT(cameras[jj]['K'], cameras[jj]['R'], cameras[jj]['t'])\n",
    "    cameras[jj]['X0'] = X0_from_P(cameras[jj]['P'])\n",
    "\n",
    "# Scale model by using camera baseline\n",
    "X01_meta = np.array([416651.52489669225,5091109.91215075,1858.908434299682])   # IMG_2092\n",
    "X02_meta = np.array([416622.27552777925,5091364.507128085,1902.4053286545502]) # IMG_0481\n",
    "camWorldBaseline = np.linalg.norm(X01_meta - X02_meta)                         # [m] From Metashape model at epoch t0\n",
    "camRelOriBaseline = np.linalg.norm(cameras[0]['X0'] - cameras[1]['X0'])\n",
    "scaleFct = camWorldBaseline / camRelOriBaseline\n",
    "cameras[1]['X0'] =  cameras[1]['X0'] * scaleFct\n",
    "cameras[1]['t'] = -np.matmul(cameras[1]['R'], cameras[1]['X0'])\n",
    "cameras[1]['P'] = P_from_KRT(cameras[1]['K'], cameras[1]['R'], cameras[1]['t'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db32be1-2728-4338-81fe-344e21df19fc",
   "metadata": {},
   "source": [
    "### Trinagulate points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd39aca5-3792-4574-a7c9-a080904df061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triangulated success: 1.0\n"
     ]
    }
   ],
   "source": [
    "pts0_und = cv2.undistortPoints(features[0]['mkpts0'], cameras[0]['K'], cameras[0]['dist'], None, cameras[0]['K'])\n",
    "pts1_und = cv2.undistortPoints(features[0]['mkpts1'], cameras[1]['K'], cameras[1]['dist'], None, cameras[1]['K'])\n",
    "M, status = iterative_LS_triangulation(pts0_und, cameras[0]['P'],  pts1_und, cameras[1]['P'])\n",
    "points3d.insert(epoch, M)\n",
    "print(f'Triangulated success: {status.sum()/status.size}')\n",
    "\n",
    "# Interpolate colors from image \n",
    "jj = 1\n",
    "image = cv2.cvtColor(cv2.imread(images[jj][0], flags=cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
    "points3d_cols =  interpolate_point_colors(points3d[0], image, cameras[jj]['K'], cameras[jj]['R'], cameras[jj]['t'], cameras[jj]['dist'])\n",
    "#TODO: check why function doesn't work with image 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f8668-0cb6-40bf-9427-9f27d007f265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old\n",
    "# pts0_und = normalize_and_und_points(features[0]['mkpts0'], cameras[0]['K'], cameras[0]['dist'])\n",
    "# pts1_und = normalize_and_und_points(features[0]['mkpts1'], cameras[1]['K'], cameras[1]['dist'])\n",
    "# points3d, status = iterative_LS_triangulation(pts0_und, cameras[0]['P'],  pts1_und, cameras[1]['P'])\n",
    "\n",
    "# img1_und = cv2.undistort(img1, K, dist, None, K)\n",
    "# img1_kpts = cv2.drawKeypoints(img1_und,cv2.KeyPoint.convert(m),img1_und,(0,0,255),flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "# cv2.imwrite('kpts0.jpg', img1_kpts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bbad881-1f0a-4915-928e-61f30b5e7e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize and export sparse point cloud\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points3d[epoch])\n",
    "pcd.colors = o3d.utility.Vector3dVector(points3d_cols)\n",
    "o3d.visualization.draw_geometries([pcd])\n",
    "o3d.io.write_point_cloud(\"res/epoch_0/sparse_ptc.ply\", pcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659d9912-c9a6-4942-a2fc-f08ad40460db",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Rectify images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef2592a-422f-4c37-9059-1b435b739532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw epipolar lines\n",
    "\n",
    "sgm_path = Path('sgm')\n",
    "downsample = 0.25\n",
    "fast_viz = True\n",
    "\n",
    "stem0 = Path(images[0][0]).stem\n",
    "stem1 = Path(images[1][0]).stem\n",
    "\n",
    "pts0, pts1 = features[0]['mkpts0'], features[0]['mkpts1']\n",
    "F, inlMask = pydegensac.findFundamentalMatrix(pts0, pts1, px_th=1, conf=0.99999,\n",
    "                                              max_iters=100000, laf_consistensy_coef=-1.0, error_type='sampson',\n",
    "                                              symmetric_error_check=True, enable_degeneracy_check=True)\n",
    "\n",
    "# Find epilines corresponding to points in right image (second image) and drawing its lines on left image\n",
    "img0 = cv2.imread(images[0][0], flags=cv2.IMREAD_COLOR)\n",
    "img1 = cv2.imread(images[1][0], flags=cv2.IMREAD_COLOR)\n",
    "\n",
    "lines0 = cv2.computeCorrespondEpilines(pts1.reshape(-1,1,2), 2, F)\n",
    "lines0 = lines0.reshape(-1,3)\n",
    "img0_epiplines, _ = draw_epip_lines(img0,img1,lines0,pts0,pts1)\n",
    "\n",
    "# Find epilines corresponding to points in left image (first image) and drawing its lines on right image\n",
    "lines1 = cv2.computeCorrespondEpilines(pts0.reshape(-1,1,2), 1, F)\n",
    "lines1 = lines1.reshape(-1,3)\n",
    "img1_epiplines,_ = draw_epip_lines(img1,img0,lines1,pts1,pts0, fast_viz=True)\n",
    "\n",
    "if fast_viz:\n",
    "    cv2.imwrite(str(sgm_path / (stem0 + \"_epiplines.jpg\")), img0_epiplines)\n",
    "    cv2.imwrite(str(sgm_path / (stem1 + \"_epiplines.jpg\")), img1_epiplines)\n",
    "else: \n",
    "    plt.subplot(121),plt.imshow(img0_epiplines)\n",
    "    plt.subplot(122),plt.imshow(img1_epiplines)\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95b2de7-df6f-4025-aa71-dea465bd72c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform rectification\n",
    "\n",
    "img0 = cv2.imread(images[0][0], flags=cv2.IMREAD_COLOR)\n",
    "img1 = cv2.imread(images[1][0], flags=cv2.IMREAD_COLOR)\n",
    "\n",
    "# undistort images\n",
    "name0 = str(sgm_path / 'und' / (stem0 + \"_undistorted.jpg\"))\n",
    "name1 = str(sgm_path / 'und' / (stem1 + \"_undistorted.jpg\"))\n",
    "img0, K0_scaled = undistort_image(img0, cameras[0]['K'],  cameras[0]['dist'], downsample, name0)\n",
    "img1, K1_scaled = undistort_image(img1, cameras[1]['K'],  cameras[1]['dist'], downsample, name1)\n",
    "\n",
    "# Rectify uncalibrated\n",
    "pts0, pts1 = features[0]['mkpts1']*downsample, features[0]['mkpts0']*downsample\n",
    "h, w, _ = img0.shape\n",
    "F, inlMask = pydegensac.findFundamentalMatrix(pts0, pts1, px_th=1, conf=0.99999,\n",
    "                                              max_iters=100000, laf_consistensy_coef=-1.0, error_type='sampson',\n",
    "                                              symmetric_error_check=True, enable_degeneracy_check=True)\n",
    "print('Pydegensac: {} inliers ({:.2f}%)'.format(inlMask.sum(), inlMask.sum()*100 / len(pts0)))\n",
    "success, H1, H0 = cv2.stereoRectifyUncalibrated(pts0, pts1 , F, (w,h))\n",
    "img0_rectified = cv2.warpPerspective(img0, H0, (w,h))\n",
    "img1_rectified = cv2.warpPerspective(img1, H1, (w,h))\n",
    "\n",
    "# write images to disk\n",
    "if fast_viz:\n",
    "    cv2.imwrite(str(sgm_path / 'rectified' / (stem0 + \"_rectified.jpg\")), img0_rectified)\n",
    "    cv2.imwrite(str(sgm_path / 'rectified' / (stem1 + \"_rectified.jpg\")), img1_rectified)\n",
    "else:\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    plt.imshow(cv2.cvtColor(img0_rectified, cv2.COLOR_BGR2RGB))\n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "    plt.imshow(cv2.cvtColor(img1_rectified, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f14be7-6b0c-44fb-8fa3-ca870877a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw keypoints and matches\n",
    "pts0_rect = cv2.perspectiveTransform(np.float32(pts0).reshape(-1,1,2), H0).reshape(-1,2)\n",
    "pts1_rect = cv2.perspectiveTransform(np.float32(pts1).reshape(-1,1,2), H1).reshape(-1,2)\n",
    "\n",
    "# img0_rect_kpts = img0.copy()\n",
    "img0 = cv2.imread(images[0][0], flags=cv2.IMREAD_GRAYSCALE)\n",
    "img1 = cv2.imread(images[1][0], flags=cv2.IMREAD_GRAYSCALE)\n",
    "pts0, pts1 = features[0]['mkpts0'], features[0]['mkpts1']\n",
    "img0_kpts = cv2.drawKeypoints(img0,cv2.KeyPoint.convert(pts0),img0,(),flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "img1_kpts = cv2.drawKeypoints(img1,cv2.KeyPoint.convert(pts1),img1,(0,0,255),flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "cv2.imwrite('kpts0.jpg', img0_kpts)\n",
    "cv2.imwrite('kpts1.jpg', img1_kpts)\n",
    "\n",
    "make_matching_plot(img0, img1, pts0, pts1, path='matches.jpg')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139f4061-0dc5-435f-a0fa-963539d051f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SGM OPENCV\n",
    "# blockSize = 5\n",
    "# min_disp = 128\n",
    "# max_disp = 512\n",
    "# num_disp = max_disp-min_disp\n",
    "# stereo = cv2.StereoSGBM_create(minDisparity = min_disp,\n",
    "#     numDisparities = num_disp,\n",
    "#     blockSize = blockSize,\n",
    "#     P1 = 8*1*blockSize**2,\n",
    "#     P2 = 32*1*blockSize**2,\n",
    "#     disp12MaxDiff = 0,\n",
    "#     uniquenessRatio = 5,\n",
    "#     speckleWindowSize = 100,\n",
    "#     speckleRange = 2,\n",
    "# )\n",
    "\n",
    "# imgR = img0_rectified\n",
    "# imgL = img1_rectified\n",
    "# print('computing disparity...')\n",
    "# disparity_SGBM = stereo.compute(imgL, imgR)\n",
    "# disparity_SGBM = cv2.normalize(disparity_SGBM, disparity_SGBM, alpha=255,\n",
    "#                               beta=0, norm_type=cv2.NORM_MINMAX)\n",
    "# # disparity_SGBM = cv2.validateDisparity(disparity_SGBM, cost, minDisparity, numberOfDisparities\n",
    "# cv2.imwrite(str(sgm_path / \"disparity_SGBM_norm.png\"), disparity_SGBM)\n",
    "# print('done')\n",
    "\n",
    "# cv2.imshow(\"Disparity\", cv2.resize(disparity_SGBM, (1920,1080)))\n",
    "# cv2.waitKey()\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "# print('generating 3d point cloud...',)\n",
    "# h, w = imgL.shape[:2]\n",
    "# points = cv2.reprojectImageTo3D(disp, K)\n",
    "# colors = cv.cvtColor(imgL, cv.COLOR_BGR2RGB)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
