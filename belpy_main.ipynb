{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4acfe25c-89f3-4708-a19f-7b5f0c5336f9",
   "metadata": {},
   "source": [
    "# **Belvedere stereo matching**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7175b971-cb2c-4a78-8db3-c4ccf5a9d187",
   "metadata": {},
   "source": [
    " v0.1 2022.05.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c2bdc86-eeda-46f8-8cc2-800da7eaf51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import cv2 \n",
    "import pickle\n",
    "import json \n",
    "import matplotlib\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import open3d as o3d\n",
    "import pydegensac\n",
    "\n",
    "from lib.match_pairs import match_pair\n",
    "from lib.track_matches import track_matches\n",
    "\n",
    "from lib.io import read_img\n",
    "from lib.geometry import (estimate_pose, P_from_KRT, X0_from_P, project_points)\n",
    "from lib.utils import (normalize_and_und_points, draw_epip_lines, make_matching_plot, undistort_image, interpolate_point_colors, build_dsm)\n",
    "from lib.thirdParts.triangulation import (linear_LS_triangulation, iterative_LS_triangulation)\n",
    "\n",
    "#---  Parameters  ---#\n",
    "# TODO: put parameters in parser or in json file\n",
    "\n",
    "rootDirPath = '.'\n",
    "\n",
    "#- Folders and paths\n",
    "imFld = 'data/img'\n",
    "imExt = '.tif'\n",
    "calibFld = 'data/calib'\n",
    "matching_config = 'config/opt_matching.json'\n",
    "tracking_config = 'config/opt_tracking.json'\n",
    "\n",
    "#- CAMERAS\n",
    "numCams = 2\n",
    "camNames = ['p2', 'p3']\n",
    "\n",
    "#- Image cropping boundaries\n",
    "# maskBB = [[600,1900,5300, 3600], [800,1800,5500,3500]]             # Bounding box for processing the images from the two cameras\n",
    "maskBB = [[400,1500,5500,4000], [600,1400,5700,3900]]             # Bounding box for processing the images from the two cameras\n",
    "\n",
    "#  Load data\n",
    "cameras = []  # List for storing cameras information (as dicts)\n",
    "images = []   # List for storing image paths\n",
    "features = []  # Dict for storing all the valid matched features at all epochs\n",
    "F_matrix = [] # List for storing fundamental matrixes\n",
    "points3d = [] # List for storing 3D points\n",
    "\n",
    "#- images\n",
    "for jj, cam in enumerate(camNames):\n",
    "    d  = os.listdir(os.path.join(rootDirPath, imFld, cam))\n",
    "    for i, f in enumerate(d):\n",
    "        d[i] = os.path.join(rootDirPath, imFld, cam, f)\n",
    "    d.sort()\n",
    "    if jj > 0 and len(d) is not len(images[jj-1]):\n",
    "        print('Error: different number of images per camera')\n",
    "    else:\n",
    "        images.insert(jj, d)\n",
    "# TODO: change order of epoches and cameras to make everything consistent!\n",
    "        \n",
    "#- Cameras structures\n",
    "# TO DO: implement camera class!\n",
    "for jj, cam in enumerate(camNames):\n",
    "    path = (os.path.join(rootDirPath, calibFld, cam+'.txt'))\n",
    "    with open(path, 'r') as f:\n",
    "        data = np.loadtxt(f)\n",
    "    K = data[0:9].astype(float).reshape(3, 3, order='C')\n",
    "    dist = data[9:13].astype(float)\n",
    "    cameras.insert(jj, {'K': K, 'dist': dist})\n",
    "\n",
    "# Remove some variables\n",
    "del d, data, K, dist, path, f, i, jj\n",
    "\n",
    "print('Data loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a90f37-afdc-4030-8888-a64809ccc6cf",
   "metadata": {},
   "source": [
    "# **Process epoches** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba9049e7-3ee0-41b4-8f9d-a79f43004030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded previous matches\n"
     ]
    }
   ],
   "source": [
    "find_matches = 0\n",
    "if find_matches:\n",
    "    epoches2process = [0] # #1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "    \n",
    "    for epoch in epoches2process:\n",
    "        print(f'Processing epoch {epoch}...')\n",
    "\n",
    "        #=== Find Matches at current epoch ===#\n",
    "        print('Run Superglue to find matches at epoch {}'.format(epoch))    \n",
    "        epochdir = os.path.join('res','epoch_'+str(epoch))      \n",
    "        with open(matching_config,) as f:\n",
    "            opt_matching = json.load(f)\n",
    "        opt_matching['output_dir'] = epochdir\n",
    "        pair = [images[0][epoch], images[1][epoch]]\n",
    "        maskBB = np.array(maskBB).astype('int')\n",
    "        matchedPts, matchedDescriptors, matchedPtsScores, _ = match_pair(pair, maskBB, opt_matching)\n",
    "\n",
    "        # Store matches in features structure\n",
    "        if epoch == 0:\n",
    "            features = [{   'mkpts0': matchedPts['mkpts0'], \n",
    "                            'mkpts1': matchedPts['mkpts1'],\n",
    "                            # 'mconf': matchedPts['match_confidence'],\n",
    "                            'descr0': matchedDescriptors[0], \n",
    "                            'descr1': matchedDescriptors[1],\n",
    "                            'scores0': matchedPtsScores[0], \n",
    "                            'scores1': matchedPtsScores[1] }] \n",
    "        # TODO: Store match confidence!\n",
    "\n",
    "        #=== Track previous matches at current epoch ===#\n",
    "        if epoch > 0:\n",
    "            print('Track points from epoch {} to epoch {}'.format(epoch-1, epoch))\n",
    "            \n",
    "            trackoutdir = os.path.join('res','epoch_'+str(epoch), 'from_t'+str(epoch-1))\n",
    "            with open(tracking_config,) as f:\n",
    "                opt_tracking = json.load(f)\n",
    "            opt_tracking['output_dir'] = trackoutdir\n",
    "            pairs = [ [ images[0][epoch-1], images[0][epoch] ], \n",
    "                      [ images[1][epoch-1], images[1][epoch] ] ] \n",
    "            maskBB = np.array(maskBB).astype('int')\n",
    "                            \n",
    "            prevs = [{'keypoints0': np.float32(features[epoch-1]['mkpts0']), \n",
    "                      'descriptors0': np.float32(features[epoch-1]['descr0']),\n",
    "                      'scores0': np.float32(features[epoch-1]['scores0']) }, \n",
    "                     {'keypoints0': np.float32(features[epoch-1]['mkpts1']), \n",
    "                      'descriptors0': np.float32(features[epoch-1]['descr1']), \n",
    "                      'scores0': np.float32(features[epoch-1]['scores1'])  }  ]\n",
    "            tracked_cam0, tracked_cam1 = track_matches(pairs, maskBB, prevs, opt_tracking)\n",
    "            # TODO: tenere traccia anche dei descriptors and scores dei punti traccati!\n",
    "            # TODO: tenere traccia dell'epoca in cui Ã¨ stato trovato il match\n",
    "            # TODO: Problema nei punti tracciati... vengono rigettati da pydegensac\n",
    "\n",
    "            # Store all matches in features structure\n",
    "            features.append({'mkpts0': np.concatenate((matchedPts['mkpts0'], tracked_cam0['keypoints1']), axis=0 ), \n",
    "                             'mkpts1': np.concatenate((matchedPts['mkpts1'], tracked_cam1['keypoints1']), axis=0 ),\n",
    "                             # 'mconf': matchedPts['match_confidence'],\n",
    "                              'descr0': np.concatenate((matchedDescriptors[0], tracked_cam0['descriptors1']), axis=1 ),\n",
    "                              'descr1': np.concatenate((matchedDescriptors[1], tracked_cam1['descriptors1']), axis=1 ),\n",
    "                              'scores0': np.concatenate((matchedPtsScores[0], tracked_cam0['scores1']), axis=0 ), \n",
    "                              'scores1': np.concatenate((matchedPtsScores[1], tracked_cam1['scores1']), axis=0 ), \n",
    "                             })\n",
    "\n",
    "            # Run Pydegensac to estimate F matrix and reject outliers                         \n",
    "            F, inlMask = pydegensac.findFundamentalMatrix(features[epoch]['mkpts0'], features[epoch]['mkpts1'], px_th=3, conf=0.9,\n",
    "                                                          max_iters=100000, laf_consistensy_coef=-1.0, error_type='sampson',\n",
    "                                                          symmetric_error_check=True, enable_degeneracy_check=True)\n",
    "            F_matrix.append(F)\n",
    "            print('Matches at epoch {}: pydegensac found {} inliers ({:.2f}%)'.format(epoch, inlMask.sum(),\n",
    "                            inlMask.sum()*100 / len(features[epoch]['mkpts0'])))\n",
    "\n",
    "        # Write matched points to disk   \n",
    "        stem0, stem1 = Path(images[0][epoch]).stem, Path(images[1][epoch]).stem\n",
    "        np.savetxt(os.path.join(epochdir, stem0+'_matchedPts.txt'), \n",
    "                   features[epoch]['mkpts0'] , fmt='%i', delimiter=',', newline='\\n',\n",
    "                   header='x,y') \n",
    "        np.savetxt(os.path.join(epochdir, stem1+'_matchedPts.txt'), \n",
    "                   features[epoch]['mkpts1'] , fmt='%i', delimiter=' ', newline='\\n',                   \n",
    "                   header='x,y') \n",
    "        with open(os.path.join(epochdir, stem0+'_'+stem1+'_features.pickle'), 'wb') as f:\n",
    "            pickle.dump(features, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print('Matching completed')\n",
    "\n",
    "elif not features: \n",
    "    epoch = 0\n",
    "    matches_path = 'res/epoch_0/IMG_0520_IMG_2131_features.pickle'\n",
    "    with open(matches_path, 'rb') as f:\n",
    "        features = pickle.load(f)\n",
    "        print(\"Loaded previous matches\")\n",
    "else:\n",
    "    print(\"Features already present, nothing was changed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef699362-7a6a-48e0-bfda-fefb95594c7e",
   "metadata": {},
   "source": [
    "# **SfM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8b98ef-bc05-4cec-9443-504d7f57dbdd",
   "metadata": {},
   "source": [
    "### Realtive Pose with Essential Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f899bb5e-7485-406a-ac01-2b8715a6f180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing relative pose. Valid points: 3533/5315\n"
     ]
    }
   ],
   "source": [
    "pts0, pts1 = features[0]['mkpts0'], features[0]['mkpts1']\n",
    "rel_pose = estimate_pose(pts0, pts1, cameras[0]['K'],  cameras[1]['K'], thresh=1, conf=0.99999)\n",
    "R = rel_pose[0]\n",
    "t = rel_pose[1]\n",
    "valid = rel_pose[2]\n",
    "print('Computing relative pose. Valid points: {}/{}'.format(valid.sum(),len(valid)))\n",
    "\n",
    "# Build cameras structures\n",
    "cameras[0]['R'], cameras[0]['t'] = np.eye(3), np.zeros((3,1))\n",
    "cameras[1]['R'], cameras[1]['t'] = R, t.reshape(3,1)\n",
    "for jj in range(0,numCams):\n",
    "    cameras[jj]['P'] = P_from_KRT(cameras[jj]['K'], cameras[jj]['R'], cameras[jj]['t'])\n",
    "    cameras[jj]['X0'] = X0_from_P(cameras[jj]['P'])\n",
    "\n",
    "# Scale model by using camera baseline\n",
    "X01_meta = np.array([416651.52489669225,5091109.91215075,1858.908434299682])   # IMG_2092\n",
    "X02_meta = np.array([416622.27552777925,5091364.507128085,1902.4053286545502]) # IMG_0481\n",
    "camWorldBaseline = np.linalg.norm(X01_meta - X02_meta)                         # [m] From Metashape model at epoch t0\n",
    "camRelOriBaseline = np.linalg.norm(cameras[0]['X0'] - cameras[1]['X0'])\n",
    "scaleFct = camWorldBaseline / camRelOriBaseline\n",
    "cameras[1]['X0'] =  cameras[1]['X0'] * scaleFct\n",
    "cameras[1]['t'] = -np.matmul(cameras[1]['R'], cameras[1]['X0'])\n",
    "cameras[1]['P'] = P_from_KRT(cameras[1]['K'], cameras[1]['R'], cameras[1]['t'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db32be1-2728-4338-81fe-344e21df19fc",
   "metadata": {},
   "source": [
    "### Trinagulate points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd39aca5-3792-4574-a7c9-a080904df061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triangulated success: 1.0\n"
     ]
    }
   ],
   "source": [
    "pts0_und = cv2.undistortPoints(features[0]['mkpts0'], cameras[0]['K'], cameras[0]['dist'], None, cameras[0]['K'])\n",
    "pts1_und = cv2.undistortPoints(features[0]['mkpts1'], cameras[1]['K'], cameras[1]['dist'], None, cameras[1]['K'])\n",
    "M, status = iterative_LS_triangulation(pts0_und, cameras[0]['P'],  pts1_und, cameras[1]['P'])\n",
    "points3d.insert(epoch, M)\n",
    "print(f'Triangulated success: {status.sum()/status.size}')\n",
    "\n",
    "# Interpolate colors from image \n",
    "jj = 1\n",
    "image = cv2.cvtColor(cv2.imread(images[jj][0], flags=cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
    "points3d_cols =  interpolate_point_colors(points3d[0], image, cameras[jj]['P'], cameras[jj]['K'], cameras[jj]['dist'])\n",
    "\n",
    "# Visualize and export sparse point cloud\n",
    "do_viz = True\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points3d[epoch])\n",
    "pcd.colors = o3d.utility.Vector3dVector(points3d_cols)\n",
    "o3d.io.write_point_cloud(\"res/epoch_0/sparsepts_t\"+str(epoch)+\".ply\", pcd)\n",
    "if do_viz:\n",
    "    o3d.visualization.draw_geometries([pcd])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ba728f8-e97f-4df3-8fb4-a841d3671a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build approximate DSM\n",
    "dsm = build_dsm(points3d[epoch], dsm_step=0.1, save_path=\"sfm/dsm_approx.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91580347-11df-4f76-9101-36b57c1cd554",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_96317/695379725.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Generate Ortophotos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeometry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mP_from_KRT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_ortophoto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdsm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mxx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdsm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "# Generate Ortophotos\n",
    "from lib.geometry import (P_from_KRT, project_points)\n",
    "\n",
    "def generate_ortophoto(image, dsm, P, res=1):\n",
    "    xx = dsm.x\n",
    "    yy = dsm.y\n",
    "    zz = dsm.z\n",
    "    \n",
    "    dsm_shape = dsm.x.shape\n",
    "    ncell = dsm_shape[0]*dsm_shape[1]\n",
    "    xyz = np.zeros((ncell,3))\n",
    "    xyz[:,0] = xx.flatten()\n",
    "    xyz[:,1] = yy.flatten()\n",
    "    xyz[:,2] = zz.flatten()\n",
    "    valid_cell = np.invert(np.isnan(xyz[:,2]))\n",
    "    \n",
    "    cols = np.full((ncell,3),0)\n",
    "    cols[valid_cell,:] = interpolate_point_colors(xyz[valid_cell,:], image, camera['P'], camera['K'], camera['dist'])\n",
    "    ortophoto = np.zeros((dsm_shape[0],dsm_shape[1],3))\n",
    "    ortophoto[:,:,0] = cols[:,0].reshape(dsm_shape[0], dsm_shape[1])\n",
    "    ortophoto[:,:,1] = cols[:,1].reshape(dsm_shape[0], dsm_shape[1])\n",
    "    ortophoto[:,:,2] = cols[:,2].reshape(dsm_shape[0], dsm_shape[1])\n",
    "    ortophoto = np.uint8(ortophoto*255)\n",
    "    \n",
    "    # for a, b, c in zip(xx.flatten(), yy.flatten(), zz.flatten()):\n",
    "    #     xyz.append([a, b, c]) \n",
    "    # ortophoto = cols[:,0].reshape(dsm_shape[0], dsm_shape[1],3)\n",
    "    # ortophoto = np.uint8(ortophoto*255)\n",
    "\n",
    "    return xyz, cols, ortophoto\n",
    "\n",
    "    \n",
    "    # ortophoto = None\n",
    "    # return ortophoto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659d9912-c9a6-4942-a2fc-f08ad40460db",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DENSE MATCHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c477741d-0143-49b1-90c4-c56517f83e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgm_path = Path('sgm')\n",
    "downsample = 0.25\n",
    "fast_viz = True\n",
    "\n",
    "stem0 = Path(images[0][0]).stem\n",
    "stem1 = Path(images[1][0]).stem\n",
    "\n",
    "pts0, pts1 = features[0]['mkpts0'], features[0]['mkpts1']\n",
    "F, inlMask = pydegensac.findFundamentalMatrix(pts0, pts1, px_th=1, conf=0.99999,\n",
    "                                              max_iters=100000, laf_consistensy_coef=-1.0, error_type='sampson',\n",
    "                                              symmetric_error_check=True, enable_degeneracy_check=True)\n",
    "\n",
    "img0 = cv2.imread(images[0][0], flags=cv2.IMREAD_COLOR)\n",
    "img1 = cv2.imread(images[1][0], flags=cv2.IMREAD_COLOR)\n",
    "h, w, _ = img0.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a70ff1-8eec-4a01-a899-79b16459d39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rectify calibrated\n",
    "R1,R2,P1,P2,Q = cv2.stereoRectify(cameras[0]['K'], cameras[0]['dist'], cameras[1]['K'], cameras[1]['dist'],\\\n",
    "             (h,w), cameras[1]['R'], cameras[1]['t'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fff3b0-6e9e-442d-ae04-d6cbe84a36b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recify Uncalibrated\n",
    "\n",
    "# undistort images\n",
    "name0 = str(sgm_path / 'und' / (stem0 + \"_undistorted.jpg\"))\n",
    "name1 = str(sgm_path / 'und' / (stem1 + \"_undistorted.jpg\"))\n",
    "img0, K0_scaled = undistort_image(img0, cameras[0]['K'],  cameras[0]['dist'], downsample, name0)\n",
    "img1, K1_scaled = undistort_image(img1, cameras[1]['K'],  cameras[1]['dist'], downsample, name1)\n",
    "\n",
    "# Rectify uncalibrated\n",
    "pts0, pts1 = features[0]['mkpts1']*downsample, features[0]['mkpts0']*downsample\n",
    "F, inlMask = pydegensac.findFundamentalMatrix(pts0, pts1, px_th=1, conf=0.99999,\n",
    "                                              max_iters=100000, laf_consistensy_coef=-1.0, error_type='sampson',\n",
    "                                              symmetric_error_check=True, enable_degeneracy_check=True)\n",
    "print('Pydegensac: {} inliers ({:.2f}%)'.format(inlMask.sum(), inlMask.sum()*100 / len(pts0)))\n",
    "success, H1, H0 = cv2.stereoRectifyUncalibrated(pts0, pts1 , F, (w,h))\n",
    "img0_rectified = cv2.warpPerspective(img0, H0, (w,h))\n",
    "img1_rectified = cv2.warpPerspective(img1, H1, (w,h))\n",
    "\n",
    "# write images to disk\n",
    "path0 = str(sgm_path / 'rectified' / (stem0 + \"_rectified.jpg\"))\n",
    "path1 = str(sgm_path / 'rectified' / (stem1 + \"_rectified.jpg\"))\n",
    "cv2.imwrite(path0, img0_rectified)\n",
    "cv2.imwrite(path1, img1_rectified)\n",
    "\n",
    "if fast_viz: \n",
    "else:\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    plt.imshow(cv2.cvtColor(img0_rectified, cv2.COLOR_BGR2RGB))\n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "    plt.imshow(cv2.cvtColor(img1_rectified, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b24eca-3890-413f-89ab-e04b58c0d7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PSMNet to compute disparity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd300e1-4aef-4ebf-860d-e89631720a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find epilines corresponding to points in right image (second image) and drawing its lines on left image\n",
    "img0 = cv2.imread(images[0][0], flags=cv2.IMREAD_COLOR)\n",
    "img1 = cv2.imread(images[1][0], flags=cv2.IMREAD_COLOR)\n",
    "\n",
    "lines0 = cv2.computeCorrespondEpilines(pts1.reshape(-1,1,2), 2, F)\n",
    "lines0 = lines0.reshape(-1,3)\n",
    "img0_epiplines, _ = draw_epip_lines(img0,img1,lines0,pts0,pts1)\n",
    "\n",
    "# Find epilines corresponding to points in left image (first image) and drawing its lines on right image\n",
    "lines1 = cv2.computeCorrespondEpilines(pts0.reshape(-1,1,2), 1, F)\n",
    "lines1 = lines1.reshape(-1,3)\n",
    "img1_epiplines,_ = draw_epip_lines(img1,img0,lines1,pts1,pts0, fast_viz=True)\n",
    "\n",
    "if fast_viz:\n",
    "    cv2.imwrite(str(sgm_path / (stem0 + \"_epiplines.jpg\")), img0_epiplines)\n",
    "    cv2.imwrite(str(sgm_path / (stem1 + \"_epiplines.jpg\")), img1_epiplines)\n",
    "else: \n",
    "    plt.subplot(121),plt.imshow(img0_epiplines)\n",
    "    plt.subplot(122),plt.imshow(img1_epiplines)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f14be7-6b0c-44fb-8fa3-ca870877a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw keypoints and matches\n",
    "pts0_rect = cv2.perspectiveTransform(np.float32(pts0).reshape(-1,1,2), H0).reshape(-1,2)\n",
    "pts1_rect = cv2.perspectiveTransform(np.float32(pts1).reshape(-1,1,2), H1).reshape(-1,2)\n",
    "\n",
    "# img0_rect_kpts = img0.copy()\n",
    "img0 = cv2.imread(images[0][0], flags=cv2.IMREAD_GRAYSCALE)\n",
    "img1 = cv2.imread(images[1][0], flags=cv2.IMREAD_GRAYSCALE)\n",
    "pts0, pts1 = features[0]['mkpts0'], features[0]['mkpts1']\n",
    "img0_kpts = cv2.drawKeypoints(img0,cv2.KeyPoint.convert(pts0),img0,(),flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "img1_kpts = cv2.drawKeypoints(img1,cv2.KeyPoint.convert(pts1),img1,(0,0,255),flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "cv2.imwrite('kpts0.jpg', img0_kpts)\n",
    "cv2.imwrite('kpts1.jpg', img1_kpts)\n",
    "\n",
    "make_matching_plot(img0, img1, pts0, pts1, path='matches.jpg')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd4fc9c-18d0-4d40-8135-767966e369f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139f4061-0dc5-435f-a0fa-963539d051f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SGM OPENCV\n",
    "# blockSize = 5\n",
    "# min_disp = 128\n",
    "# max_disp = 512\n",
    "# num_disp = max_disp-min_disp\n",
    "# stereo = cv2.StereoSGBM_create(minDisparity = min_disp,\n",
    "#     numDisparities = num_disp,\n",
    "#     blockSize = blockSize,\n",
    "#     P1 = 8*1*blockSize**2,\n",
    "#     P2 = 32*1*blockSize**2,\n",
    "#     disp12MaxDiff = 0,\n",
    "#     uniquenessRatio = 5,\n",
    "#     speckleWindowSize = 100,\n",
    "#     speckleRange = 2,\n",
    "# )\n",
    "\n",
    "# imgR = img0_rectified\n",
    "# imgL = img1_rectified\n",
    "# print('computing disparity...')\n",
    "# disparity_SGBM = stereo.compute(imgL, imgR)\n",
    "# disparity_SGBM = cv2.normalize(disparity_SGBM, disparity_SGBM, alpha=255,\n",
    "#                               beta=0, norm_type=cv2.NORM_MINMAX)\n",
    "# # disparity_SGBM = cv2.validateDisparity(disparity_SGBM, cost, minDisparity, numberOfDisparities\n",
    "# cv2.imwrite(str(sgm_path / \"disparity_SGBM_norm.png\"), disparity_SGBM)\n",
    "# print('done')\n",
    "\n",
    "# cv2.imshow(\"Disparity\", cv2.resize(disparity_SGBM, (1920,1080)))\n",
    "# cv2.waitKey()\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "# print('generating 3d point cloud...',)\n",
    "# h, w = imgL.shape[:2]\n",
    "# points = cv2.reprojectImageTo3D(disp, K)\n",
    "# colors = cv.cvtColor(imgL, cv.COLOR_BGR2RGB)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
