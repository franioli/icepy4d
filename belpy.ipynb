{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Image datastores created successfully.\n",
      "Loaded previous matches\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pydegensac\n",
    "import open3d as o3d\n",
    "\n",
    "from pathlib import Path\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "from lib.classes import (Camera, Imageds, Features, Targets)\n",
    "from lib.sfm.two_view_geometry import Two_view_geometry\n",
    "from lib.sfm.absolute_orientation import (Absolute_orientation, \n",
    "                                          Space_resection,\n",
    "                                          )\n",
    "from lib.sfm.triangulation import Triangulate\n",
    "from lib.match_pairs import match_pair\n",
    "from lib.track_matches import track_matches\n",
    "\n",
    "from lib.geometry import (project_points,\n",
    "                          compute_reprojection_error\n",
    "                          )\n",
    "from lib.utils import (build_dsm,\n",
    "                       generate_ortophoto,\n",
    "                       )\n",
    "from lib.point_clouds import (create_point_cloud,\n",
    "                              write_ply,\n",
    "                              )\n",
    "from lib.visualization import (display_point_cloud,\n",
    "                               display_pc_inliers,\n",
    "                               plot_features,\n",
    "                               plot_projections,\n",
    "                               )\n",
    "from lib.misc import create_directory\n",
    "from lib.config import parse_yaml_cfg, validate_inputs\n",
    "\n",
    "# Parse options from yaml file\n",
    "cfg_file = 'config/config_base.yaml'\n",
    "cfg = parse_yaml_cfg(cfg_file)\n",
    "\n",
    "# Inizialize Variables\n",
    "cams = cfg.paths.cam_names\n",
    "features = dict.fromkeys(cams)  # @TODO: put this in an inizialization function\n",
    "\n",
    "# Create Image Datastore objects\n",
    "images = dict.fromkeys(cams)  # @TODO: put this in an inizialization function\n",
    "for cam in cams:\n",
    "    images[cam] = Imageds(cfg.paths.imdir / cam)\n",
    "\n",
    "cfg = validate_inputs(cfg, images)\n",
    "\n",
    "''' Perform matching and tracking '''\n",
    "# Load matching and tracking configurations\n",
    "with open(cfg.matching_cfg) as f:\n",
    "    opt_matching = edict(json.load(f))\n",
    "with open(cfg.tracking_cfg) as f:\n",
    "    opt_tracking = edict(json.load(f))\n",
    "\n",
    "# epoch = 0\n",
    "if cfg.proc.do_matching:\n",
    "    for cam in cams:\n",
    "        features[cam] = []\n",
    "\n",
    "    for epoch in cfg.proc.epoch_to_process:\n",
    "        print(f'Processing epoch {epoch}...')\n",
    "\n",
    "        # opt_matching = cfg.matching.copy()\n",
    "        epochdir = Path(cfg.paths.resdir) / f'epoch_{epoch}'\n",
    "\n",
    "        #-- Find Matches at current epoch --#\n",
    "        print(f'Run Superglue to find matches at epoch {epoch}')\n",
    "        opt_matching.output_dir = epochdir\n",
    "        pair = [\n",
    "            images[cams[0]].get_image_path(epoch),\n",
    "            images[cams[1]].get_image_path(epoch)\n",
    "        ]\n",
    "        # Call matching function\n",
    "        matchedPts, matchedDescriptors, matchedPtsScores = match_pair(\n",
    "            pair, cfg.images.bbox, opt_matching\n",
    "        )\n",
    "\n",
    "        # Store matches in features structure\n",
    "        for jj, cam in enumerate(cams):\n",
    "            # Dict keys are the cameras names, internal list contain epoches\n",
    "            features[cam].append(Features())\n",
    "            features[cam][epoch].append_features({\n",
    "                'kpts': matchedPts[jj],\n",
    "                'descr': matchedDescriptors[jj],\n",
    "                'score': matchedPtsScores[jj]\n",
    "            })\n",
    "            # @TODO: Store match confidence!\n",
    "\n",
    "        #=== Track previous matches at current epoch ===#\n",
    "        if cfg.proc.do_tracking and epoch > 0:\n",
    "            print(f'Track points from epoch {epoch-1} to epoch {epoch}')\n",
    "\n",
    "            trackoutdir = epochdir / f'from_t{epoch-1}'\n",
    "            opt_tracking['output_dir'] = trackoutdir\n",
    "            pairs = [\n",
    "                [images[cams[0]].get_image_path(epoch-1),\n",
    "                    images[cams[0]].get_image_path(epoch)],\n",
    "                [images[cams[1]].get_image_path(epoch-1),\n",
    "                    images[cams[1]].get_image_path(epoch)],\n",
    "            ]\n",
    "            prevs = [\n",
    "                features[cams[0]][epoch-1].get_features_as_dict(),\n",
    "                features[cams[1]][epoch-1].get_features_as_dict()\n",
    "            ]\n",
    "            # Call actual tracking function\n",
    "            tracked_cam0, tracked_cam1 = track_matches(\n",
    "                pairs, cfg.images.bbox, prevs, opt_tracking)\n",
    "            # @TODO: keep track of the epoch in which feature is matched\n",
    "            # @TODO: Check bounding box in tracking\n",
    "            # @TODO: clean tracking code\n",
    "\n",
    "            # Store all matches in features structure\n",
    "            features[cams[0]][epoch].append_features(tracked_cam0)\n",
    "            features[cams[1]][epoch].append_features(tracked_cam1)\n",
    "\n",
    "        # Run Pydegensac to estimate F matrix and reject outliers\n",
    "        F, inlMask = pydegensac.findFundamentalMatrix(\n",
    "            features[cams[0]][epoch].get_keypoints(),\n",
    "            features[cams[1]][epoch].get_keypoints(),\n",
    "            px_th=1.5, conf=0.99999, max_iters=10000,\n",
    "            laf_consistensy_coef=-1.0,\n",
    "            error_type='sampson',\n",
    "            symmetric_error_check=True,\n",
    "            enable_degeneracy_check=True,\n",
    "        )\n",
    "        print(f'Matching at epoch {epoch}: pydegensac found {inlMask.sum()} \\\n",
    "            inliers ({inlMask.sum()*100/len(features[cams[0]][epoch]):.2f}%)')\n",
    "        features[cams[0]][epoch].remove_outliers_features(inlMask)\n",
    "        features[cams[1]][epoch].remove_outliers_features(inlMask)\n",
    "\n",
    "        # Write matched points to disk\n",
    "        im_stems = images[cams[0]].get_image_stem(\n",
    "            epoch), images[cams[1]].get_image_stem(epoch)\n",
    "        for jj, cam in enumerate(cams):\n",
    "            features[cam][epoch].save_as_txt(\n",
    "                epochdir / f'{im_stems[jj]}_mktps.txt')\n",
    "        with open(epochdir / f'{im_stems[0]}_{im_stems[1]}_features.pickle', 'wb') as f:\n",
    "            pickle.dump(features, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        last_match_path = create_directory('res/last_epoch')\n",
    "        with open(last_match_path / 'last_features.pickle', 'wb') as f:\n",
    "            pickle.dump(features, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        print('Matching completed')\n",
    "\n",
    "elif not features[cams[0]]:\n",
    "    last_match_path = 'res/last_epoch/last_features.pickle'\n",
    "    with open(last_match_path, 'rb') as f:\n",
    "        features = pickle.load(f)\n",
    "        print('Loaded previous matches')\n",
    "else:\n",
    "    print('Features already present, nothing was changed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructing epoch 0...\n",
      "Using OPENCV camera model + k3\n",
      "Using OPENCV camera model + k3\n",
      "Relative Orientation - valid points: 153/178\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1289.jpg\n",
      "Points color interpolated\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "''' SfM '''\n",
    "\n",
    "\n",
    "# Initialize variables @TODO: build function for variable inizialization\n",
    "cameras = dict.fromkeys(cams)\n",
    "cameras[cams[0]], cameras[cams[1]] = [], []\n",
    "pcd = []\n",
    "tform = []\n",
    "im_height, im_width = 4000, 6000\n",
    "# @TODO: store this information in exif inside an Image Class\n",
    "\n",
    "# Read target image coordinates and object coordinates \n",
    "targets = []\n",
    "for epoch in cfg.proc.epoch_to_process:\n",
    "    \n",
    "    p1_path = cfg.georef.target_dir / (\n",
    "        images[cams[0]].get_image_stem(epoch)+cfg.georef.target_file_ext\n",
    "        )\n",
    "                        \n",
    "    p2_path = cfg.georef.target_dir / (\n",
    "        images[cams[1]].get_image_stem(epoch)+cfg.georef.target_file_ext\n",
    "        )\n",
    "\n",
    "    targets.append(Targets(\n",
    "        im_file_path=[p1_path, p2_path],\n",
    "        obj_file_path='data/target_world_p1.csv'\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "for epoch in cfg.proc.epoch_to_process:\n",
    "    # epoch = 0\n",
    "    print(f'Reconstructing epoch {epoch}...')\n",
    "\n",
    "    # Initialize Intrinsics\n",
    "    ''' Inizialize Camera Intrinsics at every epoch setting them equal to\n",
    "        the reference cameras ones.\n",
    "    '''\n",
    "    # @TODO: replace append with insert or a more robust data structure...\n",
    "    for cam in cams:\n",
    "        cameras[cam].append(\n",
    "            Camera(\n",
    "                width=im_width,\n",
    "                height=im_height,\n",
    "                calib_path=cfg.paths.caldir / f'{cam}.txt'\n",
    "            )\n",
    "        )\n",
    "            \n",
    "    #--- At the first epoch, perform Space resection of the first camera by using GCPs. At all other epoches, set camera 1 EO equal to first one. ---#\n",
    "    # if epoch == 0: \n",
    "        # ''' Initialize Single_camera_geometry class with a cameras object'''\n",
    "        # targets_to_use = ['T2','T3','T4','F2' ]\n",
    "        # space_resection = Space_resection(cameras[cams[0]][epoch])\n",
    "        # space_resection.estimate(\n",
    "        #     targets[epoch].extract_image_coor_by_label(targets_to_use,cam_id=0),\n",
    "        #     targets[epoch].extract_object_coor_by_label(targets_to_use)\n",
    "        #     )\n",
    "        # # Store result in camera 0 object\n",
    "        # cameras[cams[0]][epoch] = space_resection.camera\n",
    "    # else:\n",
    "    #     cameras[cams[0]][epoch] = cameras[cams[0]][0]\n",
    "    \n",
    "    #--- Perform Relative orientation of the two cameras ---#\n",
    "    ''' Initialize Two_view_geometry class with a list containing the two cameras and a list contaning the matched features location on each camera.\n",
    "    '''\n",
    "    relative_ori = Two_view_geometry(\n",
    "        [cameras[cams[0]][epoch], cameras[cams[1]][epoch]],\n",
    "        [features[cams[0]][epoch].get_keypoints(),\n",
    "         features[cams[1]][epoch].get_keypoints()],\n",
    "    )\n",
    "    relative_ori.relative_orientation(\n",
    "        threshold=1.5, \n",
    "        confidence=0.999999, \n",
    "        scale_factor=261.60624 # 272.888187  #  baseline_world24\n",
    "        )\n",
    "    # Store result in camera 1 object\n",
    "    cameras[cams[1]][epoch] = relative_ori.cameras[1]\n",
    "\n",
    "    #--- Triangulate Points ---#\n",
    "    ''' Initialize Triangulate class with a list containing the two cameras\n",
    "        and a list contaning the matched features location on each camera.\n",
    "        Triangulated points are saved as points3d proprierty of the\n",
    "        Triangulate object (eg., triangulation.points3d)\n",
    "    '''\n",
    "    triangulation = Triangulate(\n",
    "        [cameras[cams[0]][epoch], \n",
    "         cameras[cams[1]][epoch]],\n",
    "        [features[cams[0]][epoch].get_keypoints(),\n",
    "         features[cams[1]][epoch].get_keypoints()]\n",
    "    )\n",
    "    triangulation.triangulate_two_views()\n",
    "    triangulation.interpolate_colors_from_image(\n",
    "        images[cams[1]][epoch],\n",
    "        cameras[cams[1]][epoch],\n",
    "        convert_BRG2RGB=True,\n",
    "    )\n",
    "    points3d = triangulation.points3d\n",
    "    \n",
    "    # Absolute orientation (-> coregistration on stable points)\n",
    "    # targets_to_use = ['T2', 'F2'] # 'T4',\n",
    "    # abs_ori = Absolute_orientation(\n",
    "    #     (cameras[cams[0]][epoch], cameras[cams[1]][epoch]),\n",
    "    #     points3d_world=targets[epoch].extract_object_coor_by_label(targets_to_use),\n",
    "    #     image_points=(\n",
    "    #         targets[epoch].extract_image_coor_by_label(targets_to_use, cam_id=0),\n",
    "    #         targets[epoch].extract_image_coor_by_label(targets_to_use, cam_id=1),\n",
    "    #     )\n",
    "    # )\n",
    "    # T = abs_ori.estimate_transformation(\n",
    "    #     estimate_scale=True,\n",
    "    #     add_camera_centers=True,\n",
    "    #     camera_centers_world=tuple(cfg.georef.camera_centers_world)\n",
    "    # )                 \n",
    "    # points3d = abs_ori.apply_transformation(points3d=points3d)\n",
    "    \n",
    "    # Create point cloud and save .ply to disk\n",
    "    pcd_epc = create_point_cloud(\n",
    "        points3d, triangulation.colors)\n",
    "\n",
    "    # Filter outliers in point cloud with SOR filter\n",
    "    if cfg.other.do_SOR_filter:\n",
    "        _, ind = pcd_epc.remove_statistical_outlier(nb_neighbors=10,\n",
    "                                                    std_ratio=3.0)\n",
    "        #     display_pc_inliers(pcd_epc, ind)\n",
    "        pcd_epc = pcd_epc.select_by_index(ind)\n",
    "        print('Point cloud filtered by Statistical Oulier Removal')\n",
    "\n",
    "\n",
    "    # Write point cloud to disk and store it in Point Cloud List\n",
    "    write_ply(pcd_epc, f'res/pt_clouds/sparse_pts_t{epoch}.ply')\n",
    "    pcd.append(pcd_epc)\n",
    "\n",
    "print('Done.')\n",
    "\n",
    "\n",
    "# Visualize point cloud\n",
    "display_point_cloud(\n",
    "    pcd,\n",
    "    [cameras[cams[0]][epoch], cameras[cams[1]][epoch]],\n",
    "    plot_scale=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point triangulation succeded: 1.0.\n",
      "points3d_loc:\n",
      "[[-1.18003740e+02 -5.33216773e+01  6.47109548e+02]\n",
      " [-1.64581465e+02 -2.77938994e+01  6.53455977e+02]\n",
      " [ 1.97367794e+01  3.32878780e+01  1.32161881e+02]\n",
      " [ 7.86066064e+01 -2.22302808e+00  2.54460008e+02]\n",
      " [ 8.05393163e+01  1.47906477e+01  1.86589377e+02]\n",
      " [-2.34013202e+00  2.63451791e+01  1.13815316e+02]\n",
      " [-4.31106398e+01 -1.02363771e+01  3.74378606e+02]\n",
      " [ 1.51343228e+01 -2.52279178e-02  2.46939683e+02]\n",
      " [ 6.69601443e+00  1.96381453e+01  2.08905036e+02]\n",
      " [ 1.64087197e+01  1.78807927e+01  2.12910331e+02]\n",
      " [ 3.83476255e+01  2.99636052e+01  1.94078566e+02]\n",
      " [ 6.07183821e+01  1.47867795e+01  2.29541008e+02]\n",
      " [ 1.42903750e+01 -8.85494793e+00  3.54189050e+02]\n",
      " [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00]\n",
      " [ 2.54143414e+02 -6.20094797e+01 -1.94283192e+00]]\n",
      "points3d_world:\n",
      "[[-499.85501099  402.03009033  240.37449646]\n",
      " [-543.4710083   365.41210938  217.79049683]\n",
      " [  49.6487999   192.0874939    71.74659729]\n",
      " [ -15.55490017  315.31710815  117.34619904]\n",
      " [  41.55960083  274.7651062    92.02519989]\n",
      " [  53.24010086  163.04490662   78.29599762]\n",
      " [-194.74259949  287.70449829  150.11509705]\n",
      " [ -47.12659836  257.75338745  119.07489777]\n",
      " [ -21.95439911  227.59559631   95.20700073]\n",
      " [ -19.52910042  238.1006012    96.6108017 ]\n",
      " [   8.27680016  244.67500305   80.66320038]\n",
      " [  -6.82959986  285.01300049   98.51799774]\n",
      " [-141.93559265  324.25119019  141.52149963]\n",
      " [ 151.703        99.171        91.618     ]\n",
      " [ 312.93        300.536       135.159     ]]\n",
      "[[ 6.47167150e-01 -1.43768240e-01 -8.91474828e-01  1.56844598e+02]\n",
      " [ 8.99788477e-01  1.03504263e-02  6.51533238e-01  8.44362574e+01]\n",
      " [-7.60090269e-02 -1.10156505e+00  1.22470673e-01  9.13875025e+01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1.4600722487604436, 0.06847119947727255, 0.9472715744955945)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Absolute orientation (-> coregistration on stable points)\n",
    "from thirdparty.transformations import euler_from_matrix\n",
    "\n",
    "\n",
    "# targets_to_use = ['T2', 'T4', 'F2'] # ,\n",
    "targets_to_use = [\n",
    "    'T2','T4','F2', 'F3', 'F4', 'F5',\n",
    "    '11', '12', '13', '15', '16', '17', '19'\n",
    "    ]\n",
    "abs_ori = Absolute_orientation(\n",
    "    (cameras[cams[0]][epoch], cameras[cams[1]][epoch]),\n",
    "    points3d_world=targets[epoch].extract_object_coor_by_label(targets_to_use),\n",
    "    image_points=(\n",
    "        targets[epoch].extract_image_coor_by_label(targets_to_use, cam_id=0),\n",
    "        targets[epoch].extract_image_coor_by_label(targets_to_use, cam_id=1),\n",
    "    )\n",
    ")\n",
    "abs_ori.points3d_loc\n",
    "abs_ori.points3d_world\n",
    "\n",
    "v0 = np.concatenate(\n",
    "    (abs_ori.points3d_loc,\n",
    "    abs_ori.cameras[0].C.reshape(1, -1),\n",
    "    abs_ori.cameras[1].C.reshape(1, -1),\n",
    "    )\n",
    ")\n",
    "print(f'points3d_loc:\\n{v0}')\n",
    "\n",
    "v1 = np.concatenate(\n",
    "    (abs_ori.points3d_world,\n",
    "     cfg.georef.camera_centers_world)\n",
    ")\n",
    "print(f'points3d_world:\\n{v1}')\n",
    "\n",
    "def write_targets_on_file(\n",
    "    path: str,\n",
    "    points: np.ndarray,\n",
    "    labels: str,\n",
    "):\n",
    "    path = Path(path)\n",
    "    file = open(path, 'w')\n",
    "    for label, pt in zip(labels, points):\n",
    "        file.write(\n",
    "            f\"{label},{pt[0]},{pt[1]},{pt[2]}\\n\"\n",
    "        )\n",
    "    file.close()\n",
    "    \n",
    "\n",
    "path = Path('tmp/points3d_loc.txt')\n",
    "write_targets_on_file(\n",
    "    path,\n",
    "    v0, \n",
    "    targets_to_use\n",
    ")\n",
    "\n",
    "path = Path('tmp/points3d_world.txt')\n",
    "write_targets_on_file(\n",
    "    path,\n",
    "    v1, \n",
    "    targets_to_use\n",
    ")\n",
    "\n",
    "\n",
    "T = abs_ori.estimate_transformation(\n",
    "    estimate_scale=True,\n",
    "    add_camera_centers=True,\n",
    "    camera_centers_world=tuple(cfg.georef.camera_centers_world)\n",
    ")       \n",
    "print(T)          \n",
    "# points3d = abs_ori.apply_transformation(points3d=points3d)\n",
    "\n",
    "euler_from_matrix(T[:3,:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "# plot_features(\n",
    "#     images[cams[0]][0],\n",
    "#     targets[epoch].extract_image_coor_by_label(targets_to_use, cam_id=0),\n",
    "#     title = f'targets img {images[cams[0]].get_image_stem(0)}'\n",
    "#     )\n",
    "\n",
    "# plot_features(\n",
    "#     images[cams[1]][0],\n",
    "#     targets[epoch].extract_image_coor_by_label(targets_to_use, cam_id=1),\n",
    "#     title = f'targets img {images[cams[1]].get_image_stem(0)}'\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image IMG_2814.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from PIL import Image\n",
    "# %matplotlib inline\n",
    "\n",
    "targets_to_use = [\n",
    "    'T2','T4','F2', 'F3', 'F4', 'F5',\n",
    "    '11', '12', '13', '15', '16', '17', '19'\n",
    "    ]\n",
    "\n",
    "img = images[cams[0]][0]\n",
    "pts = targets[epoch].extract_image_coor_by_label(targets_to_use, cam_id=0)\n",
    "title = f'targets_{images[cams[0]].get_image_stem(0)}'\n",
    "\n",
    "pts_col=(0, 0, 255)\n",
    "point_size=2\n",
    "for pt in pts:\n",
    "    x0, y0 = map(int, np.round(pt))\n",
    "    cv2.circle(img, (x0, y0), point_size,\n",
    "                    pts_col, -1, lineType=cv2.LINE_AA)\n",
    "cv2.imwrite(f'{title}.jpg', img)\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.imshow(\n",
    "#     cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# )\n",
    "# plt.show()\n",
    "    \n",
    "# cv2.namedWindow(title,cv2.WINDOW_FULLSCREEN)\n",
    "# cv2.imshow(title,img)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.imshow(im)\n",
    "# ax.scatter(pts[:, 0], pts[:, 1],\n",
    "#             s=6, c='y', marker='o',\n",
    "#             alpha=0.8, edgecolors='r',\n",
    "#             linewidths=1,\n",
    "#             )\n",
    "# ax.set_title(title)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For lmfit BBA \n",
    "\n",
    "from lib.io import export_keypoints_by_image, export_points3D\n",
    "\n",
    "print(cameras['p1'][0].t)\n",
    "print(cameras['p1'][0].R)\n",
    "\n",
    "print(cameras['p2'][0].t)\n",
    "print(cameras['p2'][0].R)\n",
    "\n",
    "\n",
    "epoch = 0\n",
    "export_keypoints_by_image(\n",
    "    features=features,\n",
    "    imageds=images,\n",
    "    path = './tmp',\n",
    "    epoch=epoch,\n",
    ")\n",
    "export_points3D(\n",
    "    'tmp/points3d_280722_for_bba.txt',\n",
    "    points3D=np.asarray(pcd[epoch].points)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute orientation (-> coregistration on stable points)\n",
    "targets_to_use = ['T2', 'F2'] # 'T4',\n",
    "abs_ori = Absolute_orientation(\n",
    "    (cameras[cams[0]][epoch], cameras[cams[1]][epoch]),\n",
    "    points3d_world=targets[epoch].extract_object_coor_by_label(targets_to_use),\n",
    "    image_points=(\n",
    "        targets[epoch].extract_image_coor_by_label(targets_to_use, cam_id=0),\n",
    "        targets[epoch].extract_image_coor_by_label(targets_to_use, cam_id=1),\n",
    "    )\n",
    ")\n",
    "T = abs_ori.estimate_transformation(\n",
    "    estimate_scale=True,\n",
    "    add_camera_centers=True,\n",
    "    camera_centers_world=tuple(cfg.georef.camera_centers_world)\n",
    ")\n",
    "print(abs_ori.points3d_loc)\n",
    "print(abs_ori.points3d_world)\n",
    "\n",
    "points3d = abs_ori.apply_transformation(points3d=points3d)\n",
    "print(f'Estimated transformation: \\n{abs_ori.tform}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' For CALGE'''\n",
    "\n",
    "# CAMERA EXTERIOR ORIENTATION\n",
    "from thirdparty.transformations import euler_from_matrix\n",
    "\n",
    "print(cameras[cams[0]][0].get_C_from_pose() )\n",
    "print(cameras[cams[1]][0].get_C_from_pose() )\n",
    "print(np.array(euler_from_matrix(cameras['p1'][0].R)) * 200/np.pi)\n",
    "print(np.array(euler_from_matrix(cameras['p2'][0].R)) * 200/np.pi)\n",
    "\n",
    "baseline_world = np.linalg.norm(\n",
    "    cfg.georef.camera_centers_world[0] - cfg.georef.camera_centers_world[1]\n",
    ")\n",
    "\n",
    "print(baseline_world)\n",
    "\n",
    "\n",
    "# SAVE HOMOLOGOUS POINTS\n",
    "# NB: Remember to disable SOR filter when computing 3d coordinates of TPs\n",
    "from lib.io import export_keypoints_for_calge, export_points3D_for_calge\n",
    "\n",
    "from thirdparty.transformations import euler_from_matrix\n",
    "\n",
    "epoch = 0\n",
    "export_keypoints_for_calge('simulaCalge/keypoints_280722.txt',\n",
    "                           features=features,\n",
    "                           imageds=images,\n",
    "                           epoch=epoch,\n",
    "                           pixel_size_micron=3.773\n",
    "                           )\n",
    "export_points3D_for_calge('simulaCalge/points3D_280722.txt',\n",
    "                           points3D=np.asarray(pcd[epoch].points)\n",
    "                           )\n",
    "\n",
    "print(cameras['p1'][0].C)\n",
    "print(cameras['p2'][0].C)\n",
    "\n",
    "\n",
    "print(np.array(euler_from_matrix(cameras['p1'][0].R)) * 200/np.pi)\n",
    "print(np.array(euler_from_matrix(cameras['p2'][0].R)) * 200/np.pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Compute DSM and orthophotos '''\n",
    "# @TODO: implement better DSM class\n",
    "\n",
    "print('DSM and orthophoto generation started')\n",
    "res = 0.03\n",
    "xlim = [-100., 80.]\n",
    "ylim = [-10., 65.]\n",
    "\n",
    "dsms = []\n",
    "ortofoto = dict.fromkeys(cams)\n",
    "ortofoto[cams[0]], ortofoto[cams[1]] = [], []\n",
    "for epoch in cfg.proc.epoch_to_process:\n",
    "    print(f'Epoch {epoch}')\n",
    "    dsms.append(build_dsm(np.asarray(pcd[epoch].points),\n",
    "                          dsm_step=res,\n",
    "                          xlim=xlim, ylim=ylim,\n",
    "                          make_dsm_plot=False,\n",
    "                          # fill_value = ,\n",
    "                          save_path=f'res/dsm/dsm_app_epoch_{epoch}.tif'\n",
    "                          ))\n",
    "    print('DSM built.')\n",
    "    for cam in cams:\n",
    "        fout_name = f'res/ortofoto/ortofoto_app_cam_{cam}_epc_{epoch}.tif'\n",
    "        ortofoto[cam].append(generate_ortophoto(cv2.cvtColor(images[cam][epoch], cv2.COLOR_BGR2RGB),\n",
    "                                                dsms[epoch], cameras[cam][epoch],\n",
    "                                                xlim=xlim, ylim=ylim,\n",
    "                                                save_path=fout_name,\n",
    "                                                ))\n",
    "    print('Orthophotos built.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('belpy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "794f75d38a1e2f6efa023a40c1f5318845dbfdeb01d810874a0706b78b83ae2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
