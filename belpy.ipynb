{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image datastores created successfully.\n",
      "Processing epoch 0...\n",
      "Run Superglue to find matches at epoch 0\n",
      "Will not resize images\n",
      "Running inference on device \"cuda\"\n",
      "Loaded SuperPoint model\n",
      "Loaded SuperGlue model (\"outdoor\" weights)\n",
      "Will write matches to directory \"res/epoch_0\"\n",
      "Will write visualization images to directory \"res/epoch_0\"\n",
      "Warning: input resolution is very large, results may vary\n",
      "Warning: input resolution is very large, results may vary\n",
      "Images subdivided in 2x4 tiles\n",
      "[Finished Tile Pairs  0 -  0 of  8] matcher=3.455 viz_match=0.158 total=3.613 sec {0.3 FPS} \n",
      "[Finished Tile Pairs  1 -  1 of  8] matcher=2.492 viz_match=0.158 total=2.650 sec {0.4 FPS} \n",
      "[Finished Tile Pairs  2 -  2 of  8] matcher=1.818 viz_match=0.158 total=1.977 sec {0.5 FPS} \n",
      "[Finished Tile Pairs  3 -  3 of  8] matcher=1.346 viz_match=0.157 total=1.503 sec {0.7 FPS} \n",
      "[Finished Tile Pairs  4 -  4 of  8] matcher=1.016 viz_match=0.158 total=1.175 sec {0.9 FPS} \n",
      "[Finished Tile Pairs  5 -  5 of  8] matcher=0.785 viz_match=0.157 total=0.942 sec {1.1 FPS} \n",
      "[Finished Tile Pairs  6 -  6 of  8] matcher=0.623 viz_match=0.161 total=0.784 sec {1.3 FPS} \n",
      "[Finished Tile Pairs  7 -  7 of  8] matcher=0.510 viz_match=0.158 total=0.668 sec {1.5 FPS} \n",
      "[Finished pair] load_image=0.313 create_tiles=0.010 viz_match=7.207 total=7.530 sec {0.1 FPS} \n",
      "Matching at epoch 0: pydegensac found 178             inliers (23.42%)\n",
      "Matching completed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pydegensac\n",
    "import open3d as o3d\n",
    "\n",
    "from pathlib import Path\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "from lib.classes import (Camera, Imageds, Features, Targets)\n",
    "from lib.sfm.two_view_geometry import Two_view_geometry\n",
    "from lib.sfm.absolute_orientation import (Absolute_orientation, \n",
    "                                          Space_resection,\n",
    "                                          )\n",
    "from lib.sfm.triangulation import Triangulate\n",
    "from lib.match_pairs import match_pair\n",
    "from lib.track_matches import track_matches\n",
    "\n",
    "from lib.geometry import (project_points,\n",
    "                          compute_reprojection_error\n",
    "                          )\n",
    "from lib.utils import (build_dsm,\n",
    "                       generate_ortophoto,\n",
    "                       )\n",
    "from lib.point_clouds import (create_point_cloud,\n",
    "                              write_ply,\n",
    "                              )\n",
    "from lib.visualization import (display_point_cloud,\n",
    "                               display_pc_inliers,\n",
    "                               plot_features,\n",
    "                               plot_projections,\n",
    "                               )\n",
    "from lib.misc import create_directory\n",
    "from lib.config import parse_yaml_cfg, validate_inputs\n",
    "\n",
    "# Parse options from yaml file\n",
    "cfg_file = 'config/config_base.yaml'\n",
    "cfg = parse_yaml_cfg(cfg_file)\n",
    "\n",
    "# Inizialize Variables\n",
    "cams = cfg.paths.cam_names\n",
    "features = dict.fromkeys(cams)  # @TODO: put this in an inizialization function\n",
    "\n",
    "# Create Image Datastore objects\n",
    "images = dict.fromkeys(cams)  # @TODO: put this in an inizialization function\n",
    "for cam in cams:\n",
    "    images[cam] = Imageds(cfg.paths.imdir / cam)\n",
    "\n",
    "cfg = validate_inputs(cfg, images)\n",
    "\n",
    "''' Perform matching and tracking '''\n",
    "# Load matching and tracking configurations\n",
    "with open(cfg.matching_cfg) as f:\n",
    "    opt_matching = edict(json.load(f))\n",
    "with open(cfg.tracking_cfg) as f:\n",
    "    opt_tracking = edict(json.load(f))\n",
    "\n",
    "# epoch = 0\n",
    "if cfg.proc.do_matching:\n",
    "    for cam in cams:\n",
    "        features[cam] = []\n",
    "\n",
    "    for epoch in cfg.proc.epoch_to_process:\n",
    "        print(f'Processing epoch {epoch}...')\n",
    "\n",
    "        # opt_matching = cfg.matching.copy()\n",
    "        epochdir = Path(cfg.paths.resdir) / f'epoch_{epoch}'\n",
    "\n",
    "        #-- Find Matches at current epoch --#\n",
    "        print(f'Run Superglue to find matches at epoch {epoch}')\n",
    "        opt_matching.output_dir = epochdir\n",
    "        pair = [\n",
    "            images[cams[0]].get_image_path(epoch),\n",
    "            images[cams[1]].get_image_path(epoch)\n",
    "        ]\n",
    "        # Call matching function\n",
    "        matchedPts, matchedDescriptors, matchedPtsScores = match_pair(\n",
    "            pair, cfg.images.bbox, opt_matching\n",
    "        )\n",
    "\n",
    "        # Store matches in features structure\n",
    "        for jj, cam in enumerate(cams):\n",
    "            # Dict keys are the cameras names, internal list contain epoches\n",
    "            features[cam].append(Features())\n",
    "            features[cam][epoch].append_features({\n",
    "                'kpts': matchedPts[jj],\n",
    "                'descr': matchedDescriptors[jj],\n",
    "                'score': matchedPtsScores[jj]\n",
    "            })\n",
    "            # @TODO: Store match confidence!\n",
    "\n",
    "        #=== Track previous matches at current epoch ===#\n",
    "        if cfg.proc.do_tracking and epoch > 0:\n",
    "            print(f'Track points from epoch {epoch-1} to epoch {epoch}')\n",
    "\n",
    "            trackoutdir = epochdir / f'from_t{epoch-1}'\n",
    "            opt_tracking['output_dir'] = trackoutdir\n",
    "            pairs = [\n",
    "                [images[cams[0]].get_image_path(epoch-1),\n",
    "                    images[cams[0]].get_image_path(epoch)],\n",
    "                [images[cams[1]].get_image_path(epoch-1),\n",
    "                    images[cams[1]].get_image_path(epoch)],\n",
    "            ]\n",
    "            prevs = [\n",
    "                features[cams[0]][epoch-1].get_features_as_dict(),\n",
    "                features[cams[1]][epoch-1].get_features_as_dict()\n",
    "            ]\n",
    "            # Call actual tracking function\n",
    "            tracked_cam0, tracked_cam1 = track_matches(\n",
    "                pairs, cfg.images.bbox, prevs, opt_tracking)\n",
    "            # @TODO: keep track of the epoch in which feature is matched\n",
    "            # @TODO: Check bounding box in tracking\n",
    "            # @TODO: clean tracking code\n",
    "\n",
    "            # Store all matches in features structure\n",
    "            features[cams[0]][epoch].append_features(tracked_cam0)\n",
    "            features[cams[1]][epoch].append_features(tracked_cam1)\n",
    "\n",
    "        # Run Pydegensac to estimate F matrix and reject outliers\n",
    "        F, inlMask = pydegensac.findFundamentalMatrix(\n",
    "            features[cams[0]][epoch].get_keypoints(),\n",
    "            features[cams[1]][epoch].get_keypoints(),\n",
    "            px_th=1.5, conf=0.99999, max_iters=10000,\n",
    "            laf_consistensy_coef=-1.0,\n",
    "            error_type='sampson',\n",
    "            symmetric_error_check=True,\n",
    "            enable_degeneracy_check=True,\n",
    "        )\n",
    "        print(f'Matching at epoch {epoch}: pydegensac found {inlMask.sum()} \\\n",
    "            inliers ({inlMask.sum()*100/len(features[cams[0]][epoch]):.2f}%)')\n",
    "        features[cams[0]][epoch].remove_outliers_features(inlMask)\n",
    "        features[cams[1]][epoch].remove_outliers_features(inlMask)\n",
    "\n",
    "        # Write matched points to disk\n",
    "        im_stems = images[cams[0]].get_image_stem(\n",
    "            epoch), images[cams[1]].get_image_stem(epoch)\n",
    "        for jj, cam in enumerate(cams):\n",
    "            features[cam][epoch].save_as_txt(\n",
    "                epochdir / f'{im_stems[jj]}_mktps.txt')\n",
    "        with open(epochdir / f'{im_stems[0]}_{im_stems[1]}_features.pickle', 'wb') as f:\n",
    "            pickle.dump(features, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        last_match_path = create_directory('res/last_epoch')\n",
    "        with open(last_match_path / 'last_features.pickle', 'wb') as f:\n",
    "            pickle.dump(features, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        print('Matching completed')\n",
    "\n",
    "elif not features[cams[0]]:\n",
    "    last_match_path = 'res/last_epoch/last_features.pickle'\n",
    "    with open(last_match_path, 'rb') as f:\n",
    "        features = pickle.load(f)\n",
    "        print('Loaded previous matches')\n",
    "else:\n",
    "    print('Features already present, nothing was changed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructing epoch 0...\n",
      "Using OPENCV camera model + k3\n",
      "Space resection succeded. Number of inlier points: 4/4\n",
      "Using OPENCV camera model + k3\n",
      "Space resection succeded. Number of inlier points: 4/4\n",
      "Relative Orientation - valid points: 153/178\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1289.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "''' SfM '''\n",
    "\n",
    "\n",
    "# Initialize variables @TODO: build function for variable inizialization\n",
    "cameras = dict.fromkeys(cams)\n",
    "cameras[cams[0]], cameras[cams[1]] = [], []\n",
    "pcd = []\n",
    "tform = []\n",
    "im_height, im_width = 4000, 6000\n",
    "# @TODO: store this information in exif inside an Image Class\n",
    "\n",
    "# Read target image coordinates and object coordinates \n",
    "targets = []\n",
    "for epoch in cfg.proc.epoch_to_process:\n",
    "    \n",
    "    p1_path = cfg.georef.target_dir / (\n",
    "        images[cams[0]].get_image_stem(epoch)+cfg.georef.target_file_ext\n",
    "        )\n",
    "                        \n",
    "    p2_path = cfg.georef.target_dir / (\n",
    "        images[cams[1]].get_image_stem(epoch)+cfg.georef.target_file_ext\n",
    "        )\n",
    "\n",
    "    targets.append(Targets(\n",
    "        im_file_path=[p1_path, p2_path],\n",
    "        obj_file_path='data/target_world_p1.csv'\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "for epoch in cfg.proc.epoch_to_process:\n",
    "    # epoch = 0\n",
    "    print(f'Reconstructing epoch {epoch}...')\n",
    "\n",
    "    # Initialize Intrinsics\n",
    "    ''' Inizialize Camera Intrinsics at every epoch setting them equal to\n",
    "        the reference cameras ones.\n",
    "    '''\n",
    "    # @TODO: replace append with insert or a more robust data structure...\n",
    "    for cam in cams:\n",
    "        cameras[cam].append(\n",
    "            Camera(\n",
    "                width=im_width,\n",
    "                height=im_height,\n",
    "                calib_path=cfg.paths.caldir / f'{cam}.txt'\n",
    "            )\n",
    "        )\n",
    "            \n",
    "    #--- At the first epoch, perform Space resection of the first camera by using GCPs. At all other epoches, set camera 1 EO equal to first one. ---#\n",
    "    # if epoch == 0: \n",
    "        ''' Initialize Single_camera_geometry class with a cameras object'''\n",
    "        targets_to_use = ['T2','T3','T4','F2' ]\n",
    "        space_resection = Space_resection(cameras[cams[0]][epoch])\n",
    "        space_resection.estimate(\n",
    "            targets[epoch].extract_image_coor_by_label(targets_to_use,cam_id=0),\n",
    "            targets[epoch].extract_object_coor_by_label(targets_to_use)\n",
    "            )\n",
    "        # Store result in camera 0 object\n",
    "        cameras[cams[0]][epoch] = space_resection.camera\n",
    "    # else:\n",
    "    #     cameras[cams[0]][epoch] = cameras[cams[0]][0]\n",
    "    \n",
    "    #--- Perform Relative orientation of the two cameras ---#\n",
    "    ''' Initialize Two_view_geometry class with a list containing the two cameras and a list contaning the matched features location on each camera.\n",
    "    '''\n",
    "    relative_ori = Two_view_geometry(\n",
    "        [cameras[cams[0]][epoch], cameras[cams[1]][epoch]],\n",
    "        [features[cams[0]][epoch].get_keypoints(),\n",
    "         features[cams[1]][epoch].get_keypoints()],\n",
    "    )\n",
    "    relative_ori.relative_orientation(\n",
    "        threshold=1.5, \n",
    "        confidence=0.999999, \n",
    "        scale_factor=261.606245022935 # 272.888187  #  baseline_world24\n",
    "        )\n",
    "    # Store result in camera 1 object\n",
    "    cameras[cams[1]][epoch] = relative_ori.cameras[1]\n",
    "\n",
    "    #--- Triangulate Points ---#\n",
    "    ''' Initialize Triangulate class with a list containing the two cameras\n",
    "        and a list contaning the matched features location on each camera.\n",
    "        Triangulated points are saved as points3d proprierty of the\n",
    "        Triangulate object (eg., triangulation.points3d)\n",
    "    '''\n",
    "    triangulation = Triangulate(\n",
    "        [cameras[cams[0]][epoch], \n",
    "         cameras[cams[1]][epoch]],\n",
    "        [features[cams[0]][epoch].get_keypoints(),\n",
    "         features[cams[1]][epoch].get_keypoints()]\n",
    "    )\n",
    "    triangulation.triangulate_two_views()\n",
    "    triangulation.interpolate_colors_from_image(\n",
    "        images[cams[1]][epoch],\n",
    "        cameras[cams[1]][epoch],\n",
    "        convert_BRG2RGB=True,\n",
    "    )\n",
    "    points3d = triangulation.points3d\n",
    "    \n",
    "    # # Absolute orientation (-> coregistration on stable points)\n",
    "    # targets_to_use = ['T2', 'F2'] # 'T4',\n",
    "    # abs_ori = Absolute_orientation(\n",
    "    #     (cameras[cams[0]][epoch], cameras[cams[1]][epoch]),\n",
    "    #     points3d_world=targets[epoch].extract_object_coor_by_label(targets_to_use),\n",
    "    #     image_points=(\n",
    "    #         targets[epoch].extract_image_coor_by_label(targets_to_use, cam_id=0),\n",
    "    #         targets[epoch].extract_image_coor_by_label(targets_to_use, cam_id=1),\n",
    "    #     )\n",
    "    # )\n",
    "    # T = abs_ori.estimate_transformation(\n",
    "    #     estimate_scale=True,\n",
    "    #     add_camera_centers=True,\n",
    "    #     camera_centers_world=tuple(cfg.georef.camera_centers_world)\n",
    "    # )\n",
    "                                                \n",
    "    # points3d = abs_ori.apply_transformation(points3d=points3d)\n",
    "    \n",
    "    # Create point cloud and save .ply to disk\n",
    "    pcd_epc = create_point_cloud(\n",
    "        points3d, triangulation.colors)\n",
    "\n",
    "    # Filter outliers in point cloud with SOR filter\n",
    "    if cfg.other.do_SOR_filter:\n",
    "        _, ind = pcd_epc.remove_statistical_outlier(nb_neighbors=10,\n",
    "                                                    std_ratio=3.0)\n",
    "        #     display_pc_inliers(pcd_epc, ind)\n",
    "        pcd_epc = pcd_epc.select_by_index(ind)\n",
    "        print('Point cloud filtered by Statistical Oulier Removal')\n",
    "\n",
    "\n",
    "    # Write point cloud to disk and store it in Point Cloud List\n",
    "    write_ply(pcd_epc, f'res/pt_clouds/sparse_pts_t{epoch}.ply')\n",
    "    pcd.append(pcd_epc)\n",
    "\n",
    "print('Done.')\n",
    "\n",
    "\n",
    "# Visualize point cloud\n",
    "display_point_cloud(\n",
    "    pcd,\n",
    "    [cameras[cams[0]][epoch], cameras[cams[1]][epoch]],\n",
    "    plot_scale=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=open('cam0.txt','ab')\n",
    "# for epoch in cfg.proc.epoch_to_process:\n",
    "#     # print(f'Epoch {epoch}\\n: {cameras[cams[0]][epoch].C}' )\n",
    "#     np.savetxt(f, cameras[cams[0]][epoch].C.T)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-158.81315718]\n",
      " [ 108.19011592]\n",
      " [  58.13189154]]\n",
      "[[ 0.56935076  0.81892662 -0.07210347]\n",
      " [-0.13614594  0.00743009 -0.99066093]\n",
      " [-0.81074287  0.57385015  0.11572386]]\n",
      "[[-235.69749354]\n",
      " [ 124.45659994]\n",
      " [ 363.410587  ]]\n",
      "[[-0.19973149  0.97983753  0.00507472]\n",
      " [ 0.01583765  0.00840668 -0.99983924]\n",
      " [-0.97972267 -0.19961901 -0.01719741]]\n",
      "Marker exported successfully\n",
      "Points exported successfully\n"
     ]
    }
   ],
   "source": [
    "# For lmfit BBA \n",
    "\n",
    "def export_keypoints(\n",
    "    filename: str,\n",
    "    features: Features,\n",
    "    imageds: Imageds,\n",
    "    epoch: int = None,\n",
    ") -> None:\n",
    "    if epoch is not None:\n",
    "\n",
    "        cams = list(imageds.keys())\n",
    "\n",
    "        # Write header to file\n",
    "        file = open(filename, \"w\")\n",
    "        file.write(\"image_name, feature_id, x, y\\n\")\n",
    "\n",
    "        for cam in cams:\n",
    "            image_name = imageds[cam].get_image_name(epoch)\n",
    "\n",
    "            # Write image name line\n",
    "            # NB: must be manually modified if it contains characters of symbols\n",
    "            file.write(f\"{image_name}\\n\")\n",
    "\n",
    "            for id, kpt in enumerate(features[cam][epoch].get_keypoints()):\n",
    "                x, y = kpt\n",
    "                file.write(\n",
    "                        f\"{id},{x},{y} \\n\"\n",
    "                        )\n",
    "\n",
    "        file.close()\n",
    "        print(\"Marker exported successfully\")\n",
    "    else:\n",
    "        print('please, provide the epoch number.')\n",
    "        return\n",
    "\n",
    "\n",
    "def export_points3D(\n",
    "    filename: str,\n",
    "    points3D: np.ndarray,\n",
    ") -> None:\n",
    "    # Write header to file\n",
    "    file = open(filename, \"w\")\n",
    "    file.write(\"point_id, X, Y, Z\\n\")\n",
    "\n",
    "    for id, pt in enumerate(points3D):\n",
    "        file.write(f\"{id},{pt[0]},{pt[1]},{pt[2]}\\n\")\n",
    "\n",
    "    file.close()\n",
    "    print(\"Points exported successfully\")\n",
    "\n",
    "\n",
    "\n",
    "print(cameras['p1'][0].t)\n",
    "print(cameras['p1'][0].R)\n",
    "\n",
    "print(cameras['p2'][0].t)\n",
    "print(cameras['p2'][0].R)\n",
    "\n",
    "\n",
    "epoch = 0\n",
    "export_keypoints(\n",
    "    'keypoints_280722_for_bba.txt',\n",
    "    features=features,\n",
    "    imageds=images,\n",
    "    epoch=epoch,\n",
    ")\n",
    "export_points3D(\n",
    "    'points3d_280722_for_bba.txt',\n",
    "    points3D=np.asarray(pcd[epoch].points)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point triangulation succeded: 1.0.\n",
      "[[-432.29874426  370.04982024  224.99360535]\n",
      " [  52.95834577  187.08039129   70.15870865]\n",
      " [ 152.28005274   95.89346389   89.00149443]\n",
      " [ 308.56292808  300.96232373  133.27623934]]\n",
      "[[-499.85501099  402.03009033  240.37449646]\n",
      " [  49.6487999   192.0874939    71.74659729]\n",
      " [ 151.703        99.171        91.618     ]\n",
      " [ 312.93        300.536       135.159     ]]\n",
      "Estimated transformation: \n",
      "[[ 1.10039670e+00  1.61566153e-02 -1.65177377e-03 -2.24542228e+01]\n",
      " [-1.61561855e-02  1.10039790e+00  2.98151636e-04 -1.36942779e+01]\n",
      " [ 1.65597285e-03 -2.73870225e-04  1.10051526e+00 -7.60381713e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Absolute orientation (-> coregistration on stable points)\n",
    "targets_to_use = ['T2', 'F2'] # 'T4',\n",
    "abs_ori = Absolute_orientation(\n",
    "    (cameras[cams[0]][epoch], cameras[cams[1]][epoch]),\n",
    "    points3d_world=targets[epoch].extract_object_coor_by_label(targets_to_use),\n",
    "    image_points=(\n",
    "        targets[epoch].extract_image_coor_by_label(targets_to_use, cam_id=0),\n",
    "        targets[epoch].extract_image_coor_by_label(targets_to_use, cam_id=1),\n",
    "    )\n",
    ")\n",
    "T = abs_ori.estimate_transformation(\n",
    "    estimate_scale=True,\n",
    "    add_camera_centers=True,\n",
    "    camera_centers_world=tuple(cfg.georef.camera_centers_world)\n",
    ")\n",
    "print(abs_ori.points3d_loc)\n",
    "print(abs_ori.points3d_world)\n",
    "\n",
    "points3d = abs_ori.apply_transformation(points3d=points3d)\n",
    "print(f'Estimated transformation: \\n{abs_ori.tform}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' For CALGE'''\n",
    "\n",
    "# CAMERA EXTERIOR ORIENTATION\n",
    "from thirdparty.transformations import euler_from_matrix\n",
    "\n",
    "print(cameras[cams[0]][0].get_C_from_pose() )\n",
    "print(cameras[cams[1]][0].get_C_from_pose() )\n",
    "print(np.array(euler_from_matrix(cameras['p1'][0].R)) * 200/np.pi)\n",
    "print(np.array(euler_from_matrix(cameras['p2'][0].R)) * 200/np.pi)\n",
    "\n",
    "\n",
    "baseline_world = np.linalg.norm(\n",
    "    cfg.georef.camera_centers_world[0] - cfg.georef.camera_centers_world[1]\n",
    ")\n",
    "\n",
    "print(baseline_world)\n",
    "\n",
    "\n",
    "# SAVE HOMOLOGOUS POINTS\n",
    "# NB: Remember to disable SOR filter when computing 3d coordinates of TPs\n",
    "from lib.io import export_keypoints_for_calge, export_points3D_for_calge\n",
    "\n",
    "from thirdparty.transformations import euler_from_matrix\n",
    "\n",
    "epoch = 0\n",
    "export_keypoints_for_calge('simulaCalge/keypoints_280722.txt',\n",
    "                           features=features,\n",
    "                           imageds=images,\n",
    "                           epoch=epoch,\n",
    "                           pixel_size_micron=3.773\n",
    "                           )\n",
    "export_points3D_for_calge('simulaCalge/points3D_280722.txt',\n",
    "                           points3D=np.asarray(pcd[epoch].points)\n",
    "                           )\n",
    "\n",
    "print(cameras['p1'][0].C)\n",
    "print(cameras['p2'][0].C)\n",
    "\n",
    "\n",
    "print(np.array(euler_from_matrix(cameras['p1'][0].R)) * 200/np.pi)\n",
    "print(np.array(euler_from_matrix(cameras['p2'][0].R)) * 200/np.pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Compute DSM and orthophotos '''\n",
    "# @TODO: implement better DSM class\n",
    "\n",
    "print('DSM and orthophoto generation started')\n",
    "res = 0.03\n",
    "xlim = [-100., 80.]\n",
    "ylim = [-10., 65.]\n",
    "\n",
    "dsms = []\n",
    "ortofoto = dict.fromkeys(cams)\n",
    "ortofoto[cams[0]], ortofoto[cams[1]] = [], []\n",
    "for epoch in cfg.proc.epoch_to_process:\n",
    "    print(f'Epoch {epoch}')\n",
    "    dsms.append(build_dsm(np.asarray(pcd[epoch].points),\n",
    "                          dsm_step=res,\n",
    "                          xlim=xlim, ylim=ylim,\n",
    "                          make_dsm_plot=False,\n",
    "                          # fill_value = ,\n",
    "                          save_path=f'res/dsm/dsm_app_epoch_{epoch}.tif'\n",
    "                          ))\n",
    "    print('DSM built.')\n",
    "    for cam in cams:\n",
    "        fout_name = f'res/ortofoto/ortofoto_app_cam_{cam}_epc_{epoch}.tif'\n",
    "        ortofoto[cam].append(generate_ortophoto(cv2.cvtColor(images[cam][epoch], cv2.COLOR_BGR2RGB),\n",
    "                                                dsms[epoch], cameras[cam][epoch],\n",
    "                                                xlim=xlim, ylim=ylim,\n",
    "                                                save_path=fout_name,\n",
    "                                                ))\n",
    "    print('Orthophotos built.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('belpy_gdal')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5257680a82f661cea8699dc8fe4567e52d11c753044270df4ff2b694c33cdedf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
