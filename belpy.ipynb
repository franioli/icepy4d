{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Input parameters are valid.\n",
      "\n",
      "Image datastores created successfully.\n",
      "Processing epoch 0...\n",
      "Run Superglue to find matches at epoch 0\n",
      "Will not resize images\n",
      "Running inference on device \"cuda\"\n",
      "Loaded SuperPoint model\n",
      "Loaded SuperGlue model (\"outdoor\" weights)\n",
      "Will write matches to directory \"res/epoch_0\"\n",
      "Will write visualization images to directory \"res/epoch_0\"\n",
      "Warning: input resolution is very large, results may vary\n",
      "Warning: input resolution is very large, results may vary\n",
      "Images subdivided in 2x4 tiles\n",
      "[Finished Tile Pairs  0 -  0 of  8] matcher=6.168 viz_match=0.253 total=6.421 sec {0.2 FPS} \n",
      "[Finished Tile Pairs  1 -  1 of  8] matcher=5.748 viz_match=0.257 total=6.006 sec {0.2 FPS} \n",
      "[Finished Tile Pairs  2 -  2 of  8] matcher=5.094 viz_match=0.263 total=5.357 sec {0.2 FPS} \n",
      "[Finished Tile Pairs  3 -  3 of  8] matcher=5.008 viz_match=0.281 total=5.289 sec {0.2 FPS} \n",
      "[Finished Tile Pairs  4 -  4 of  8] matcher=4.365 viz_match=0.288 total=4.652 sec {0.2 FPS} \n",
      "[Finished Tile Pairs  5 -  5 of  8] matcher=4.496 viz_match=0.436 total=4.933 sec {0.2 FPS} \n",
      "[Finished Tile Pairs  6 -  6 of  8] matcher=4.489 viz_match=0.392 total=4.881 sec {0.2 FPS} \n",
      "[Finished Tile Pairs  7 -  7 of  8] matcher=4.579 viz_match=0.345 total=4.924 sec {0.2 FPS} \n",
      "[Finished pair] load_image=0.310 create_tiles=0.010 viz_match=40.182 total=40.502 sec {0.0 FPS} \n",
      "Matching at epoch 0: pydegensac found 3911             inliers (53.01%)\n",
      "Matching completed\n"
     ]
    }
   ],
   "source": [
    "from lib.validate_inputs import validate\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pydegensac\n",
    "import open3d as o3d\n",
    "\n",
    "from pathlib import Path\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "from lib.classes import (Camera, Imageds, Features, Targets)\n",
    "from lib.config import parse_yaml_cfg\n",
    "from lib.sfm.two_view_geometry import Two_view_geometry, Single_camera_geometry\n",
    "from lib.sfm.triangulation import Triangulate\n",
    "from lib.match_pairs import match_pair\n",
    "from lib.track_matches import track_matches\n",
    "\n",
    "from lib.geometry import (project_points,\n",
    "                          compute_reprojection_error\n",
    "                          )\n",
    "from lib.utils import (build_dsm,\n",
    "                       generate_ortophoto,\n",
    "                       )\n",
    "from lib.point_clouds import (create_point_cloud,\n",
    "                              write_ply,\n",
    "                              )\n",
    "from lib.visualization import (display_point_cloud,\n",
    "                               display_pc_inliers,\n",
    "                               plot_features,\n",
    "                               plot_projections,\n",
    "                               )\n",
    "from lib.misc import (convert_to_homogeneous,\n",
    "                      convert_from_homogeneous,\n",
    "                      create_directory,\n",
    "                      )\n",
    "\n",
    "from thirdparty.transformations import affine_matrix_from_points\n",
    "\n",
    "# Parse options from yaml file\n",
    "cfg_file = 'config/config_base.yaml'\n",
    "cfg = parse_yaml_cfg(cfg_file)\n",
    "\n",
    "# Inizialize Variables\n",
    "cams = cfg.paths.cam_names\n",
    "features = dict.fromkeys(cams)  # @TODO: put this in an inizialization function\n",
    "\n",
    "# Create Image Datastore objects\n",
    "images = dict.fromkeys(cams)  # @TODO: put this in an inizialization function\n",
    "for cam in cams:\n",
    "    images[cam] = Imageds(cfg.paths.imdir / cam)\n",
    "\n",
    "cfg = validate(cfg, images)\n",
    "\n",
    "''' Perform matching and tracking '''\n",
    "# Load matching and tracking configurations\n",
    "with open(cfg.matching_cfg) as f:\n",
    "    opt_matching = edict(json.load(f))\n",
    "with open(cfg.tracking_cfg) as f:\n",
    "    opt_tracking = edict(json.load(f))\n",
    "\n",
    "# epoch = 0\n",
    "if cfg.proc.do_matching:\n",
    "    for cam in cams:\n",
    "        features[cam] = []\n",
    "\n",
    "    for epoch in cfg.proc.epoch_to_process:\n",
    "        print(f'Processing epoch {epoch}...')\n",
    "\n",
    "        # opt_matching = cfg.matching.copy()\n",
    "        epochdir = Path(cfg.paths.resdir) / f'epoch_{epoch}'\n",
    "\n",
    "        #-- Find Matches at current epoch --#\n",
    "        print(f'Run Superglue to find matches at epoch {epoch}')\n",
    "        opt_matching.output_dir = epochdir\n",
    "        pair = [\n",
    "            images[cams[0]].get_image_path(epoch),\n",
    "            images[cams[1]].get_image_path(epoch)\n",
    "        ]\n",
    "        # Call matching function\n",
    "        matchedPts, matchedDescriptors, matchedPtsScores = match_pair(\n",
    "            pair, cfg.images.bbox, opt_matching\n",
    "        )\n",
    "\n",
    "        # Store matches in features structure\n",
    "        for jj, cam in enumerate(cams):\n",
    "            # Dict keys are the cameras names, internal list contain epoches\n",
    "            features[cam].append(Features())\n",
    "            features[cam][epoch].append_features({\n",
    "                'kpts': matchedPts[jj],\n",
    "                'descr': matchedDescriptors[jj],\n",
    "                'score': matchedPtsScores[jj]\n",
    "            })\n",
    "            # @TODO: Store match confidence!\n",
    "\n",
    "        #=== Track previous matches at current epoch ===#\n",
    "        if cfg.proc.do_tracking and epoch > 0:\n",
    "            print(f'Track points from epoch {epoch-1} to epoch {epoch}')\n",
    "\n",
    "            trackoutdir = epochdir / f'from_t{epoch-1}'\n",
    "            opt_tracking['output_dir'] = trackoutdir\n",
    "            pairs = [\n",
    "                [images[cams[0]].get_image_path(epoch-1),\n",
    "                    images[cams[0]].get_image_path(epoch)],\n",
    "                [images[cams[1]].get_image_path(epoch-1),\n",
    "                    images[cams[1]].get_image_path(epoch)],\n",
    "            ]\n",
    "            prevs = [\n",
    "                features[cams[0]][epoch-1].get_features_as_dict(),\n",
    "                features[cams[1]][epoch-1].get_features_as_dict()\n",
    "            ]\n",
    "            # Call actual tracking function\n",
    "            tracked_cam0, tracked_cam1 = track_matches(\n",
    "                pairs, cfg.images.bbox, prevs, opt_tracking)\n",
    "            # @TODO: keep track of the epoch in which feature is matched\n",
    "            # @TODO: Check bounding box in tracking\n",
    "            # @TODO: clean tracking code\n",
    "\n",
    "            # Store all matches in features structure\n",
    "            features[cams[0]][epoch].append_features(tracked_cam0)\n",
    "            features[cams[1]][epoch].append_features(tracked_cam1)\n",
    "\n",
    "        # Run Pydegensac to estimate F matrix and reject outliers\n",
    "        F, inlMask = pydegensac.findFundamentalMatrix(\n",
    "            features[cams[0]][epoch].get_keypoints(),\n",
    "            features[cams[1]][epoch].get_keypoints(),\n",
    "            px_th=1.5, conf=0.99999, max_iters=10000,\n",
    "            laf_consistensy_coef=-1.0,\n",
    "            error_type='sampson',\n",
    "            symmetric_error_check=True,\n",
    "            enable_degeneracy_check=True,\n",
    "        )\n",
    "        print(f'Matching at epoch {epoch}: pydegensac found {inlMask.sum()} \\\n",
    "            inliers ({inlMask.sum()*100/len(features[cams[0]][epoch]):.2f}%)')\n",
    "        features[cams[0]][epoch].remove_outliers_features(inlMask)\n",
    "        features[cams[1]][epoch].remove_outliers_features(inlMask)\n",
    "\n",
    "        # Write matched points to disk\n",
    "        im_stems = images[cams[0]].get_image_stem(\n",
    "            epoch), images[cams[1]].get_image_stem(epoch)\n",
    "        for jj, cam in enumerate(cams):\n",
    "            features[cam][epoch].save_as_txt(\n",
    "                epochdir / f'{im_stems[jj]}_mktps.txt')\n",
    "        with open(epochdir / f'{im_stems[0]}_{im_stems[1]}_features.pickle', 'wb') as f:\n",
    "            pickle.dump(features, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        last_match_path = create_directory('res/last_epoch')\n",
    "        with open(last_match_path / 'last_features.pickle', 'wb') as f:\n",
    "            pickle.dump(features, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        print('Matching completed')\n",
    "\n",
    "elif not features[cams[0]]:\n",
    "    last_match_path = 'res/last_epoch/last_features.pickle',\n",
    "    with open(last_match_path, 'rb') as f:\n",
    "        features = pickle.load(f)\n",
    "        print(\"Loaded previous matches\")\n",
    "else:\n",
    "    print(\"Features already present, nothing was changed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructing epoch 0...\n",
      "Using OPENCV camera model.\n",
      "Using OPENCV camera model.\n",
      "Space resection succeded. Number of inlier points: 12/14\n",
      "Relative Orientation - valid points: 3464/3911\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1289.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "''' SfM '''\n",
    "\n",
    "# Initialize variables @TODO: build function for variable inizialization\n",
    "cameras = dict.fromkeys(cams)\n",
    "cameras[cams[0]], cameras[cams[1]] = [], []\n",
    "pcd = []\n",
    "tform = []\n",
    "im_height, im_width = 4000, 6000\n",
    "# @TODO: store this information in exif inside an Image Class\n",
    "\n",
    "# Read target image coordinates and object coordinates \n",
    "# @TODO: Put world coordinates file path in config and arrange a better way to store/read targets information for all epoches and all cams\n",
    "target_paths = [\"data/target_image_p1_all.csv\", \"data/target_image_p2.csv\"]\n",
    "targets = [Targets(\n",
    "    im_file_path=target_paths,\n",
    "    obj_file_path=\"data/target_world_p1.csv\"), ]\n",
    "\n",
    "for epoch in cfg.proc.epoch_to_process:\n",
    "    # epoch = 0\n",
    "    print(f'Reconstructing epoch {epoch}...')\n",
    "\n",
    "    # Initialize Intrinsics\n",
    "    ''' Inizialize Camera Intrinsics at every epoch setting them equal to\n",
    "        the reference cameras ones.\n",
    "    '''\n",
    "    # @TODO: replace append with insert or a more robust data structure...\n",
    "    for cam in cams:\n",
    "        cameras[cam].append(\n",
    "            Camera(\n",
    "                width=im_width,\n",
    "                height=im_height,\n",
    "                calib_path=cfg.paths.caldir / f'{cam}.txt'\n",
    "            )\n",
    "        )\n",
    "            \n",
    "    #--- Perform Space resection of the first camera by using GCPs ---#\n",
    "    ''' Initialize Single_camera_geometry class with a cameras object'''\n",
    "    \n",
    "    space_resection = Single_camera_geometry(cameras[cams[0]][epoch])\n",
    "    space_resection.space_resection(\n",
    "        targets[epoch].get_im_coord(cam_id=0),\n",
    "        targets[epoch].get_obj_coord()\n",
    "        )\n",
    "    # Store result in camera 0 object\n",
    "    cameras[cams[0]][epoch] = space_resection.camera\n",
    "    \n",
    "    #--- Perform Relative orientation of the two cameras ---#\n",
    "    ''' Initialize Two_view_geometry class with a list containing the two cameras and a list contaning the matched features location on each camera.\n",
    "    '''\n",
    "    relative_ori = Two_view_geometry(\n",
    "        [cameras[cams[0]][epoch], cameras[cams[1]][epoch]],\n",
    "        [features[cams[0]][epoch].get_keypoints(),\n",
    "         features[cams[1]][epoch].get_keypoints()],\n",
    "    )\n",
    "    relative_ori.relative_orientation(\n",
    "        threshold=1.5, \n",
    "        confidence=0.999999, \n",
    "        scale_factor=272.888187  # 261.606245022935baseline_world24\n",
    "        )\n",
    "    # Store result in camera 1 object\n",
    "    cameras[cams[1]][epoch] = relative_ori.cameras[1]\n",
    "\n",
    "    #--- Triangulate Points ---#\n",
    "    ''' Initialize Triangulate class with a list containing the two cameras\n",
    "        and a list contaning the matched features location on each camera.\n",
    "        Triangulated points are saved as points3d proprierty of the\n",
    "        Triangulate object (eg., triangulation.points3d)\n",
    "    '''\n",
    "    triangulation = Triangulate(\n",
    "        [cameras[cams[0]][epoch], \n",
    "         cameras[cams[1]][epoch]],\n",
    "        [features[cams[0]][epoch].get_keypoints(),\n",
    "         features[cams[1]][epoch].get_keypoints()]\n",
    "    )\n",
    "    triangulation.triangulate_two_views()\n",
    "    triangulation.interpolate_colors_from_image(\n",
    "        images[cams[1]][epoch],\n",
    "        cameras[cams[1]][epoch],\n",
    "        convert_BRG2RGB=True,\n",
    "    )\n",
    "\n",
    "    # Create point cloud and save .ply to disk\n",
    "    pcd_epc = create_point_cloud(\n",
    "        triangulation.points3d, triangulation.colors)\n",
    "\n",
    "    # Filter outliers in point cloud with SOR filter\n",
    "    if cfg.other.do_SOR_filter:\n",
    "        _, ind = pcd_epc.remove_statistical_outlier(nb_neighbors=10,\n",
    "                                                    std_ratio=3.0)\n",
    "        #     display_pc_inliers(pcd_epc, ind)\n",
    "        pcd_epc = pcd_epc.select_by_index(ind)\n",
    "        print(\"Point cloud filtered by Statistical Oulier Removal\")\n",
    "\n",
    "    # Write point cloud to disk and store it in Point Cloud List\n",
    "    write_ply(pcd_epc, f'res/pt_clouds/sparse_pts_t{epoch}.ply')\n",
    "    pcd.append(pcd_epc)\n",
    "\n",
    "print('Done.')\n",
    "\n",
    "\n",
    "# Visualize point cloud\n",
    "display_point_cloud(\n",
    "    pcd,\n",
    "    [cameras[cams[0]][epoch], cameras[cams[1]][epoch]],\n",
    "    plot_scale=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[151.88270478]\n",
      " [ 99.10170747]\n",
      " [ 91.49643179]]\n",
      "[[316.44830385]\n",
      " [311.88603018]\n",
      " [137.41982689]]\n",
      "[ 88.0598206   60.71644147 -14.47286423]\n",
      "[-107.15521219   88.0192793   193.16035831]\n",
      "261.6062450229352\n"
     ]
    }
   ],
   "source": [
    "from thirdparty.transformations import euler_from_matrix\n",
    "\n",
    "print(cameras[cams[0]][0].get_C_from_pose() )\n",
    "print(cameras[cams[1]][0].get_C_from_pose() )\n",
    "print(np.array(euler_from_matrix(cameras['p1'][0].R)) * 200/np.pi)\n",
    "print(np.array(euler_from_matrix(cameras['p2'][0].R)) * 200/np.pi)\n",
    "\n",
    "\n",
    "baseline_world = np.linalg.norm(\n",
    "    cfg.georef.camera_centers_world[0] - cfg.georef.camera_centers_world[1]\n",
    ")\n",
    "\n",
    "print(baseline_world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OPENCV camera model.\n",
      "Using OPENCV camera model.\n",
      "Relative Orientation - valid points: 3464/3911\n",
      "No scaling factor (e.g., computed from camera baseline) is provided. Two-view-geometry estimated up to a scale factor.\n",
      "Point triangulation succeded: 1.0.\n",
      "Point triangulation succeded: 1.0.\n",
      "Baseline in world ref system: 611.9359741210938\n",
      "Baseline in local ref system: 2.2255312457861023\n",
      "Computed world-local scale factor: 274.96175363961055\n"
     ]
    }
   ],
   "source": [
    "'''OLD'''\n",
    "# Camera baseline\n",
    "baseline_world = np.linalg.norm(\n",
    "    cfg.georef.camera_centers_world[0] - cfg.georef.camera_centers_world[1]\n",
    ")\n",
    "\n",
    "# cam_baseline = np.linalg.norm(\n",
    "#     cameras[cams[0]][0].get_C_from_pose() -\n",
    "#     cameras[cams[1]][0].get_C_from_pose()\n",
    "# )\n",
    "# scale_fct = baseline_world / cam_baseline\n",
    "# scale_fct = 261.60624502293524\n",
    "\n",
    "# Fix the EO of both the cameras as those estimated in the first epoch\n",
    "# if epoch > 0:\n",
    "#     for cam in cams:\n",
    "#         cameras[cam][epoch] = cameras[cam][0]\n",
    "#     print('Camera exterior orientation fixed to that of the master cameras.')\n",
    "\n",
    "\"\"\" Compute scale factor .... to be placed in two_view_geometry class\"\"\"\n",
    "# Perform Relative orientation of the two cameras in arbitrary local RS\n",
    "cameras = dict.fromkeys(cams)\n",
    "cameras[cams[0]], cameras[cams[1]] = [], []\n",
    "for cam in cams:\n",
    "    cameras[cam].append(\n",
    "        Camera(\n",
    "            width=im_width,\n",
    "            height=im_height,\n",
    "            calib_path=cfg.paths.caldir / f'{cam}.txt'\n",
    "        )\n",
    "    )\n",
    "relative_ori = Two_view_geometry(\n",
    "    [cameras[cams[0]][epoch], cameras[cams[1]][epoch]],\n",
    "    [features[cams[0]][epoch].get_keypoints(),\n",
    "        features[cams[1]][epoch].get_keypoints()],\n",
    ")\n",
    "relative_ori.relative_orientation(\n",
    "    threshold=1.5,\n",
    "    confidence=0.999999,\n",
    ")\n",
    "cameras[cams[1]][epoch] = relative_ori.cameras[1]\n",
    "\n",
    "'''Compute scale factor from targets'''\n",
    "# Read Targets \n",
    "target_paths = [\"data/target_image_p1_all.csv\", \"data/target_image_p2_all.csv\"]\n",
    "targets = [Targets(\n",
    "    im_file_path=target_paths,\n",
    "    obj_file_path=\"data/target_world_all.csv\"), ]\n",
    "\n",
    "# Extract targets for compute world-local scale factor\n",
    "targets_to_extract = [\"F2\", \"F3\"]\n",
    "im_coord = {}\n",
    "for id, cam in enumerate(cams):\n",
    "    im_coord[cam] = targets[0].extract_image_coor_by_label(\n",
    "        targets_to_extract, id)\n",
    "targets_world = targets[0].extract_object_coor_by_label(targets_to_extract)\n",
    "\n",
    "# Triangulate targets\n",
    "triangulation = Triangulate# Read Targets\n",
    "target_paths = [\"data/target_image_p1_all.csv\", \"data/target_image_p2_all.csv\"]\n",
    "targets = [Targets(\n",
    "    im_file_path=target_paths,\n",
    "    obj_file_path=\"data/target_world_all.csv\"), ]\n",
    "\n",
    "# Extract targets for compute world-local scale factor\n",
    "targets_to_extract = [\"F2\", \"T2\"]\n",
    "im_coord = {}\n",
    "for id, cam in enumerate(cams):\n",
    "    im_coord[cam] = targets[0].extract_image_coor_by_label(\n",
    "        targets_to_extract, id)\n",
    "targets_world = targets[0].extract_object_coor_by_label(targets_to_extract)\n",
    "\n",
    "# Triangulate targets\n",
    "triangulation = Triangulate(\n",
    "    [cameras[cams[0]][epoch],\n",
    "     cameras[cams[1]][epoch]],\n",
    "    [im_coord[cams[0]],\n",
    "     im_coord[cams[1]]]\n",
    ")\n",
    "targets_local = triangulation.triangulate_two_views()\n",
    "\n",
    "# Compute baselines and scale factor\n",
    "baseline_local = np.linalg.norm(\n",
    "    targets_local[0] - targets_local[1]\n",
    ")\n",
    "baseline_world = np.linalg.norm(\n",
    "    targets_world[0] - targets_world[1]\n",
    ")\n",
    "(\n",
    "    [cameras[cams[0]][epoch],\n",
    "     cameras[cams[1]][epoch]],\n",
    "    [im_coord[cams[0]],\n",
    "     im_coord[cams[1]]]\n",
    ")\n",
    "targets_local = triangulation.triangulate_two_views()\n",
    "\n",
    "# Compute baselines and scale factor\n",
    "baseline_local = np.linalg.norm(\n",
    "    targets_local[0] - targets_local[1]\n",
    ")\n",
    "baseline_world = np.linalg.norm(\n",
    "    targets_world[0] - targets_world[1]\n",
    ")\n",
    "\n",
    "\n",
    "# \"\"\"Compute scale factor from perspective centers from CALGE\"\"\"\n",
    "# C0 = np.array([151.8827,99.1017,91.49643])\n",
    "# C1 = np.array([328.4277,302.5639,135.11935])\n",
    "# baseline_world = np.linalg.norm(\n",
    "#     C0 - C1\n",
    "# )\n",
    "\n",
    "# baseline_local = np.linalg.norm(\n",
    "#     cameras[cams[0]][0].get_C_from_pose() -\n",
    "#     cameras[cams[1]][0].get_C_from_pose()\n",
    "# )\n",
    "\n",
    "\n",
    "scale_fct = baseline_world / baseline_local\n",
    "print(f\"Baseline in world ref system: {baseline_world}\")\n",
    "print(f\"Baseline in local ref system: {baseline_local}\")\n",
    "print(f\"Computed world-local scale factor: {scale_fct}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" For CALGE\"\"\"\n",
    "# NB: Remember to disable SOR filter when computing 3d coordinates of TPs\n",
    "\n",
    "from lib.io import export_keypoints_for_calge, export_points3D_for_calge\n",
    "\n",
    "from thirdparty.transformations import euler_from_matrix\n",
    "\n",
    "epoch = 0\n",
    "export_keypoints_for_calge('simulaCalge/keypoints_280722.txt',\n",
    "                           features=features,\n",
    "                           imageds=images,\n",
    "                           epoch=epoch,\n",
    "                           pixel_size_micron=3.773\n",
    "                           )\n",
    "export_points3D_for_calge('simulaCalge/points3D_280722.txt',\n",
    "                           points3D=np.asarray(pcd[epoch].points)\n",
    "                           )\n",
    "\n",
    "print(cameras['p1'][0].C)\n",
    "print(cameras['p2'][0].C)\n",
    "\n",
    "\n",
    "print(np.array(euler_from_matrix(cameras['p1'][0].R)) * 200/np.pi)\n",
    "print(np.array(euler_from_matrix(cameras['p2'][0].R)) * 200/np.pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Compute DSM and orthophotos '''\n",
    "# @TODO: implement better DSM class\n",
    "\n",
    "print('DSM and orthophoto generation started')\n",
    "res = 0.03\n",
    "xlim = [-100., 80.]\n",
    "ylim = [-10., 65.]\n",
    "\n",
    "dsms = []\n",
    "ortofoto = dict.fromkeys(cams)\n",
    "ortofoto[cams[0]], ortofoto[cams[1]] = [], []\n",
    "for epoch in cfg.proc.epoch_to_process:\n",
    "    print(f'Epoch {epoch}')\n",
    "    dsms.append(build_dsm(np.asarray(pcd[epoch].points),\n",
    "                          dsm_step=res,\n",
    "                          xlim=xlim, ylim=ylim,\n",
    "                          make_dsm_plot=False,\n",
    "                          # fill_value = ,\n",
    "                          save_path=f'res/dsm/dsm_app_epoch_{epoch}.tif'\n",
    "                          ))\n",
    "    print('DSM built.')\n",
    "    for cam in cams:\n",
    "        fout_name = f'res/ortofoto/ortofoto_app_cam_{cam}_epc_{epoch}.tif'\n",
    "        ortofoto[cam].append(generate_ortophoto(cv2.cvtColor(images[cam][epoch], cv2.COLOR_BGR2RGB),\n",
    "                                                dsms[epoch], cameras[cam][epoch],\n",
    "                                                xlim=xlim, ylim=ylim,\n",
    "                                                save_path=fout_name,\n",
    "                                                ))\n",
    "    print('Orthophotos built.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('belpy_gdal')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5257680a82f661cea8699dc8fe4567e52d11c753044270df4ff2b694c33cdedf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
