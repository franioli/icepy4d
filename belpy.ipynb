{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Image datastores created successfully.\n",
      "Loaded previous matches\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pydegensac\n",
    "import open3d as o3d\n",
    "\n",
    "from pathlib import Path\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "from lib.classes import (Camera, Imageds, Features, Targets)\n",
    "from lib.sfm.two_view_geometry import Two_view_geometry\n",
    "from lib.sfm.absolute_orientation import (Absolute_orientation, \n",
    "                                          Single_camera_geometry,\n",
    "                                          )\n",
    "from lib.sfm.triangulation import Triangulate\n",
    "from lib.match_pairs import match_pair\n",
    "from lib.track_matches import track_matches\n",
    "\n",
    "from lib.geometry import (project_points,\n",
    "                          compute_reprojection_error\n",
    "                          )\n",
    "from lib.utils import (build_dsm,\n",
    "                       generate_ortophoto,\n",
    "                       )\n",
    "from lib.point_clouds import (create_point_cloud,\n",
    "                              write_ply,\n",
    "                              )\n",
    "from lib.visualization import (display_point_cloud,\n",
    "                               display_pc_inliers,\n",
    "                               plot_features,\n",
    "                               plot_projections,\n",
    "                               )\n",
    "from lib.misc import create_directory\n",
    "from lib.config import parse_yaml_cfg, validate_inputs\n",
    "\n",
    "# Parse options from yaml file\n",
    "cfg_file = 'config/config_base.yaml'\n",
    "cfg = parse_yaml_cfg(cfg_file)\n",
    "\n",
    "# Inizialize Variables\n",
    "cams = cfg.paths.cam_names\n",
    "features = dict.fromkeys(cams)  # @TODO: put this in an inizialization function\n",
    "\n",
    "# Create Image Datastore objects\n",
    "images = dict.fromkeys(cams)  # @TODO: put this in an inizialization function\n",
    "for cam in cams:\n",
    "    images[cam] = Imageds(cfg.paths.imdir / cam)\n",
    "\n",
    "cfg = validate_inputs(cfg, images)\n",
    "\n",
    "''' Perform matching and tracking '''\n",
    "# Load matching and tracking configurations\n",
    "with open(cfg.matching_cfg) as f:\n",
    "    opt_matching = edict(json.load(f))\n",
    "with open(cfg.tracking_cfg) as f:\n",
    "    opt_tracking = edict(json.load(f))\n",
    "\n",
    "# epoch = 0\n",
    "if cfg.proc.do_matching:\n",
    "    for cam in cams:\n",
    "        features[cam] = []\n",
    "\n",
    "    for epoch in cfg.proc.epoch_to_process:\n",
    "        print(f'Processing epoch {epoch}...')\n",
    "\n",
    "        # opt_matching = cfg.matching.copy()\n",
    "        epochdir = Path(cfg.paths.resdir) / f'epoch_{epoch}'\n",
    "\n",
    "        #-- Find Matches at current epoch --#\n",
    "        print(f'Run Superglue to find matches at epoch {epoch}')\n",
    "        opt_matching.output_dir = epochdir\n",
    "        pair = [\n",
    "            images[cams[0]].get_image_path(epoch),\n",
    "            images[cams[1]].get_image_path(epoch)\n",
    "        ]\n",
    "        # Call matching function\n",
    "        matchedPts, matchedDescriptors, matchedPtsScores = match_pair(\n",
    "            pair, cfg.images.bbox, opt_matching\n",
    "        )\n",
    "\n",
    "        # Store matches in features structure\n",
    "        for jj, cam in enumerate(cams):\n",
    "            # Dict keys are the cameras names, internal list contain epoches\n",
    "            features[cam].append(Features())\n",
    "            features[cam][epoch].append_features({\n",
    "                'kpts': matchedPts[jj],\n",
    "                'descr': matchedDescriptors[jj],\n",
    "                'score': matchedPtsScores[jj]\n",
    "            })\n",
    "            # @TODO: Store match confidence!\n",
    "\n",
    "        #=== Track previous matches at current epoch ===#\n",
    "        if cfg.proc.do_tracking and epoch > 0:\n",
    "            print(f'Track points from epoch {epoch-1} to epoch {epoch}')\n",
    "\n",
    "            trackoutdir = epochdir / f'from_t{epoch-1}'\n",
    "            opt_tracking['output_dir'] = trackoutdir\n",
    "            pairs = [\n",
    "                [images[cams[0]].get_image_path(epoch-1),\n",
    "                    images[cams[0]].get_image_path(epoch)],\n",
    "                [images[cams[1]].get_image_path(epoch-1),\n",
    "                    images[cams[1]].get_image_path(epoch)],\n",
    "            ]\n",
    "            prevs = [\n",
    "                features[cams[0]][epoch-1].get_features_as_dict(),\n",
    "                features[cams[1]][epoch-1].get_features_as_dict()\n",
    "            ]\n",
    "            # Call actual tracking function\n",
    "            tracked_cam0, tracked_cam1 = track_matches(\n",
    "                pairs, cfg.images.bbox, prevs, opt_tracking)\n",
    "            # @TODO: keep track of the epoch in which feature is matched\n",
    "            # @TODO: Check bounding box in tracking\n",
    "            # @TODO: clean tracking code\n",
    "\n",
    "            # Store all matches in features structure\n",
    "            features[cams[0]][epoch].append_features(tracked_cam0)\n",
    "            features[cams[1]][epoch].append_features(tracked_cam1)\n",
    "\n",
    "        # Run Pydegensac to estimate F matrix and reject outliers\n",
    "        F, inlMask = pydegensac.findFundamentalMatrix(\n",
    "            features[cams[0]][epoch].get_keypoints(),\n",
    "            features[cams[1]][epoch].get_keypoints(),\n",
    "            px_th=1.5, conf=0.99999, max_iters=10000,\n",
    "            laf_consistensy_coef=-1.0,\n",
    "            error_type='sampson',\n",
    "            symmetric_error_check=True,\n",
    "            enable_degeneracy_check=True,\n",
    "        )\n",
    "        print(f'Matching at epoch {epoch}: pydegensac found {inlMask.sum()} \\\n",
    "            inliers ({inlMask.sum()*100/len(features[cams[0]][epoch]):.2f}%)')\n",
    "        features[cams[0]][epoch].remove_outliers_features(inlMask)\n",
    "        features[cams[1]][epoch].remove_outliers_features(inlMask)\n",
    "\n",
    "        # Write matched points to disk\n",
    "        im_stems = images[cams[0]].get_image_stem(\n",
    "            epoch), images[cams[1]].get_image_stem(epoch)\n",
    "        for jj, cam in enumerate(cams):\n",
    "            features[cam][epoch].save_as_txt(\n",
    "                epochdir / f'{im_stems[jj]}_mktps.txt')\n",
    "        with open(epochdir / f'{im_stems[0]}_{im_stems[1]}_features.pickle', 'wb') as f:\n",
    "            pickle.dump(features, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        last_match_path = create_directory('res/last_epoch')\n",
    "        with open(last_match_path / 'last_features.pickle', 'wb') as f:\n",
    "            pickle.dump(features, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        print('Matching completed')\n",
    "\n",
    "elif not features[cams[0]]:\n",
    "    last_match_path = 'res/last_epoch/last_features.pickle'\n",
    "    with open(last_match_path, 'rb') as f:\n",
    "        features = pickle.load(f)\n",
    "        print('Loaded previous matches')\n",
    "else:\n",
    "    print('Features already present, nothing was changed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructing epoch 0...\n",
      "Using OPENCV camera model + k3\n",
      "Using OPENCV camera model + k3\n",
      "Space resection succeded. Number of inlier points: 4/4\n",
      "Relative Orientation - valid points: 3127/3520\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1289.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Reconstructing epoch 1...\n",
      "Using OPENCV camera model + k3\n",
      "Using OPENCV camera model + k3\n",
      "Relative Orientation - valid points: 4526/4894\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1290.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Reconstructing epoch 2...\n",
      "Using OPENCV camera model + k3\n",
      "Using OPENCV camera model + k3\n",
      "Relative Orientation - valid points: 2934/3197\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1293.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Reconstructing epoch 3...\n",
      "Using OPENCV camera model + k3\n",
      "Using OPENCV camera model + k3\n",
      "Relative Orientation - valid points: 2787/3504\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1294.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Reconstructing epoch 4...\n",
      "Using OPENCV camera model + k3\n",
      "Using OPENCV camera model + k3\n",
      "Relative Orientation - valid points: 3086/3430\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1296.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Reconstructing epoch 5...\n",
      "Using OPENCV camera model + k3\n",
      "Using OPENCV camera model + k3\n",
      "Relative Orientation - valid points: 3175/3516\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1298.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Reconstructing epoch 6...\n",
      "Using OPENCV camera model + k3\n",
      "Using OPENCV camera model + k3\n",
      "Relative Orientation - valid points: 2449/2982\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1300.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Reconstructing epoch 7...\n",
      "Using OPENCV camera model + k3\n",
      "Using OPENCV camera model + k3\n",
      "Relative Orientation - valid points: 3737/4058\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1303.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Reconstructing epoch 8...\n",
      "Using OPENCV camera model + k3\n",
      "Using OPENCV camera model + k3\n",
      "Relative Orientation - valid points: 3389/4080\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1304.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Reconstructing epoch 9...\n",
      "Using OPENCV camera model + k3\n",
      "Using OPENCV camera model + k3\n",
      "Relative Orientation - valid points: 2700/3129\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1306.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Reconstructing epoch 10...\n",
      "Using OPENCV camera model + k3\n",
      "Using OPENCV camera model + k3\n",
      "Relative Orientation - valid points: 3455/3818\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1308.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Reconstructing epoch 11...\n",
      "Using OPENCV camera model + k3\n",
      "Using OPENCV camera model + k3\n",
      "Relative Orientation - valid points: 3114/3711\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1310.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Reconstructing epoch 12...\n",
      "Using OPENCV camera model + k3\n",
      "Using OPENCV camera model + k3\n",
      "Relative Orientation - valid points: 2927/3394\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1312.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Reconstructing epoch 13...\n",
      "Using OPENCV camera model + k3\n",
      "Using OPENCV camera model + k3\n",
      "Relative Orientation - valid points: 3072/3577\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1314.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Reconstructing epoch 14...\n",
      "Using OPENCV camera model + k3\n",
      "Using OPENCV camera model + k3\n",
      "Relative Orientation - valid points: 3414/3898\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1316.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Reconstructing epoch 15...\n",
      "Using OPENCV camera model + k3\n",
      "Using OPENCV camera model + k3\n",
      "Relative Orientation - valid points: 3157/3429\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1318.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Reconstructing epoch 16...\n",
      "Using OPENCV camera model + k3\n",
      "Using OPENCV camera model + k3\n",
      "Relative Orientation - valid points: 3252/3613\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1320.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Reconstructing epoch 17...\n",
      "Using OPENCV camera model + k3\n",
      "Using OPENCV camera model + k3\n",
      "Relative Orientation - valid points: 4216/4782\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1322.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Reconstructing epoch 18...\n",
      "Using OPENCV camera model + k3\n",
      "Using OPENCV camera model + k3\n",
      "Relative Orientation - valid points: 4313/5004\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1324.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Reconstructing epoch 19...\n",
      "Using OPENCV camera model + k3\n",
      "Using OPENCV camera model + k3\n",
      "Relative Orientation - valid points: 2875/3315\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1326.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Reconstructing epoch 20...\n",
      "Using OPENCV camera model + k3\n",
      "Using OPENCV camera model + k3\n",
      "Relative Orientation - valid points: 5019/5396\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1330.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Reconstructing epoch 21...\n",
      "Using OPENCV camera model + k3\n",
      "Using OPENCV camera model + k3\n",
      "Relative Orientation - valid points: 3717/3866\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1332.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Reconstructing epoch 22...\n",
      "Using OPENCV camera model + k3\n",
      "Using OPENCV camera model + k3\n",
      "Relative Orientation - valid points: 2731/3242\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1334.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Reconstructing epoch 23...\n",
      "Using OPENCV camera model + k3\n",
      "Using OPENCV camera model + k3\n",
      "Relative Orientation - valid points: 3015/3573\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1336.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Reconstructing epoch 24...\n",
      "Using OPENCV camera model + k3\n",
      "Using OPENCV camera model + k3\n",
      "Relative Orientation - valid points: 2485/2835\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1338.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Reconstructing epoch 25...\n",
      "Using OPENCV camera model + k3\n",
      "Using OPENCV camera model + k3\n",
      "Relative Orientation - valid points: 2838/3370\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1340.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Reconstructing epoch 26...\n",
      "Using OPENCV camera model + k3\n",
      "Using OPENCV camera model + k3\n",
      "Relative Orientation - valid points: 2811/3295\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1342.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "''' SfM '''\n",
    "\n",
    "\n",
    "# Initialize variables @TODO: build function for variable inizialization\n",
    "cameras = dict.fromkeys(cams)\n",
    "cameras[cams[0]], cameras[cams[1]] = [], []\n",
    "pcd = []\n",
    "tform = []\n",
    "im_height, im_width = 4000, 6000\n",
    "# @TODO: store this information in exif inside an Image Class\n",
    "\n",
    "# Read target image coordinates and object coordinates \n",
    "targets = []\n",
    "for epoch in cfg.proc.epoch_to_process:\n",
    "    \n",
    "    p1_path = cfg.georef.target_dir / (\n",
    "        images[cams[0]].get_image_stem(epoch)+cfg.georef.target_file_ext\n",
    "        )\n",
    "                        \n",
    "    p2_path = cfg.georef.target_dir / (\n",
    "        images[cams[1]].get_image_stem(epoch)+cfg.georef.target_file_ext\n",
    "        )\n",
    "\n",
    "    targets.append(Targets(\n",
    "        im_file_path=[p1_path, p2_path],\n",
    "        obj_file_path='data/target_world_p1.csv'\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "for epoch in cfg.proc.epoch_to_process:\n",
    "    # epoch = 0\n",
    "    print(f'Reconstructing epoch {epoch}...')\n",
    "\n",
    "    # Initialize Intrinsics\n",
    "    ''' Inizialize Camera Intrinsics at every epoch setting them equal to\n",
    "        the reference cameras ones.\n",
    "    '''\n",
    "    # @TODO: replace append with insert or a more robust data structure...\n",
    "    for cam in cams:\n",
    "        cameras[cam].append(\n",
    "            Camera(\n",
    "                width=im_width,\n",
    "                height=im_height,\n",
    "                calib_path=cfg.paths.caldir / f'{cam}.txt'\n",
    "            )\n",
    "        )\n",
    "            \n",
    "    #--- At the first epoch, perform Space resection of the first camera by using GCPs. At all other epoches, set camera 1 EO equal to first one. ---#\n",
    "    if epoch == 0: \n",
    "        ''' Initialize Single_camera_geometry class with a cameras object'''\n",
    "        targets_to_use = ['T2','T3','T4','F2' ]\n",
    "        space_resection = Single_camera_geometry(cameras[cams[0]][epoch])\n",
    "        space_resection.space_resection(\n",
    "            targets[epoch].extract_image_coor_by_label(targets_to_use,cam_id=0),\n",
    "            targets[epoch].extract_object_coor_by_label(targets_to_use)\n",
    "            )\n",
    "        # Store result in camera 0 object\n",
    "        cameras[cams[0]][epoch] = space_resection.camera\n",
    "    else:\n",
    "        cameras[cams[0]][epoch] = cameras[cams[0]][0]\n",
    "    \n",
    "    #--- Perform Relative orientation of the two cameras ---#\n",
    "    ''' Initialize Two_view_geometry class with a list containing the two cameras and a list contaning the matched features location on each camera.\n",
    "    '''\n",
    "    relative_ori = Two_view_geometry(\n",
    "        [cameras[cams[0]][epoch], cameras[cams[1]][epoch]],\n",
    "        [features[cams[0]][epoch].get_keypoints(),\n",
    "         features[cams[1]][epoch].get_keypoints()],\n",
    "    )\n",
    "    relative_ori.relative_orientation(\n",
    "        threshold=1.5, \n",
    "        confidence=0.999999, \n",
    "        scale_factor=261.606245022935 # 272.888187  #  baseline_world24\n",
    "        )\n",
    "    # Store result in camera 1 object\n",
    "    cameras[cams[1]][epoch] = relative_ori.cameras[1]\n",
    "\n",
    "    #--- Triangulate Points ---#\n",
    "    ''' Initialize Triangulate class with a list containing the two cameras\n",
    "        and a list contaning the matched features location on each camera.\n",
    "        Triangulated points are saved as points3d proprierty of the\n",
    "        Triangulate object (eg., triangulation.points3d)\n",
    "    '''\n",
    "    triangulation = Triangulate(\n",
    "        [cameras[cams[0]][epoch], \n",
    "         cameras[cams[1]][epoch]],\n",
    "        [features[cams[0]][epoch].get_keypoints(),\n",
    "         features[cams[1]][epoch].get_keypoints()]\n",
    "    )\n",
    "    triangulation.triangulate_two_views()\n",
    "    triangulation.interpolate_colors_from_image(\n",
    "        images[cams[1]][epoch],\n",
    "        cameras[cams[1]][epoch],\n",
    "        convert_BRG2RGB=True,\n",
    "    )\n",
    "    points3d = triangulation.points3d\n",
    "    \n",
    "    # # Absolute orientation (-> coregistration on stable points)\n",
    "    # targets_to_use = ['T2', 'F2'] # 'T4',\n",
    "    # abs_ori = Absolute_orientation(\n",
    "    #     (cameras[cams[0]][epoch], cameras[cams[1]][epoch]),\n",
    "    #     points3d_world=targets[epoch].extract_object_coor_by_label(targets_to_use),\n",
    "    #     image_points=(\n",
    "    #         targets[epoch].extract_image_coor_by_label(targets_to_use, cam_id=0),\n",
    "    #         targets[epoch].extract_image_coor_by_label(targets_to_use, cam_id=1),\n",
    "    #     )\n",
    "    # )\n",
    "    # T = abs_ori.estimate_transformation(\n",
    "    #     estimate_scale=True,\n",
    "    #     add_camera_centers=True,\n",
    "    #     camera_centers_world=tuple(cfg.georef.camera_centers_world)\n",
    "    # )\n",
    "                                                \n",
    "    # points3d = abs_ori.apply_transformation(points3d=points3d)\n",
    "    \n",
    "    # Create point cloud and save .ply to disk\n",
    "    pcd_epc = create_point_cloud(\n",
    "        points3d, triangulation.colors)\n",
    "\n",
    "    # Filter outliers in point cloud with SOR filter\n",
    "    if cfg.other.do_SOR_filter:\n",
    "        _, ind = pcd_epc.remove_statistical_outlier(nb_neighbors=10,\n",
    "                                                    std_ratio=3.0)\n",
    "        #     display_pc_inliers(pcd_epc, ind)\n",
    "        pcd_epc = pcd_epc.select_by_index(ind)\n",
    "        print('Point cloud filtered by Statistical Oulier Removal')\n",
    "\n",
    "\n",
    "    # Write point cloud to disk and store it in Point Cloud List\n",
    "    write_ply(pcd_epc, f'res/pt_clouds/sparse_pts_t{epoch}.ply')\n",
    "    pcd.append(pcd_epc)\n",
    "\n",
    "print('Done.')\n",
    "\n",
    "\n",
    "# Visualize point cloud\n",
    "display_point_cloud(\n",
    "    pcd,\n",
    "    [cameras[cams[0]][epoch], cameras[cams[1]][epoch]],\n",
    "    plot_scale=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List, Tuple\n",
    "\n",
    "# from lib.sfm.triangulation import Triangulate\n",
    "# from lib.misc import convert_to_homogeneous, convert_from_homogeneous\n",
    "\n",
    "\n",
    "# from thirdparty.transformations import affine_matrix_from_points\n",
    "\n",
    "\n",
    "# class Absolute_orientation():\n",
    "\n",
    "#     def __init__(self, \n",
    "#                  cameras: Tuple[Camera],\n",
    "#                  points3d_world: np.ndarray = None,\n",
    "#                  points3d_loc: np.ndarray = None,\n",
    "#                  image_points: Tuple[np.ndarray] = None,\n",
    "#                  ) -> None:\n",
    "        \n",
    "#         self.cameras = cameras\n",
    "#         if points3d_world is not None:\n",
    "#             self.points3d_world = points3d_world\n",
    "#         else:\n",
    "#             raise ValueError(\n",
    "#                 'Missing input for points in world reference system. Please, provide the their 3D coordinates in nx3 numpy array.')\n",
    "#         if points3d_loc is not None:\n",
    "#             self.points3d_loc = points3d_loc\n",
    "#         elif image_points is not None:\n",
    "#             self.points3d_loc = self.triangulate_image_points(image_points)\n",
    "#         else: \n",
    "#             raise ValueError('Missing input for points in local reference system. Please, provide the their 3D coordinates or the image points to be triangulated')\n",
    "            \n",
    "#     def triangulate_image_points(self, \n",
    "#                                  image_points: List[np.ndarray]\n",
    "#                                  ): # -> np.ndarray:\n",
    "#         #  @TODO: add possibility of using multiple cameras\n",
    "\n",
    "#         triangulation = Triangulate(\n",
    "#             self.cameras,\n",
    "#             image_points,\n",
    "#         )\n",
    "#         triangulation.triangulate_two_views()\n",
    "#         return triangulation.points3d\n",
    "    \n",
    "#     def estimate_rigid_body_transformation(self,\n",
    "#                         v0: np.ndarray = None,\n",
    "#                         v1: np.ndarray = None,\n",
    "#                         estimate_scale:  bool = False,\n",
    "#                         add_camera_centers: bool = False,\n",
    "#                         camera_centers_world: Tuple = None,\n",
    "#                         ) -> np.ndarray:\n",
    "#         ''' Wrapper around 'affine_matrix_from_points' function from 'transformation' \n",
    "#         '''\n",
    "#         if v0 is None:       \n",
    "#             v0 = self.points3d_loc\n",
    "#         if v1 is None: \n",
    "#             v1 = self.points3d_world\n",
    "            \n",
    "#         if add_camera_centers:\n",
    "#             if camera_centers_world is None:\n",
    "#                 raise ValueError('Missing camera_centers_world argument. Please, provide Tuple with coordinates of the camera centers in world reference system to be added')\n",
    "#             v0 = np.concatenate(\n",
    "#                 (v0, \n",
    "#                 #  @TODO: add possibility of using multiple cameras\n",
    "#                  self.cameras[0].C.reshape(1, -1),\n",
    "#                  self.cameras[1].C.reshape(1, -1),\n",
    "#                 )\n",
    "#             )\n",
    "#             # print(f'v0: {v0}')\n",
    "#             v1 = np.concatenate(\n",
    "#                 (v1, camera_centers_world)                 \n",
    "#             )\n",
    "#             # print(f'v1: {v1}')\n",
    "            \n",
    "#         self.tform = affine_matrix_from_points(\n",
    "#             v0.T, \n",
    "#             v1.T,\n",
    "#             shear=False,\n",
    "#             scale=estimate_scale,\n",
    "#             usesvd=True\n",
    "#         )\n",
    "#         return self.tform\n",
    "\n",
    "        \n",
    "#     def apply_transformation(self,\n",
    "#                             points3d: np.ndarray,\n",
    "#                             ) -> np.ndarray:\n",
    "#         # @TODO: Apply transformation also to cameras!print(pts3D)\n",
    "\n",
    "#         points3d = convert_to_homogeneous(points3d.T)\n",
    "#         points_out = self.tform @ points3d\n",
    "#         return convert_from_homogeneous(points_out)\n",
    "    \n",
    "\n",
    "# targets_to_use = ['T2', 'T4', 'F2']\n",
    "# abs_ori = Absolute_orientation(\n",
    "#     (cameras[cams[0]][epoch], cameras[cams[1]][epoch]),\n",
    "#     points3d_world=targets[epoch].extract_object_coor_by_label(targets_to_use),\n",
    "#     image_points=(\n",
    "#         targets[epoch].extract_image_coor_by_label(targets_to_use, cam_id=0),\n",
    "#         targets[epoch].extract_image_coor_by_label(targets_to_use, cam_id=1),\n",
    "#     )\n",
    "# )\n",
    "# T = abs_ori.estimate_rigid_body_transformation(estimate_scale=True,\n",
    "#                                                add_camera_centers=True,\n",
    "#                                                camera_centers_world=tuple(\n",
    "#                                                    cfg.georef.camera_centers_world)\n",
    "#                                                )                                \n",
    "# print(f'Estimated transformation: \\n{T}')\n",
    "# pts3D = np.asarray(pcd[0].points)\n",
    "# points_coreg = abs_ori.apply_transformation(points3d=pts3D)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets = []\n",
    "# target_dir = Path('data/targets')\n",
    "# target_file_ext = '.csv'\n",
    "# for epoch in cfg.proc.epoch_to_process:\n",
    "#     print(f'Epoch: {epoch}')\n",
    "\n",
    "#     p1_path = target_dir / (\n",
    "#         images[cams[0]].get_image_stem(epoch)+target_file_ext\n",
    "#     )\n",
    "\n",
    "#     p2_path = target_dir / (\n",
    "#         images[cams[1]].get_image_stem(epoch)+target_file_ext\n",
    "#     )\n",
    "#     print(f'p1_path: {p1_path}, p2_path: {p2_path}')\n",
    "#     targets.append(Targets(\n",
    "#         im_file_path=[p1_path, p2_path],\n",
    "#         obj_file_path='data/target_world_p1.csv'\n",
    "#         )\n",
    "#     )\n",
    "#     print(f'targets: {targets[epoch].im_coor}')\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' For CALGE'''\n",
    "\n",
    "# CAMERA EXTERIOR ORIENTATION\n",
    "from thirdparty.transformations import euler_from_matrix\n",
    "\n",
    "print(cameras[cams[0]][0].get_C_from_pose() )\n",
    "print(cameras[cams[1]][0].get_C_from_pose() )\n",
    "print(np.array(euler_from_matrix(cameras['p1'][0].R)) * 200/np.pi)\n",
    "print(np.array(euler_from_matrix(cameras['p2'][0].R)) * 200/np.pi)\n",
    "\n",
    "\n",
    "baseline_world = np.linalg.norm(\n",
    "    cfg.georef.camera_centers_world[0] - cfg.georef.camera_centers_world[1]\n",
    ")\n",
    "\n",
    "print(baseline_world)\n",
    "\n",
    "\n",
    "# SAVE HOMOLOGOUS POINTS\n",
    "# NB: Remember to disable SOR filter when computing 3d coordinates of TPs\n",
    "from lib.io import export_keypoints_for_calge, export_points3D_for_calge\n",
    "\n",
    "from thirdparty.transformations import euler_from_matrix\n",
    "\n",
    "epoch = 0\n",
    "export_keypoints_for_calge('simulaCalge/keypoints_280722.txt',\n",
    "                           features=features,\n",
    "                           imageds=images,\n",
    "                           epoch=epoch,\n",
    "                           pixel_size_micron=3.773\n",
    "                           )\n",
    "export_points3D_for_calge('simulaCalge/points3D_280722.txt',\n",
    "                           points3D=np.asarray(pcd[epoch].points)\n",
    "                           )\n",
    "\n",
    "print(cameras['p1'][0].C)\n",
    "print(cameras['p2'][0].C)\n",
    "\n",
    "\n",
    "print(np.array(euler_from_matrix(cameras['p1'][0].R)) * 200/np.pi)\n",
    "print(np.array(euler_from_matrix(cameras['p2'][0].R)) * 200/np.pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Compute DSM and orthophotos '''\n",
    "# @TODO: implement better DSM class\n",
    "\n",
    "print('DSM and orthophoto generation started')\n",
    "res = 0.03\n",
    "xlim = [-100., 80.]\n",
    "ylim = [-10., 65.]\n",
    "\n",
    "dsms = []\n",
    "ortofoto = dict.fromkeys(cams)\n",
    "ortofoto[cams[0]], ortofoto[cams[1]] = [], []\n",
    "for epoch in cfg.proc.epoch_to_process:\n",
    "    print(f'Epoch {epoch}')\n",
    "    dsms.append(build_dsm(np.asarray(pcd[epoch].points),\n",
    "                          dsm_step=res,\n",
    "                          xlim=xlim, ylim=ylim,\n",
    "                          make_dsm_plot=False,\n",
    "                          # fill_value = ,\n",
    "                          save_path=f'res/dsm/dsm_app_epoch_{epoch}.tif'\n",
    "                          ))\n",
    "    print('DSM built.')\n",
    "    for cam in cams:\n",
    "        fout_name = f'res/ortofoto/ortofoto_app_cam_{cam}_epc_{epoch}.tif'\n",
    "        ortofoto[cam].append(generate_ortophoto(cv2.cvtColor(images[cam][epoch], cv2.COLOR_BGR2RGB),\n",
    "                                                dsms[epoch], cameras[cam][epoch],\n",
    "                                                xlim=xlim, ylim=ylim,\n",
    "                                                save_path=fout_name,\n",
    "                                                ))\n",
    "    print('Orthophotos built.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('belpy_gdal')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5257680a82f661cea8699dc8fe4567e52d11c753044270df4ff2b694c33cdedf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
