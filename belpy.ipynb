{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (classes.py, line 163)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/photogrammetry/anaconda3/envs/belpy_gdal/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3553\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"/tmp/ipykernel_4853/3656261827.py\"\u001b[0m, line \u001b[1;32m1\u001b[0m, in \u001b[1;35m<cell line: 1>\u001b[0m\n    from lib.validate_inputs import validate\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/photogrammetry/belpy/lib/validate_inputs.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from lib.classes import Imageds\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/photogrammetry/belpy/lib/classes.py\"\u001b[0;36m, line \u001b[0;32m163\u001b[0m\n\u001b[0;31m    Compute and return the camera center from projection matrix P, as\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from lib.validate_inputs import validate\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.cm as cm\n",
    "import pydegensac\n",
    "\n",
    "from pathlib import Path\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "from lib.config import parse_yaml_cfg\n",
    "from lib.classes import (Camera, Imageds, Features, Targets)\n",
    "from lib.sfm.two_view_geometry import Two_view_geometry\n",
    "from lib.sfm.triangulation import Triangulate\n",
    "from lib.match_pairs import match_pair\n",
    "from lib.track_matches import track_matches\n",
    "\n",
    "from lib.geometry import (project_points,\n",
    "                          compute_reprojection_error\n",
    "                          )\n",
    "from lib.utils import (build_dsm,\n",
    "                       generate_ortophoto,\n",
    "                       )\n",
    "from lib.point_clouds import (create_point_cloud,\n",
    "                              write_ply,\n",
    "                              )\n",
    "from lib.visualization import (display_point_cloud,\n",
    "                               display_pc_inliers,\n",
    "                               plot_features,\n",
    "                               plot_projections,\n",
    "                               )\n",
    "from lib.misc import (convert_to_homogeneous,\n",
    "                      convert_from_homogeneous,\n",
    "                      create_directory,\n",
    "                      )\n",
    "\n",
    "from thirdparty.transformations import affine_matrix_from_points\n",
    "\n",
    "# Parse options from yaml file\n",
    "cfg_file = 'config/config_base.yaml'\n",
    "cfg = parse_yaml_cfg(cfg_file)\n",
    "\n",
    "# Inizialize Variables\n",
    "cams = cfg.paths.cam_names\n",
    "features = dict.fromkeys(cams)  # @TODO: put this in an inizialization function\n",
    "\n",
    "# Create Image Datastore objects\n",
    "images = dict.fromkeys(cams)  # @TODO: put this in an inizialization function\n",
    "for cam in cams:\n",
    "    images[cam] = Imageds(cfg.paths.imdir / cam)\n",
    "\n",
    "cfg = validate(cfg, images)\n",
    "\n",
    "''' Perform matching and tracking '''\n",
    "# Load matching and tracking configurations\n",
    "with open(cfg.matching_cfg) as f:\n",
    "    opt_matching = edict(json.load(f))\n",
    "with open(cfg.tracking_cfg) as f:\n",
    "    opt_tracking = edict(json.load(f))\n",
    "\n",
    "# epoch = 0\n",
    "if cfg.proc.do_matching:\n",
    "    for cam in cams:\n",
    "        features[cam] = []\n",
    "\n",
    "    for epoch in cfg.proc.epoch_to_process:\n",
    "        print(f'Processing epoch {epoch}...')\n",
    "\n",
    "        # opt_matching = cfg.matching.copy()\n",
    "        epochdir = Path(cfg.paths.resdir) / f'epoch_{epoch}'\n",
    "\n",
    "        #-- Find Matches at current epoch --#\n",
    "        print(f'Run Superglue to find matches at epoch {epoch}')\n",
    "        opt_matching.output_dir = epochdir\n",
    "        pair = [\n",
    "            images[cams[0]].get_image_path(epoch),\n",
    "            images[cams[1]].get_image_path(epoch)\n",
    "        ]\n",
    "        # Call matching function\n",
    "        matchedPts, matchedDescriptors, matchedPtsScores = match_pair(\n",
    "            pair, cfg.images.bbox, opt_matching\n",
    "        )\n",
    "\n",
    "        # Store matches in features structure\n",
    "        for jj, cam in enumerate(cams):\n",
    "            # Dict keys are the cameras names, internal list contain epoches\n",
    "            features[cam].append(Features())\n",
    "            features[cam][epoch].append_features({\n",
    "                'kpts': matchedPts[jj],\n",
    "                'descr': matchedDescriptors[jj],\n",
    "                'score': matchedPtsScores[jj]\n",
    "            })\n",
    "            # @TODO: Store match confidence!\n",
    "\n",
    "        #=== Track previous matches at current epoch ===#\n",
    "        if cfg.proc.do_tracking and epoch > 0:\n",
    "            print(f'Track points from epoch {epoch-1} to epoch {epoch}')\n",
    "\n",
    "            trackoutdir = epochdir / f'from_t{epoch-1}'\n",
    "            opt_tracking['output_dir'] = trackoutdir\n",
    "            pairs = [\n",
    "                [images[cams[0]].get_image_path(epoch-1),\n",
    "                    images[cams[0]].get_image_path(epoch)],\n",
    "                [images[cams[1]].get_image_path(epoch-1),\n",
    "                    images[cams[1]].get_image_path(epoch)],\n",
    "            ]\n",
    "            prevs = [\n",
    "                features[cams[0]][epoch-1].get_features_as_dict(),\n",
    "                features[cams[1]][epoch-1].get_features_as_dict()\n",
    "            ]\n",
    "            # Call actual tracking function\n",
    "            tracked_cam0, tracked_cam1 = track_matches(\n",
    "                pairs, cfg.images.bbox, prevs, opt_tracking)\n",
    "            # @TODO: keep track of the epoch in which feature is matched\n",
    "            # @TODO: Check bounding box in tracking\n",
    "            # @TODO: clean tracking code\n",
    "\n",
    "            # Store all matches in features structure\n",
    "            features[cams[0]][epoch].append_features(tracked_cam0)\n",
    "            features[cams[1]][epoch].append_features(tracked_cam1)\n",
    "\n",
    "        # Run Pydegensac to estimate F matrix and reject outliers\n",
    "        F, inlMask = pydegensac.findFundamentalMatrix(\n",
    "            features[cams[0]][epoch].get_keypoints(),\n",
    "            features[cams[1]][epoch].get_keypoints(),\n",
    "            px_th=1.5, conf=0.99999, max_iters=10000,\n",
    "            laf_consistensy_coef=-1.0,\n",
    "            error_type='sampson',\n",
    "            symmetric_error_check=True,\n",
    "            enable_degeneracy_check=True,\n",
    "        )\n",
    "        print(f'Matching at epoch {epoch}: pydegensac found {inlMask.sum()} \\\n",
    "            inliers ({inlMask.sum()*100/len(features[cams[0]][epoch]):.2f}%)')\n",
    "        features[cams[0]][epoch].remove_outliers_features(inlMask)\n",
    "        features[cams[1]][epoch].remove_outliers_features(inlMask)\n",
    "\n",
    "        # Write matched points to disk\n",
    "        im_stems = images[cams[0]].get_image_stem(\n",
    "            epoch), images[cams[1]].get_image_stem(epoch)\n",
    "        for jj, cam in enumerate(cams):\n",
    "            features[cam][epoch].save_as_txt(\n",
    "                epochdir / f'{im_stems[jj]}_mktps.txt')\n",
    "        with open(epochdir / f'{im_stems[0]}_{im_stems[1]}_features.pickle', 'wb') as f:\n",
    "            pickle.dump(features, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        last_match_path = create_directory('res/last_epoch')\n",
    "        with open(last_match_path / 'last_features.pickle', 'wb') as f:\n",
    "            pickle.dump(features, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        print('Matching completed')\n",
    "\n",
    "elif not features[cams[0]]:\n",
    "    last_match_path = 'res/last_epoch/last_features.pickle'\n",
    "    with open(last_match_path, 'rb') as f:\n",
    "        features = pickle.load(f)\n",
    "        print(\"Loaded previous matches\")\n",
    "else:\n",
    "    print(\"Features already present, nothing was changed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OPENCV camera model.\n",
      "Using OPENCV camera model.\n",
      "Reconstructing epoch 0...\n",
      "Relative Orientation - valid points: 3074/3830\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1282.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "''' SfM '''\n",
    "\n",
    "# Initialize variables\n",
    "cameras = dict.fromkeys(cams)\n",
    "cameras[cams[0]], cameras[cams[1]] = [], []\n",
    "pcd = []\n",
    "tform = []\n",
    "h, w = 4000, 6000\n",
    "\n",
    "# Build reference camera objects with only known interior orientation\n",
    "ref_cams = dict.fromkeys(cams)\n",
    "for jj, cam in enumerate(cams):\n",
    "    ref_cams[cam] = Camera(\n",
    "        width=6000, height=400,\n",
    "        calib_path=cfg.paths.caldir / f'{cam}.txt'\n",
    "    )\n",
    "\n",
    "# Read target image coordinates\n",
    "targets = Targets(cam_id=[0, 1],  im_coord_path=cfg.georef.target_paths)\n",
    "\n",
    "# Camera baseline\n",
    "baseline_world = np.linalg.norm(\n",
    "    cfg.georef.camera_centers_world[0] - cfg.georef.camera_centers_world[1]\n",
    ")\n",
    "\n",
    "for epoch in cfg.proc.epoch_to_process:\n",
    "    # epoch = 0\n",
    "    print(f'Reconstructing epoch {epoch}...')\n",
    "\n",
    "    # Initialize Intrinsics\n",
    "    ''' Inizialize Camera Intrinsics at every epoch setting them equal to\n",
    "        the reference cameras ones.\n",
    "    '''\n",
    "    # @TODO: replace append with insert or a more robust data structure...\n",
    "    for cam in cams:\n",
    "        cameras[cam].append(\n",
    "            Camera(\n",
    "                width=ref_cams[cam].width,\n",
    "                height=ref_cams[cam].height,\n",
    "                K=ref_cams[cam].K,\n",
    "                dist=ref_cams[cam].dist,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Perform Relative orientation of the two cameras\n",
    "    ''' Initialize Two_view_geometry class with a list containing the two cameras\n",
    "        and a list contaning the matched features location on each camera.\n",
    "    '''\n",
    "    relative_ori = Two_view_geometry(\n",
    "        [cameras[cams[0]][epoch], cameras[cams[1]][epoch]],\n",
    "        [features[cams[0]][epoch].get_keypoints(),\n",
    "            features[cams[1]][epoch].get_keypoints()],\n",
    "    )\n",
    "    relative_ori.relative_orientation(threshold=1.5, confidence=0.999999)\n",
    "    relative_ori.scale_model_with_baseline(baseline_world)\n",
    "\n",
    "    # Save relative orientation results in Camera objects of current epoch\n",
    "    cameras[cams[0]][epoch] = relative_ori.cameras[0]\n",
    "    cameras[cams[1]][epoch] = relative_ori.cameras[1]\n",
    "\n",
    "    if cfg.proc.do_coregistration:\n",
    "        # @TODO: make wrappers to handle RS transformations\n",
    "\n",
    "        # Triangulate targets\n",
    "        triangulate = Triangulate(\n",
    "            [cameras[cams[0]][epoch], cameras[cams[1]][epoch]],\n",
    "            [targets.get_im_coord(0)[epoch],\n",
    "                targets.get_im_coord(1)[epoch]],\n",
    "        )\n",
    "        targets.append_obj_cord(triangulate.triangulate_two_views())\n",
    "\n",
    "        # Estimate rigid body transformation between first epoch RS and current epoch RS\n",
    "        # @TODO: make a wrapper for this\n",
    "        v0 = np.concatenate((cameras[cams[0]][0].C,\n",
    "                             cameras[cams[1]][0].C,\n",
    "                             targets.get_obj_coord()[0].reshape(3, 1),\n",
    "                             ), axis=1)\n",
    "        v1 = np.concatenate((cameras[cams[0]][epoch].C,\n",
    "                             cameras[cams[1]][epoch].C,\n",
    "                             targets.get_obj_coord()[epoch].reshape(3, 1),\n",
    "                             ), axis=1)\n",
    "        tform.append(affine_matrix_from_points(\n",
    "            v1, v0, shear=False, scale=False, usesvd=True))\n",
    "        print('Point cloud coregistered based on {len(v0)} points.')\n",
    "\n",
    "    elif epoch > 0:\n",
    "        # Fix the EO of both the cameras as those estimated in the first epoch\n",
    "        for cam in cams:\n",
    "            cameras[cam][epoch] = cameras[cam][0]\n",
    "        print('Camera exterior orientation fixed to that of the master cameras.')\n",
    "\n",
    "    #--- Triangulate Points ---#\n",
    "    ''' Initialize Triangulate class with a list containing the two cameras\n",
    "        and a list contaning the matched features location on each camera.\n",
    "        Triangulated points are saved as points3d proprierty of the\n",
    "        Triangulate object (eg., triangulation.points3d)\n",
    "    '''\n",
    "    triangulation = Triangulate(\n",
    "        [cameras[cams[0]][epoch], cameras[cams[1]][epoch]],\n",
    "        [\n",
    "            features[cams[0]][epoch].get_keypoints(),\n",
    "            features[cams[1]][epoch].get_keypoints()\n",
    "        ]\n",
    "    )\n",
    "    triangulation.triangulate_two_views()\n",
    "    triangulation.interpolate_colors_from_image(\n",
    "        images[cams[1]][epoch],\n",
    "        cameras[cams[1]][epoch],\n",
    "        convert_BRG2RGB=True,\n",
    "    )\n",
    "\n",
    "    if cfg.proc.do_coregistration:\n",
    "        # Apply rigid body transformation to triangulated points\n",
    "        # @TODO: make wrapper for apply transformation to arrays\n",
    "        pts = np.dot(tform[epoch],\n",
    "                     convert_to_homogeneous(triangulation.points3d.T)\n",
    "                     )\n",
    "        triangulation.points3d = convert_from_homogeneous(pts).T\n",
    "\n",
    "    # Create point cloud and save .ply to disk\n",
    "    pcd_epc = create_point_cloud(\n",
    "        triangulation.points3d, triangulation.colors)\n",
    "\n",
    "    # Filter outliers in point cloud with SOR filter\n",
    "    if cfg.other.do_SOR_filter:\n",
    "        _, ind = pcd_epc.remove_statistical_outlier(nb_neighbors=10,\n",
    "                                                    std_ratio=3.0)\n",
    "        #     display_pc_inliers(pcd_epc, ind)\n",
    "        pcd_epc = pcd_epc.select_by_index(ind)\n",
    "        print(\"Point cloud filtered by Statistical Oulier Removal\")\n",
    "\n",
    "    # Write point cloud to disk and store it in Point Cloud List\n",
    "    write_ply(pcd_epc, f'res/pt_clouds/sparse_pts_t{epoch}.ply')\n",
    "    pcd.append(pcd_epc)\n",
    "\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "# from traceback import print_tb\n",
    "\n",
    "from lib.visualization import display_point_cloud\n",
    "from thirdparty.transformations import euler_from_matrix, euler_matrix\n",
    "\n",
    "cam1_loc = ref_cams[cams[0]]\n",
    "cam2_loc = ref_cams[cams[1]]\n",
    "\n",
    "# IMG_1289.jpg \n",
    "# loc = np.array([312.930747,300.536595,135.159918]).reshape(3,-1)\n",
    "# angles = np.deg2rad(np.array([-83.914310,80.177810,174.222339]))\n",
    "\n",
    "# IMG_2814.jpg\n",
    "loc = np.array([151.703845, 99.171778, 91.618363]).reshape(3, -1)\n",
    "ang = np.deg2rad(np.array([100.281584, 54.781799, -12.652574]))\n",
    "R1_world2cam = euler_matrix(ang[0], ang[1], ang[2])[:3,:3]\n",
    "# print(R_world2cam1_loc)\n",
    "\n",
    "# Read point cloud in LOC RS\n",
    "sparse_loc = o3d.io.read_point_cloud('belvedere_20220728_sparse_LOC.ply')\n",
    "\n",
    "# Define cam1_locera EO\n",
    "\n",
    "\n",
    "def build_pose_matrix(R: np.ndarray, C: np.ndarray) -> np.ndarray:\n",
    "    # Check for input dimensions \n",
    "    if R.shape != (3,3):\n",
    "        raise ValueError('Wrong dimension of the R matrix. It must be a 3x3 numpy array')\n",
    "    if C.shape == (3,) or C.shape == (1,3):\n",
    "        C = C.T\n",
    "    elif C.shape != (3,1):\n",
    "        raise ValueError('Wrong dimension of the C vector. It must be a 3x1 or a 1x3 numpy array')\n",
    "    \n",
    "    pose = np.eye(4)\n",
    "    pose[0:3, 0:3] = R\n",
    "    pose[0:3, 3:4] = C\n",
    "    return pose \n",
    "\n",
    "def compute_camera_EO(camera: Camera, \n",
    "                      extrinsics: np.ndarray = None, \n",
    "                      pose: np.ndarray = None\n",
    "                      ) -> Camera:\n",
    "    \n",
    "    if extrinsics is not None:\n",
    "        camera.extrinsics = extrinsics\n",
    "        camera.extrinsics_to_pose()\n",
    "        camera.update_camera_from_extrinsics()\n",
    "        return camera\n",
    "    \n",
    "    if pose is not None: \n",
    "        camera.pose = pose\n",
    "        camera.pose_to_extrinsics()\n",
    "        camera.update_camera_from_extrinsics()\n",
    "        return camera\n",
    "    \n",
    "    else: \n",
    "        raise ValueError('Not enough data to build Camera External Orientation matrixes.')\n",
    "\n",
    "\n",
    "cam1toWorld = build_pose_matrix(R1_world2cam.T, loc) @ cameras[cams[0]][0].pose\n",
    "cam1_loc = compute_camera_EO(cam1_loc, pose=cam1toWorld)\n",
    "\n",
    "cam2toWorld = cam1_loc.pose @ cameras[cams[1]][0].pose\n",
    "cam2_loc = compute_camera_EO(cam2_loc, pose=cam2toWorld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PnP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize point cloud\n",
    "display_point_cloud(\n",
    "    sparse_loc,\n",
    "    [cam1_loc, cam2_loc], \n",
    "    plot_scale=7,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('belpy_gdal')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5257680a82f661cea8699dc8fe4567e52d11c753044270df4ff2b694c33cdedf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
