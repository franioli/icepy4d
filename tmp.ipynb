{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Input parameters are valid.\n",
      "\n",
      "Image datastores created successfully.\n",
      "Processing epoch 0...\n",
      "Run Superglue to find matches at epoch 0\n",
      "Will not resize images\n",
      "Running inference on device \"cuda\"\n",
      "Loaded SuperPoint model\n",
      "Loaded SuperGlue model (\"outdoor\" weights)\n",
      "Will write matches to directory \"res/epoch_0\"\n",
      "Will write visualization images to directory \"res/epoch_0\"\n",
      "Warning: input resolution is very large, results may vary\n",
      "Warning: input resolution is very large, results may vary\n",
      "Images subdivided in 2x4 tiles\n",
      "[Finished Tile Pairs  0 -  0 of  8] matcher=5.939 viz_match=0.264 total=6.203 sec {0.2 FPS} \n",
      "[Finished Tile Pairs  1 -  1 of  8] matcher=5.490 viz_match=0.260 total=5.750 sec {0.2 FPS} \n",
      "[Finished Tile Pairs  2 -  2 of  8] matcher=4.912 viz_match=0.265 total=5.177 sec {0.2 FPS} \n",
      "[Finished Tile Pairs  3 -  3 of  8] matcher=4.766 viz_match=0.266 total=5.033 sec {0.2 FPS} \n",
      "[Finished Tile Pairs  4 -  4 of  8] matcher=4.268 viz_match=0.269 total=4.537 sec {0.2 FPS} \n",
      "[Finished Tile Pairs  5 -  5 of  8] matcher=4.319 viz_match=0.269 total=4.588 sec {0.2 FPS} \n",
      "[Finished Tile Pairs  6 -  6 of  8] matcher=4.353 viz_match=0.275 total=4.629 sec {0.2 FPS} \n",
      "[Finished Tile Pairs  7 -  7 of  8] matcher=4.376 viz_match=0.263 total=4.640 sec {0.2 FPS} \n",
      "[Finished pair] load_image=0.319 create_tiles=0.010 viz_match=38.155 total=38.484 sec {0.0 FPS} \n",
      "Matching at epoch 0: pydegensac found 3796             inliers (47.23%)\n",
      "Matching completed\n"
     ]
    }
   ],
   "source": [
    "from lib.validate_inputs import validate\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.cm as cm\n",
    "import pydegensac\n",
    "\n",
    "from pathlib import Path\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "from lib.config import parse_yaml_cfg\n",
    "from lib.classes import (Camera, Imageds, Features, Targets)\n",
    "from lib.sfm.two_view_geometry import Two_view_geometry\n",
    "from lib.sfm.triangulation import Triangulate\n",
    "from lib.match_pairs import match_pair\n",
    "from lib.track_matches import track_matches\n",
    "\n",
    "from lib.geometry import (project_points,\n",
    "                          compute_reprojection_error\n",
    "                          )\n",
    "from lib.utils import (build_dsm,\n",
    "                       generate_ortophoto,\n",
    "                       )\n",
    "from lib.point_clouds import (create_point_cloud,\n",
    "                              write_ply,\n",
    "                              )\n",
    "from lib.visualization import (display_point_cloud,\n",
    "                               display_pc_inliers,\n",
    "                               plot_features,\n",
    "                               plot_projections,\n",
    "                               )\n",
    "from lib.misc import (convert_to_homogeneous,\n",
    "                      convert_from_homogeneous,\n",
    "                      create_directory,\n",
    "                      )\n",
    "\n",
    "from thirdparty.transformations import affine_matrix_from_points\n",
    "\n",
    "# Parse options from yaml file\n",
    "cfg_file = 'config/config_base.yaml'\n",
    "cfg = parse_yaml_cfg(cfg_file)\n",
    "\n",
    "# Inizialize Variables\n",
    "cams = cfg.paths.cam_names\n",
    "features = dict.fromkeys(cams)  # @TODO: put this in an inizialization function\n",
    "\n",
    "# Create Image Datastore objects\n",
    "images = dict.fromkeys(cams)  # @TODO: put this in an inizialization function\n",
    "for cam in cams:\n",
    "    images[cam] = Imageds(cfg.paths.imdir / cam)\n",
    "\n",
    "cfg = validate(cfg, images)\n",
    "\n",
    "''' Perform matching and tracking '''\n",
    "# Load matching and tracking configurations\n",
    "with open(cfg.matching_cfg) as f:\n",
    "    opt_matching = edict(json.load(f))\n",
    "with open(cfg.tracking_cfg) as f:\n",
    "    opt_tracking = edict(json.load(f))\n",
    "\n",
    "epoch = 0\n",
    "if cfg.proc.do_matching:\n",
    "    for cam in cams:\n",
    "        features[cam] = []\n",
    "\n",
    "    for epoch in cfg.proc.epoch_to_process:\n",
    "        print(f'Processing epoch {epoch}...')\n",
    "\n",
    "        # opt_matching = cfg.matching.copy()\n",
    "        epochdir = Path(cfg.paths.resdir) / f'epoch_{epoch}'\n",
    "\n",
    "        #-- Find Matches at current epoch --#\n",
    "        print(f'Run Superglue to find matches at epoch {epoch}')\n",
    "        opt_matching.output_dir = epochdir\n",
    "        pair = [\n",
    "            images[cams[0]].get_image_path(epoch),\n",
    "            images[cams[1]].get_image_path(epoch)\n",
    "        ]\n",
    "        # Call matching function\n",
    "        matchedPts, matchedDescriptors, matchedPtsScores = match_pair(\n",
    "            pair, cfg.images.bbox, opt_matching\n",
    "        )\n",
    "\n",
    "        # Store matches in features structure\n",
    "        for jj, cam in enumerate(cams):\n",
    "            # Dict keys are the cameras names, internal list contain epoches\n",
    "            features[cam].append(Features())\n",
    "            features[cam][epoch].append_features({\n",
    "                'kpts': matchedPts[jj],\n",
    "                'descr': matchedDescriptors[jj],\n",
    "                'score': matchedPtsScores[jj]\n",
    "            })\n",
    "            # @TODO: Store match confidence!\n",
    "\n",
    "        #=== Track previous matches at current epoch ===#\n",
    "        if cfg.proc.do_tracking and epoch > 0:\n",
    "            print(f'Track points from epoch {epoch-1} to epoch {epoch}')\n",
    "\n",
    "            trackoutdir = epochdir / f'from_t{epoch-1}'\n",
    "            opt_tracking['output_dir'] = trackoutdir\n",
    "            pairs = [\n",
    "                [images[cams[0]].get_image_path(epoch-1),\n",
    "                    images[cams[0]].get_image_path(epoch)],\n",
    "                [images[cams[1]].get_image_path(epoch-1),\n",
    "                    images[cams[1]].get_image_path(epoch)],\n",
    "            ]\n",
    "            prevs = [\n",
    "                features[cams[0]][epoch-1].get_features_as_dict(),\n",
    "                features[cams[1]][epoch-1].get_features_as_dict()\n",
    "            ]\n",
    "            # Call actual tracking function\n",
    "            tracked_cam0, tracked_cam1 = track_matches(\n",
    "                pairs, cfg.images.bbox, prevs, opt_tracking)\n",
    "            # @TODO: keep track of the epoch in which feature is matched\n",
    "            # @TODO: Check bounding box in tracking\n",
    "            # @TODO: clean tracking code\n",
    "\n",
    "            # Store all matches in features structure\n",
    "            features[cams[0]][epoch].append_features(tracked_cam0)\n",
    "            features[cams[1]][epoch].append_features(tracked_cam1)\n",
    "\n",
    "        # Run Pydegensac to estimate F matrix and reject outliers\n",
    "        F, inlMask = pydegensac.findFundamentalMatrix(\n",
    "            features[cams[0]][epoch].get_keypoints(),\n",
    "            features[cams[1]][epoch].get_keypoints(),\n",
    "            px_th=1.5, conf=0.99999, max_iters=10000,\n",
    "            laf_consistensy_coef=-1.0,\n",
    "            error_type='sampson',\n",
    "            symmetric_error_check=True,\n",
    "            enable_degeneracy_check=True,\n",
    "        )\n",
    "        print(f'Matching at epoch {epoch}: pydegensac found {inlMask.sum()} \\\n",
    "            inliers ({inlMask.sum()*100/len(features[cams[0]][epoch]):.2f}%)')\n",
    "        features[cams[0]][epoch].remove_outliers_features(inlMask)\n",
    "        features[cams[1]][epoch].remove_outliers_features(inlMask)\n",
    "\n",
    "        # Write matched points to disk\n",
    "        im_stems = images[cams[0]].get_image_stem(\n",
    "            epoch), images[cams[1]].get_image_stem(epoch)\n",
    "        for jj, cam in enumerate(cams):\n",
    "            features[cam][epoch].save_as_txt(\n",
    "                epochdir / f'{im_stems[jj]}_mktps.txt')\n",
    "        with open(epochdir / f'{im_stems[0]}_{im_stems[1]}_features.pickle', 'wb') as f:\n",
    "            pickle.dump(features, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        last_match_path = create_directory('res/last_epoch')\n",
    "        with open(last_match_path / 'last_features.pickle', 'wb') as f:\n",
    "            pickle.dump(features, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        print('Matching completed')\n",
    "\n",
    "elif not features[cams[0]]:\n",
    "    last_match_path = 'res/last_epoch/last_features.pickle'\n",
    "    with open(last_match_path, 'rb') as f:\n",
    "        features = pickle.load(f)\n",
    "        print(\"Loaded previous matches\")\n",
    "else:\n",
    "    print(\"Features already present, nothing was changed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OPENCV camera model.\n",
      "Using OPENCV camera model.\n",
      "Reconstructing epoch 0...\n",
      "Relative Orientation - valid points: 3290/3796\n",
      "Point triangulation succeded: 1.0.\n",
      "Loaded image IMG_1282.jpg\n",
      "Points color interpolated\n",
      "Point cloud filtered by Statistical Oulier Removal\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "''' SfM '''\n",
    "\n",
    "# Initialize variables\n",
    "cameras = dict.fromkeys(cams)\n",
    "cameras[cams[0]], cameras[cams[1]] = [], []\n",
    "pcd = []\n",
    "tform = []\n",
    "h, w = 4000, 6000\n",
    "\n",
    "# Build reference camera objects with only known interior orientation\n",
    "ref_cams = dict.fromkeys(cams)\n",
    "for jj, cam in enumerate(cams):\n",
    "    ref_cams[cam] = Camera(\n",
    "        width=6000, height=400,\n",
    "        calib_path=cfg.paths.caldir / f'{cam}.txt'\n",
    "    )\n",
    "\n",
    "# Read target image coordinates\n",
    "targets = Targets(cam_id=[0, 1],  im_coord_path=cfg.georef.target_paths)\n",
    "\n",
    "# Camera baseline\n",
    "baseline_world = np.linalg.norm(\n",
    "    cfg.georef.camera_centers_world[0] - cfg.georef.camera_centers_world[1]\n",
    ")\n",
    "\n",
    "for epoch in cfg.proc.epoch_to_process:\n",
    "    # epoch = 0\n",
    "    print(f'Reconstructing epoch {epoch}...')\n",
    "\n",
    "    # Initialize Intrinsics\n",
    "    ''' Inizialize Camera Intrinsics at every epoch setting them equal to\n",
    "        the reference cameras ones.\n",
    "    '''\n",
    "    # @TODO: replace append with insert or a more robust data structure...\n",
    "    for cam in cams:\n",
    "        cameras[cam].append(\n",
    "            Camera(\n",
    "                width=ref_cams[cam].width,\n",
    "                height=ref_cams[cam].height,\n",
    "                K=ref_cams[cam].K,\n",
    "                dist=ref_cams[cam].dist,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Perform Relative orientation of the two cameras\n",
    "    ''' Initialize Two_view_geometry class with a list containing the two cameras\n",
    "        and a list contaning the matched features location on each camera.\n",
    "    '''\n",
    "    relative_ori = Two_view_geometry(\n",
    "        [cameras[cams[0]][epoch], cameras[cams[1]][epoch]],\n",
    "        [features[cams[0]][epoch].get_keypoints(),\n",
    "            features[cams[1]][epoch].get_keypoints()],\n",
    "    )\n",
    "    relative_ori.relative_orientation(threshold=1.5, confidence=0.999999)\n",
    "    relative_ori.scale_model_with_baseline(baseline_world)\n",
    "\n",
    "    # Save relative orientation results in Camera objects of current epoch\n",
    "    cameras[cams[0]][epoch] = relative_ori.cameras[0]\n",
    "    cameras[cams[1]][epoch] = relative_ori.cameras[1]\n",
    "\n",
    "    if cfg.proc.do_coregistration:\n",
    "        # @TODO: make wrappers to handle RS transformations\n",
    "\n",
    "        # Triangulate targets\n",
    "        triangulate = Triangulate(\n",
    "            [cameras[cams[0]][epoch], cameras[cams[1]][epoch]],\n",
    "            [targets.get_im_coord(0)[epoch],\n",
    "                targets.get_im_coord(1)[epoch]],\n",
    "        )\n",
    "        targets.append_obj_cord(triangulate.triangulate_two_views())\n",
    "\n",
    "        # Estimate rigid body transformation between first epoch RS and current epoch RS\n",
    "        # @TODO: make a wrapper for this\n",
    "        v0 = np.concatenate((cameras[cams[0]][0].C,\n",
    "                             cameras[cams[1]][0].C,\n",
    "                             targets.get_obj_coord()[0].reshape(3, 1),\n",
    "                             ), axis=1)\n",
    "        v1 = np.concatenate((cameras[cams[0]][epoch].C,\n",
    "                             cameras[cams[1]][epoch].C,\n",
    "                             targets.get_obj_coord()[epoch].reshape(3, 1),\n",
    "                             ), axis=1)\n",
    "        tform.append(affine_matrix_from_points(\n",
    "            v1, v0, shear=False, scale=False, usesvd=True))\n",
    "        print('Point cloud coregistered based on {len(v0)} points.')\n",
    "\n",
    "    elif epoch > 0:\n",
    "        # Fix the EO of both the cameras as those estimated in the first epoch\n",
    "        for cam in cams:\n",
    "            cameras[cam][epoch] = cameras[cam][0]\n",
    "        print('Camera exterior orientation fixed to that of the master cameras.')\n",
    "\n",
    "    #--- Triangulate Points ---#\n",
    "    ''' Initialize Triangulate class with a list containing the two cameras\n",
    "        and a list contaning the matched features location on each camera.\n",
    "        Triangulated points are saved as points3d proprierty of the\n",
    "        Triangulate object (eg., triangulation.points3d)\n",
    "    '''\n",
    "    triangulation = Triangulate(\n",
    "        [cameras[cams[0]][epoch], cameras[cams[1]][epoch]],\n",
    "        [\n",
    "            features[cams[0]][epoch].get_keypoints(),\n",
    "            features[cams[1]][epoch].get_keypoints()\n",
    "        ]\n",
    "    )\n",
    "    triangulation.triangulate_two_views()\n",
    "    triangulation.interpolate_colors_from_image(\n",
    "        images[cams[1]][epoch],\n",
    "        cameras[cams[1]][epoch],\n",
    "        convert_BRG2RGB=True,\n",
    "    )\n",
    "\n",
    "    if cfg.proc.do_coregistration:\n",
    "        # Apply rigid body transformation to triangulated points\n",
    "        # @TODO: make wrapper for apply transformation to arrays\n",
    "        pts = np.dot(tform[epoch],\n",
    "                     convert_to_homogeneous(triangulation.points3d.T)\n",
    "                     )\n",
    "        triangulation.points3d = convert_from_homogeneous(pts).T\n",
    "\n",
    "    # Create point cloud and save .ply to disk\n",
    "    pcd_epc = create_point_cloud(\n",
    "        triangulation.points3d, triangulation.colors)\n",
    "\n",
    "    # Filter outliers in point cloud with SOR filter\n",
    "    if cfg.other.do_SOR_filter:\n",
    "        _, ind = pcd_epc.remove_statistical_outlier(nb_neighbors=10,\n",
    "                                                    std_ratio=3.0)\n",
    "        #     display_pc_inliers(pcd_epc, ind)\n",
    "        pcd_epc = pcd_epc.select_by_index(ind)\n",
    "        print(\"Point cloud filtered by Statistical Oulier Removal\")\n",
    "\n",
    "    # Write point cloud to disk and store it in Point Cloud List\n",
    "    write_ply(pcd_epc, f'res/pt_clouds/sparse_pts_t{epoch}.ply')\n",
    "    pcd.append(pcd_epc)\n",
    "\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[151.703845]\n",
      " [ 99.171778]\n",
      " [ 91.618363]]\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "from traceback import print_tb\n",
    "from thirdparty.transformations import euler_from_matrix, euler_matrix\n",
    "\n",
    "cam1_loc = ref_cams[cams[0]]\n",
    "\n",
    "# IMG_1289.jpg \n",
    "# loc = np.array([312.930747,300.536595,135.159918]).reshape(3,-1)\n",
    "# angles = np.deg2rad(np.array([-83.914310,80.177810,174.222339]))\n",
    "\n",
    "# IMG_2814.jpg\n",
    "loc = np.array([151.703845, 99.171778, 91.618363]).reshape(3, -1)\n",
    "ang = np.deg2rad(np.array([100.281584, 54.781799, -12.652574]))\n",
    "R1_world2cam = euler_matrix(ang[0], ang[1], ang[2])[:3,:3]\n",
    "# print(R_world2cam1_loc)\n",
    "\n",
    "# Read point cloud in LOC RS\n",
    "sparse_loc = o3d.io.read_point_cloud('belvedere_20220728_sparse_LOC.ply')\n",
    "\n",
    "# Define cam1_locera EO\n",
    "\n",
    "\n",
    "def build_pose_matrix(R: np.ndarray, C: np.ndarray) -> np.ndarray:\n",
    "    pose = np.eye(4)\n",
    "    pose[0:3, 0:3] = R\n",
    "    pose[0:3, 3:4] = C\n",
    "    return pose \n",
    "\n",
    "cam1_loc.pose = build_pose_matrix(R1_world2cam.T, loc)\n",
    "cam1_loc.pose_to_extrinsics()\n",
    "cam1_loc.update_camera_from_extrinsics()\n",
    "cam1_loc.C_from_P()\n",
    "print(cam1_loc.C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.68365927  0.08919199 -0.72433059]\n",
      " [ 0.02700303  0.98873265  0.1472365 ]\n",
      " [ 0.72930161 -0.12021872  0.67354778]]\n",
      "[[ 2.54107820e+02]\n",
      " [-6.21853880e+01]\n",
      " [-1.44115213e-01]]\n",
      "[[302.66001972]\n",
      " [310.23701348]\n",
      " [ 58.42910085]]\n"
     ]
    }
   ],
   "source": [
    "R_2to1 = cameras['p2'][0].R.T\n",
    "t_2to1 = cameras['p2'][0].get_C_from_pose()\n",
    "print(R_2to1)\n",
    "print(t_2to1)\n",
    "\n",
    "\n",
    "cam2_loc = ref_cams[cams[1]]\n",
    "cam2_loc.pose = cam1_loc.pose @ build_pose_matrix(R_2to1, t_2to1)\n",
    "cam2_loc.pose_to_extrinsics()\n",
    "cam2_loc.update_camera_from_extrinsics()\n",
    "cam2_loc.C_from_P()\n",
    "\n",
    "print(cam2_loc.C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.84686584e-01,  6.64683334e-02,  2.59163057e-01,\n",
       "         9.30510124e+01],\n",
       "       [-3.41096344e-03, -3.46278297e-01, -1.36651550e-01,\n",
       "        -1.45411317e+01],\n",
       "       [-5.95811518e-01, -6.82159014e-02, -6.93292196e-02,\n",
       "        -1.16403090e+02],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "cam2_loc.C = - t_2to1 + cam1_loc.C\n",
    "R_2toWorld = R_2to1 * cam1_loc.R\n",
    "cam2_loc.R = R_2toWorld.T\n",
    "\n",
    "cam2_loc.t_from_RC()\n",
    "cam2_loc.Rt_to_extrinsics()\n",
    "cam2_loc.extrinsics_to_pose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize point cloud\n",
    "epoch = 0\n",
    "display_point_cloud(\n",
    "    sparse_loc,\n",
    "    [cam1_loc, cam2_loc], \n",
    "    plot_scale=7,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C2:  [[ 2.54107820e+02]\n",
      " [-6.21853880e+01]\n",
      " [-1.44115213e-01]]\n",
      "C1_w:  [[151.703845]\n",
      " [ 99.171778]\n",
      " [ 91.618363]]\n"
     ]
    }
   ],
   "source": [
    "print('C2: ' , cameras['p2'][0].get_C_from_pose())\n",
    "print('C1_w: ', cam1_loc.C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# print(np.dot(cameras['p2'][0].R.T, cameras['p2'][0].C))\n",
    "cam2_loc = ref_cams[cams[1]]\n",
    "t_2toWorld = np.dot( - cam1_loc.R.T, cameras['p2'][0].C)\n",
    "print(t_2toWorld)\n",
    "\n",
    "\n",
    "# R_2to1 = cameras['p2'][0].R.T\n",
    "# t_2to1 = \n",
    "\n",
    "# cam2_loc = ref_cams[cams[1]]\n",
    "# cam2_loc.C = - t_2to1 + cam1_loc.C\n",
    "# R_2toWorld = R_2to1 * cam1_loc.R\n",
    "# cam2_loc.R = R_2toWorld.T\n",
    "\n",
    "# cam2_loc.t_from_RC()\n",
    "# cam2_loc.Rt_to_extrinsics()\n",
    "# cam2_loc.extrinsics_to_pose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.62687589e-01 -1.26317803e-01 -8.16961743e-01  1.51703845e+02]\n",
      " [ 7.45227639e-01 -3.50224399e-01  5.67431613e-01  9.91717780e+01]\n",
      " [-3.57796650e-01 -9.28109198e-01 -1.02931405e-01  9.16183630e+01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
      "[[302.66001972]\n",
      " [310.23701348]\n",
      " [ 58.42910085]\n",
      " [  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# IMG_2814.jpg\n",
    "loc = np.array([151.703845, 99.171778, 91.618363]).reshape(3, -1)\n",
    "ang = np.deg2rad(np.array([100.281584, 54.781799, -12.652574]))\n",
    "R1_world2cam = euler_matrix(ang[0], ang[1], ang[2])[:3,:3]\n",
    "# print(R_world2cam1_loc)\n",
    "\n",
    "T_1toW = np.eye(4) \n",
    "T_1toW[:3,:3] = R1_world2cam.T\n",
    "T_1toW[:3,3:4] = loc\n",
    "print(T_1toW)\n",
    "\n",
    "\n",
    "point = convert_to_homogeneous(cameras['p2'][0].get_C_from_pose())\n",
    "# point = np.array([5,5,0,1])\n",
    "\n",
    "print(T_1toW@point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.],\n",
       "       [-0.],\n",
       "       [-0.],\n",
       "       [ 1.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('belpy_gdal')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5257680a82f661cea8699dc8fe4567e52d11c753044270df4ff2b694c33cdedf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
